Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Tensor("tower_0/conv1/conv1/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8aa1ffd0>
Tensor("tower_0/residual1/residual1_conv2/residual1_conv2/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a968f10>
Tensor("tower_0/residual1/residual1_conv3/residual1_conv3/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a894e50>
Tensor("tower_0/residual1/residual1_conv4/residual1_conv4/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a7cdd10>
Tensor("tower_0/residual1/residual1_conv5/residual1_conv5/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a775e10>
Tensor("tower_0/residual1/residual1_conv6/residual1_conv6/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a703f90>
Tensor("tower_0/residual1/residual1_conv7/residual1_conv7/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a62f810>
Tensor("tower_0/residual1/residual1_sum3:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:1)
Tensor("tower_0/residual2/residual2_conv8/residual2_conv8/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a508410>
Tensor("tower_0/residual2/residual2_resize8/residual2_resize8/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a544490>
Tensor("tower_0/residual2/residual2_conv9/residual2_conv9/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a3e0410>
Tensor("tower_0/residual2/residual2_conv10/residual2_conv10/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a35de50>
Tensor("tower_0/residual2/residual2_conv11/residual2_conv11/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a28a990>
Tensor("tower_0/residual2/residual2_conv12/residual2_conv12/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a1cc910>
Tensor("tower_0/residual2/residual2_conv13/residual2_conv13/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a183fd0>
Tensor("tower_0/residual2/residual2_sum3:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:1)
Tensor("tower_0/residual3/residual3_conv14/residual3_conv14/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a0be450>
Tensor("tower_0/residual3/residual3_resize14/residual3_resize14/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a05c910>
Tensor("tower_0/residual3/residual3_conv15/residual3_conv15/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89f6cc50>
Tensor("tower_0/residual3/residual3_conv16/residual3_conv16/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89eb5f50>
Tensor("tower_0/residual3/residual3_conv17/residual3_conv17/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89de0f10>
Tensor("tower_0/residual3/residual3_conv18/residual3_conv18/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89d1a390>
Tensor("tower_0/residual3/residual3_conv19/residual3_conv19/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:1)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89cc6350>
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Tensor("tower_1/conv1/conv1/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8aa1ffd0>
Tensor("tower_1/residual1/residual1_conv2/residual1_conv2/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a968f10>
Tensor("tower_1/residual1/residual1_conv3/residual1_conv3/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a894e50>
Tensor("tower_1/residual1/residual1_conv4/residual1_conv4/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a7cdd10>
Tensor("tower_1/residual1/residual1_conv5/residual1_conv5/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a775e10>
Tensor("tower_1/residual1/residual1_conv6/residual1_conv6/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a703f90>
Tensor("tower_1/residual1/residual1_conv7/residual1_conv7/bn/add_1:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a62f810>
Tensor("tower_1/residual1/residual1_sum3:0", shape=(128, 32, 32, 16), dtype=float32, device=/device:GPU:2)
Tensor("tower_1/residual2/residual2_conv8/residual2_conv8/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a508410>
Tensor("tower_1/residual2/residual2_resize8/residual2_resize8/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a544490>
Tensor("tower_1/residual2/residual2_conv9/residual2_conv9/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a3e0410>
Tensor("tower_1/residual2/residual2_conv10/residual2_conv10/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a35de50>
Tensor("tower_1/residual2/residual2_conv11/residual2_conv11/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a28a990>
Tensor("tower_1/residual2/residual2_conv12/residual2_conv12/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a1cc910>
Tensor("tower_1/residual2/residual2_conv13/residual2_conv13/bn/add_1:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a183fd0>
Tensor("tower_1/residual2/residual2_sum3:0", shape=(128, 16, 16, 32), dtype=float32, device=/device:GPU:2)
Tensor("tower_1/residual3/residual3_conv14/residual3_conv14/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a0be450>
Tensor("tower_1/residual3/residual3_resize14/residual3_resize14/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc8a05c910>
Tensor("tower_1/residual3/residual3_conv15/residual3_conv15/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89f6cc50>
Tensor("tower_1/residual3/residual3_conv16/residual3_conv16/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89eb5f50>
Tensor("tower_1/residual3/residual3_conv17/residual3_conv17/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89de0f10>
Tensor("tower_1/residual3/residual3_conv18/residual3_conv18/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89d1a390>
Tensor("tower_1/residual3/residual3_conv19/residual3_conv19/bn/add_1:0", shape=(128, 8, 8, 64), dtype=float32, device=/device:GPU:2)
<tensorflow.python.ops.variables.Variable object at 0x7fdc89cc6350>
2016-05-26 04:42:19.403826: step 0, loss = 2.31 (20.4 examples/sec; 6.282 sec/batch)
eval once
2016-05-26 04:43:02.277859: accuracy @ 1 = 0.100, 4988 / 50048 at 0
2016-05-26 04:43:04.792383: step 10, loss = 2.25 (1063.1 examples/sec; 0.120 sec/batch)
2016-05-26 04:43:07.262593: step 20, loss = 2.11 (1069.9 examples/sec; 0.120 sec/batch)
2016-05-26 04:43:09.767521: step 30, loss = 2.13 (1014.6 examples/sec; 0.126 sec/batch)
2016-05-26 04:43:12.284406: step 40, loss = 2.09 (1003.3 examples/sec; 0.128 sec/batch)
2016-05-26 04:43:14.774074: step 50, loss = 2.00 (986.2 examples/sec; 0.130 sec/batch)
2016-05-26 04:43:17.339088: step 60, loss = 2.00 (1004.9 examples/sec; 0.127 sec/batch)
2016-05-26 04:43:19.940100: step 70, loss = 1.88 (999.5 examples/sec; 0.128 sec/batch)
2016-05-26 04:43:22.526776: step 80, loss = 1.88 (987.5 examples/sec; 0.130 sec/batch)
2016-05-26 04:43:25.111083: step 90, loss = 1.76 (1002.3 examples/sec; 0.128 sec/batch)
2016-05-26 04:43:27.638080: step 100, loss = 1.83 (1002.6 examples/sec; 0.128 sec/batch)
2016-05-26 04:43:30.506112: step 110, loss = 1.77 (986.5 examples/sec; 0.130 sec/batch)
2016-05-26 04:43:33.018588: step 120, loss = 1.82 (997.4 examples/sec; 0.128 sec/batch)
2016-05-26 04:43:35.548589: step 130, loss = 1.77 (1039.1 examples/sec; 0.123 sec/batch)
2016-05-26 04:43:38.133186: step 140, loss = 1.68 (956.0 examples/sec; 0.134 sec/batch)
2016-05-26 04:43:40.644463: step 150, loss = 1.60 (970.1 examples/sec; 0.132 sec/batch)
2016-05-26 04:43:43.244211: step 160, loss = 1.73 (981.0 examples/sec; 0.130 sec/batch)
2016-05-26 04:43:45.795550: step 170, loss = 1.79 (919.4 examples/sec; 0.139 sec/batch)
2016-05-26 04:43:48.289250: step 180, loss = 1.61 (979.1 examples/sec; 0.131 sec/batch)
2016-05-26 04:43:50.809006: step 190, loss = 1.51 (934.1 examples/sec; 0.137 sec/batch)
2016-05-26 04:43:53.317421: step 200, loss = 1.52 (1041.2 examples/sec; 0.123 sec/batch)
2016-05-26 04:43:56.211901: step 210, loss = 1.59 (993.4 examples/sec; 0.129 sec/batch)
2016-05-26 04:43:58.730141: step 220, loss = 1.57 (1042.8 examples/sec; 0.123 sec/batch)
2016-05-26 04:44:01.255639: step 230, loss = 1.69 (1029.0 examples/sec; 0.124 sec/batch)
2016-05-26 04:44:03.755635: step 240, loss = 1.58 (1079.4 examples/sec; 0.119 sec/batch)
2016-05-26 04:44:06.279503: step 250, loss = 1.68 (1032.4 examples/sec; 0.124 sec/batch)
2016-05-26 04:44:08.778952: step 260, loss = 1.46 (1009.2 examples/sec; 0.127 sec/batch)
2016-05-26 04:44:11.278876: step 270, loss = 1.50 (1001.2 examples/sec; 0.128 sec/batch)
2016-05-26 04:44:13.782676: step 280, loss = 1.41 (999.7 examples/sec; 0.128 sec/batch)
2016-05-26 04:44:16.285152: step 290, loss = 1.55 (1077.4 examples/sec; 0.119 sec/batch)
2016-05-26 04:44:18.802854: step 300, loss = 1.48 (988.7 examples/sec; 0.129 sec/batch)
2016-05-26 04:44:21.658185: step 310, loss = 1.46 (988.9 examples/sec; 0.129 sec/batch)
2016-05-26 04:44:24.182263: step 320, loss = 1.46 (1005.4 examples/sec; 0.127 sec/batch)
2016-05-26 04:44:26.726376: step 330, loss = 1.61 (1002.5 examples/sec; 0.128 sec/batch)
2016-05-26 04:44:29.213675: step 340, loss = 1.38 (1072.9 examples/sec; 0.119 sec/batch)
2016-05-26 04:44:31.714087: step 350, loss = 1.29 (1019.0 examples/sec; 0.126 sec/batch)
2016-05-26 04:44:34.199814: step 360, loss = 1.41 (1044.4 examples/sec; 0.123 sec/batch)
2016-05-26 04:44:36.736593: step 370, loss = 1.50 (1039.0 examples/sec; 0.123 sec/batch)
2016-05-26 04:44:39.301582: step 380, loss = 1.34 (1017.6 examples/sec; 0.126 sec/batch)
2016-05-26 04:44:41.807296: step 390, loss = 1.29 (1046.2 examples/sec; 0.122 sec/batch)
2016-05-26 04:44:44.331107: step 400, loss = 1.27 (1008.3 examples/sec; 0.127 sec/batch)
2016-05-26 04:44:47.192556: step 410, loss = 1.38 (1025.5 examples/sec; 0.125 sec/batch)
2016-05-26 04:44:49.753045: step 420, loss = 1.26 (1011.0 examples/sec; 0.127 sec/batch)
2016-05-26 04:44:52.271827: step 430, loss = 1.16 (978.3 examples/sec; 0.131 sec/batch)
2016-05-26 04:44:54.817926: step 440, loss = 1.26 (970.0 examples/sec; 0.132 sec/batch)
2016-05-26 04:44:57.337688: step 450, loss = 1.28 (1054.5 examples/sec; 0.121 sec/batch)
2016-05-26 04:44:59.875483: step 460, loss = 1.15 (962.6 examples/sec; 0.133 sec/batch)
2016-05-26 04:45:02.422892: step 470, loss = 1.49 (1061.0 examples/sec; 0.121 sec/batch)
2016-05-26 04:45:04.891274: step 480, loss = 1.19 (999.4 examples/sec; 0.128 sec/batch)
2016-05-26 04:45:07.421323: step 490, loss = 1.28 (970.0 examples/sec; 0.132 sec/batch)
2016-05-26 04:45:09.918576: step 500, loss = 1.12 (1013.0 examples/sec; 0.126 sec/batch)
2016-05-26 04:45:12.740661: step 510, loss = 1.26 (1028.4 examples/sec; 0.124 sec/batch)
2016-05-26 04:45:15.234976: step 520, loss = 1.31 (1024.6 examples/sec; 0.125 sec/batch)
2016-05-26 04:45:17.775198: step 530, loss = 1.36 (962.7 examples/sec; 0.133 sec/batch)
2016-05-26 04:45:20.370402: step 540, loss = 1.12 (1034.6 examples/sec; 0.124 sec/batch)
2016-05-26 04:45:22.941709: step 550, loss = 1.15 (979.1 examples/sec; 0.131 sec/batch)
2016-05-26 04:45:25.421456: step 560, loss = 1.33 (1055.4 examples/sec; 0.121 sec/batch)
2016-05-26 04:45:28.007632: step 570, loss = 1.18 (981.7 examples/sec; 0.130 sec/batch)
2016-05-26 04:45:30.512522: step 580, loss = 1.12 (1048.7 examples/sec; 0.122 sec/batch)
2016-05-26 04:45:33.015523: step 590, loss = 1.10 (1034.2 examples/sec; 0.124 sec/batch)
2016-05-26 04:45:35.523036: step 600, loss = 1.07 (1029.2 examples/sec; 0.124 sec/batch)
2016-05-26 04:45:38.407961: step 610, loss = 1.11 (957.1 examples/sec; 0.134 sec/batch)
2016-05-26 04:45:40.935426: step 620, loss = 1.15 (1015.7 examples/sec; 0.126 sec/batch)
2016-05-26 04:45:43.441078: step 630, loss = 1.07 (1082.6 examples/sec; 0.118 sec/batch)
2016-05-26 04:45:45.949200: step 640, loss = 1.11 (1019.9 examples/sec; 0.126 sec/batch)
2016-05-26 04:45:48.438801: step 650, loss = 1.21 (999.2 examples/sec; 0.128 sec/batch)
2016-05-26 04:45:50.985684: step 660, loss = 1.05 (1003.2 examples/sec; 0.128 sec/batch)
2016-05-26 04:45:53.556396: step 670, loss = 1.15 (1019.5 examples/sec; 0.126 sec/batch)
2016-05-26 04:45:56.039963: step 680, loss = 1.10 (1030.7 examples/sec; 0.124 sec/batch)
2016-05-26 04:45:58.582736: step 690, loss = 1.09 (971.7 examples/sec; 0.132 sec/batch)
2016-05-26 04:46:01.087211: step 700, loss = 1.01 (1051.3 examples/sec; 0.122 sec/batch)
2016-05-26 04:46:03.915575: step 710, loss = 0.96 (1030.8 examples/sec; 0.124 sec/batch)
2016-05-26 04:46:06.433943: step 720, loss = 1.14 (1005.4 examples/sec; 0.127 sec/batch)
2016-05-26 04:46:08.944442: step 730, loss = 0.93 (1042.3 examples/sec; 0.123 sec/batch)
2016-05-26 04:46:11.494571: step 740, loss = 1.05 (977.3 examples/sec; 0.131 sec/batch)
2016-05-26 04:46:13.980142: step 750, loss = 0.97 (1040.8 examples/sec; 0.123 sec/batch)
2016-05-26 04:46:16.541533: step 760, loss = 1.15 (964.5 examples/sec; 0.133 sec/batch)
2016-05-26 04:46:19.041722: step 770, loss = 1.09 (1011.8 examples/sec; 0.127 sec/batch)
2016-05-26 04:46:21.588323: step 780, loss = 0.89 (1006.5 examples/sec; 0.127 sec/batch)
2016-05-26 04:46:24.110478: step 790, loss = 0.98 (980.4 examples/sec; 0.131 sec/batch)
2016-05-26 04:46:26.633703: step 800, loss = 1.06 (1042.0 examples/sec; 0.123 sec/batch)
2016-05-26 04:46:29.422467: step 810, loss = 1.04 (1072.4 examples/sec; 0.119 sec/batch)
2016-05-26 04:46:31.920024: step 820, loss = 1.00 (1030.7 examples/sec; 0.124 sec/batch)
2016-05-26 04:46:34.439805: step 830, loss = 1.13 (1070.1 examples/sec; 0.120 sec/batch)
2016-05-26 04:46:36.898154: step 840, loss = 0.80 (1011.7 examples/sec; 0.127 sec/batch)
2016-05-26 04:46:39.350679: step 850, loss = 0.98 (1114.8 examples/sec; 0.115 sec/batch)
2016-05-26 04:46:41.848462: step 860, loss = 0.97 (1009.6 examples/sec; 0.127 sec/batch)
2016-05-26 04:46:44.331460: step 870, loss = 0.96 (1026.2 examples/sec; 0.125 sec/batch)
2016-05-26 04:46:46.845648: step 880, loss = 0.93 (1021.3 examples/sec; 0.125 sec/batch)
2016-05-26 04:46:49.345961: step 890, loss = 1.03 (1086.0 examples/sec; 0.118 sec/batch)
2016-05-26 04:46:51.893770: step 900, loss = 0.95 (1022.4 examples/sec; 0.125 sec/batch)
2016-05-26 04:46:54.747627: step 910, loss = 0.84 (1012.7 examples/sec; 0.126 sec/batch)
2016-05-26 04:46:57.246631: step 920, loss = 0.96 (1052.1 examples/sec; 0.122 sec/batch)
2016-05-26 04:46:59.773782: step 930, loss = 1.04 (1011.6 examples/sec; 0.127 sec/batch)
2016-05-26 04:47:02.286801: step 940, loss = 0.80 (1038.3 examples/sec; 0.123 sec/batch)
2016-05-26 04:47:04.754881: step 950, loss = 0.99 (1007.4 examples/sec; 0.127 sec/batch)
2016-05-26 04:47:07.273712: step 960, loss = 0.94 (1030.3 examples/sec; 0.124 sec/batch)
2016-05-26 04:47:09.788673: step 970, loss = 0.96 (1054.2 examples/sec; 0.121 sec/batch)
2016-05-26 04:47:12.313411: step 980, loss = 0.94 (980.6 examples/sec; 0.131 sec/batch)
2016-05-26 04:47:14.875105: step 990, loss = 0.94 (1026.9 examples/sec; 0.125 sec/batch)
2016-05-26 04:47:17.378117: step 1000, loss = 0.99 (1080.1 examples/sec; 0.119 sec/batch)
eval once
2016-05-26 04:47:59.576516: accuracy @ 1 = 0.662, 33114 / 50048 at 0
2016-05-26 04:48:02.189047: step 1010, loss = 0.79 (964.5 examples/sec; 0.133 sec/batch)
2016-05-26 04:48:04.671820: step 1020, loss = 0.89 (1023.8 examples/sec; 0.125 sec/batch)
2016-05-26 04:48:07.126744: step 1030, loss = 1.04 (969.5 examples/sec; 0.132 sec/batch)
2016-05-26 04:48:09.643439: step 1040, loss = 0.90 (1008.2 examples/sec; 0.127 sec/batch)
2016-05-26 04:48:12.101862: step 1050, loss = 0.97 (1004.2 examples/sec; 0.127 sec/batch)
2016-05-26 04:48:14.623910: step 1060, loss = 0.94 (963.8 examples/sec; 0.133 sec/batch)
2016-05-26 04:48:17.140836: step 1070, loss = 0.87 (1011.1 examples/sec; 0.127 sec/batch)
2016-05-26 04:48:19.654100: step 1080, loss = 0.82 (1086.0 examples/sec; 0.118 sec/batch)
2016-05-26 04:48:22.117385: step 1090, loss = 1.06 (992.7 examples/sec; 0.129 sec/batch)
2016-05-26 04:48:24.574322: step 1100, loss = 0.90 (1061.9 examples/sec; 0.121 sec/batch)
2016-05-26 04:48:27.451304: step 1110, loss = 0.80 (1000.3 examples/sec; 0.128 sec/batch)
2016-05-26 04:48:29.964597: step 1120, loss = 0.98 (1051.4 examples/sec; 0.122 sec/batch)
2016-05-26 04:48:32.472876: step 1130, loss = 1.11 (1007.8 examples/sec; 0.127 sec/batch)
2016-05-26 04:48:35.046776: step 1140, loss = 0.74 (964.6 examples/sec; 0.133 sec/batch)
2016-05-26 04:48:37.555581: step 1150, loss = 0.73 (1085.8 examples/sec; 0.118 sec/batch)
2016-05-26 04:48:40.100545: step 1160, loss = 0.93 (959.1 examples/sec; 0.133 sec/batch)
2016-05-26 04:48:42.603332: step 1170, loss = 0.85 (1062.7 examples/sec; 0.120 sec/batch)
2016-05-26 04:48:45.048103: step 1180, loss = 0.74 (1039.5 examples/sec; 0.123 sec/batch)
2016-05-26 04:48:47.554272: step 1190, loss = 0.90 (991.6 examples/sec; 0.129 sec/batch)
2016-05-26 04:48:50.105540: step 1200, loss = 0.86 (1054.8 examples/sec; 0.121 sec/batch)
2016-05-26 04:48:52.954592: step 1210, loss = 0.75 (1020.5 examples/sec; 0.125 sec/batch)
2016-05-26 04:48:55.516849: step 1220, loss = 0.80 (984.7 examples/sec; 0.130 sec/batch)
2016-05-26 04:48:58.074209: step 1230, loss = 0.93 (999.6 examples/sec; 0.128 sec/batch)
2016-05-26 04:49:00.533864: step 1240, loss = 0.85 (947.2 examples/sec; 0.135 sec/batch)
2016-05-26 04:49:03.048045: step 1250, loss = 1.02 (1060.8 examples/sec; 0.121 sec/batch)
2016-05-26 04:49:05.608521: step 1260, loss = 0.97 (984.3 examples/sec; 0.130 sec/batch)
2016-05-26 04:49:08.094833: step 1270, loss = 0.84 (1067.9 examples/sec; 0.120 sec/batch)
2016-05-26 04:49:10.585423: step 1280, loss = 0.96 (1015.8 examples/sec; 0.126 sec/batch)
2016-05-26 04:49:13.081449: step 1290, loss = 0.87 (975.7 examples/sec; 0.131 sec/batch)
2016-05-26 04:49:15.659010: step 1300, loss = 0.85 (974.6 examples/sec; 0.131 sec/batch)
2016-05-26 04:49:18.501455: step 1310, loss = 1.05 (1093.5 examples/sec; 0.117 sec/batch)
2016-05-26 04:49:20.989836: step 1320, loss = 0.94 (1006.9 examples/sec; 0.127 sec/batch)
2016-05-26 04:49:23.509458: step 1330, loss = 0.81 (991.7 examples/sec; 0.129 sec/batch)
2016-05-26 04:49:26.063863: step 1340, loss = 0.84 (985.1 examples/sec; 0.130 sec/batch)
2016-05-26 04:49:28.557438: step 1350, loss = 0.87 (1106.5 examples/sec; 0.116 sec/batch)
2016-05-26 04:49:31.107215: step 1360, loss = 0.90 (1016.4 examples/sec; 0.126 sec/batch)
2016-05-26 04:49:33.649115: step 1370, loss = 0.83 (1020.5 examples/sec; 0.125 sec/batch)
2016-05-26 04:49:36.210174: step 1380, loss = 0.81 (957.0 examples/sec; 0.134 sec/batch)
2016-05-26 04:49:38.703412: step 1390, loss = 0.62 (1034.6 examples/sec; 0.124 sec/batch)
2016-05-26 04:49:41.277090: step 1400, loss = 0.94 (981.5 examples/sec; 0.130 sec/batch)
2016-05-26 04:49:44.137055: step 1410, loss = 0.81 (981.1 examples/sec; 0.130 sec/batch)
2016-05-26 04:49:46.641976: step 1420, loss = 0.68 (1061.0 examples/sec; 0.121 sec/batch)
2016-05-26 04:49:49.194572: step 1430, loss = 0.95 (1005.8 examples/sec; 0.127 sec/batch)
2016-05-26 04:49:51.740043: step 1440, loss = 0.75 (1057.8 examples/sec; 0.121 sec/batch)
2016-05-26 04:49:54.272059: step 1450, loss = 0.82 (965.9 examples/sec; 0.133 sec/batch)
2016-05-26 04:49:56.820239: step 1460, loss = 0.69 (973.8 examples/sec; 0.131 sec/batch)
2016-05-26 04:49:59.342844: step 1470, loss = 0.74 (1034.6 examples/sec; 0.124 sec/batch)
2016-05-26 04:50:01.884038: step 1480, loss = 0.59 (1064.7 examples/sec; 0.120 sec/batch)
2016-05-26 04:50:04.438057: step 1490, loss = 0.67 (1012.3 examples/sec; 0.126 sec/batch)
2016-05-26 04:50:06.925669: step 1500, loss = 0.92 (1048.0 examples/sec; 0.122 sec/batch)
2016-05-26 04:50:09.680053: step 1510, loss = 0.91 (1086.7 examples/sec; 0.118 sec/batch)
2016-05-26 04:50:12.178964: step 1520, loss = 0.75 (1047.0 examples/sec; 0.122 sec/batch)
2016-05-26 04:50:14.685486: step 1530, loss = 0.75 (1016.5 examples/sec; 0.126 sec/batch)
2016-05-26 04:50:17.155417: step 1540, loss = 0.66 (1024.6 examples/sec; 0.125 sec/batch)
2016-05-26 04:50:19.646287: step 1550, loss = 0.71 (1037.6 examples/sec; 0.123 sec/batch)
2016-05-26 04:50:22.133502: step 1560, loss = 0.74 (1007.7 examples/sec; 0.127 sec/batch)
2016-05-26 04:50:24.697741: step 1570, loss = 0.82 (1062.1 examples/sec; 0.121 sec/batch)
2016-05-26 04:50:27.164788: step 1580, loss = 0.89 (1057.7 examples/sec; 0.121 sec/batch)
2016-05-26 04:50:29.740645: step 1590, loss = 0.86 (1022.8 examples/sec; 0.125 sec/batch)
2016-05-26 04:50:32.271145: step 1600, loss = 0.76 (1028.2 examples/sec; 0.124 sec/batch)
2016-05-26 04:50:35.074280: step 1610, loss = 0.64 (981.8 examples/sec; 0.130 sec/batch)
2016-05-26 04:50:37.578640: step 1620, loss = 0.75 (1079.9 examples/sec; 0.119 sec/batch)
2016-05-26 04:50:40.128369: step 1630, loss = 1.02 (1037.3 examples/sec; 0.123 sec/batch)
2016-05-26 04:50:42.616326: step 1640, loss = 0.72 (1036.2 examples/sec; 0.124 sec/batch)
2016-05-26 04:50:45.172962: step 1650, loss = 0.81 (1030.2 examples/sec; 0.124 sec/batch)
2016-05-26 04:50:47.695259: step 1660, loss = 0.82 (1023.2 examples/sec; 0.125 sec/batch)
2016-05-26 04:50:50.196632: step 1670, loss = 0.67 (1070.4 examples/sec; 0.120 sec/batch)
2016-05-26 04:50:52.706393: step 1680, loss = 0.71 (933.4 examples/sec; 0.137 sec/batch)
2016-05-26 04:50:55.210853: step 1690, loss = 0.69 (980.9 examples/sec; 0.130 sec/batch)
2016-05-26 04:50:57.670417: step 1700, loss = 0.66 (1013.5 examples/sec; 0.126 sec/batch)
2016-05-26 04:51:00.530261: step 1710, loss = 0.73 (974.3 examples/sec; 0.131 sec/batch)
2016-05-26 04:51:03.058630: step 1720, loss = 0.72 (986.2 examples/sec; 0.130 sec/batch)
2016-05-26 04:51:05.542852: step 1730, loss = 0.67 (1017.4 examples/sec; 0.126 sec/batch)
2016-05-26 04:51:08.013413: step 1740, loss = 0.86 (1051.4 examples/sec; 0.122 sec/batch)
2016-05-26 04:51:10.531820: step 1750, loss = 0.85 (1015.5 examples/sec; 0.126 sec/batch)
2016-05-26 04:51:13.007054: step 1760, loss = 0.68 (1047.2 examples/sec; 0.122 sec/batch)
2016-05-26 04:51:15.509985: step 1770, loss = 0.68 (977.3 examples/sec; 0.131 sec/batch)
2016-05-26 04:51:18.065492: step 1780, loss = 0.66 (1047.8 examples/sec; 0.122 sec/batch)
2016-05-26 04:51:20.554482: step 1790, loss = 0.68 (1042.8 examples/sec; 0.123 sec/batch)
2016-05-26 04:51:23.047082: step 1800, loss = 0.69 (1004.0 examples/sec; 0.127 sec/batch)
2016-05-26 04:51:25.846801: step 1810, loss = 0.70 (979.3 examples/sec; 0.131 sec/batch)
2016-05-26 04:51:28.367793: step 1820, loss = 0.75 (1012.6 examples/sec; 0.126 sec/batch)
2016-05-26 04:51:30.883446: step 1830, loss = 0.72 (1045.2 examples/sec; 0.122 sec/batch)
2016-05-26 04:51:33.380985: step 1840, loss = 0.76 (992.1 examples/sec; 0.129 sec/batch)
2016-05-26 04:51:35.927039: step 1850, loss = 0.59 (1024.5 examples/sec; 0.125 sec/batch)
2016-05-26 04:51:38.391190: step 1860, loss = 0.70 (1032.7 examples/sec; 0.124 sec/batch)
2016-05-26 04:51:40.893135: step 1870, loss = 0.70 (1007.2 examples/sec; 0.127 sec/batch)
2016-05-26 04:51:43.402364: step 1880, loss = 0.74 (999.2 examples/sec; 0.128 sec/batch)
2016-05-26 04:51:45.911082: step 1890, loss = 0.63 (1078.5 examples/sec; 0.119 sec/batch)
2016-05-26 04:51:48.461313: step 1900, loss = 0.71 (997.6 examples/sec; 0.128 sec/batch)
2016-05-26 04:51:51.259855: step 1910, loss = 0.87 (1046.4 examples/sec; 0.122 sec/batch)
2016-05-26 04:51:53.759218: step 1920, loss = 0.83 (1032.4 examples/sec; 0.124 sec/batch)
2016-05-26 04:51:56.225939: step 1930, loss = 0.88 (1071.3 examples/sec; 0.119 sec/batch)
2016-05-26 04:51:58.770314: step 1940, loss = 0.76 (1016.8 examples/sec; 0.126 sec/batch)
2016-05-26 04:52:01.312781: step 1950, loss = 0.72 (956.7 examples/sec; 0.134 sec/batch)
2016-05-26 04:52:03.830508: step 1960, loss = 0.74 (993.9 examples/sec; 0.129 sec/batch)
2016-05-26 04:52:06.403104: step 1970, loss = 0.66 (951.8 examples/sec; 0.134 sec/batch)
2016-05-26 04:52:08.921495: step 1980, loss = 0.74 (1029.9 examples/sec; 0.124 sec/batch)
2016-05-26 04:52:11.412365: step 1990, loss = 0.79 (972.5 examples/sec; 0.132 sec/batch)
2016-05-26 04:52:13.931852: step 2000, loss = 0.63 (952.3 examples/sec; 0.134 sec/batch)
eval once
2016-05-26 04:52:56.134868: accuracy @ 1 = 0.762, 38127 / 50048 at 0
2016-05-26 04:52:58.743678: step 2010, loss = 0.66 (988.7 examples/sec; 0.129 sec/batch)
2016-05-26 04:53:01.235318: step 2020, loss = 0.67 (966.3 examples/sec; 0.132 sec/batch)
2016-05-26 04:53:03.744217: step 2030, loss = 0.68 (1010.0 examples/sec; 0.127 sec/batch)
2016-05-26 04:53:06.245216: step 2040, loss = 0.67 (1047.6 examples/sec; 0.122 sec/batch)
2016-05-26 04:53:08.761480: step 2050, loss = 0.66 (1038.5 examples/sec; 0.123 sec/batch)
2016-05-26 04:53:11.273916: step 2060, loss = 0.72 (1024.9 examples/sec; 0.125 sec/batch)
2016-05-26 04:53:13.788093: step 2070, loss = 0.77 (965.0 examples/sec; 0.133 sec/batch)
2016-05-26 04:53:16.227896: step 2080, loss = 0.67 (1085.2 examples/sec; 0.118 sec/batch)
2016-05-26 04:53:18.665494: step 2090, loss = 0.74 (976.3 examples/sec; 0.131 sec/batch)
2016-05-26 04:53:21.205651: step 2100, loss = 0.64 (1002.0 examples/sec; 0.128 sec/batch)
2016-05-26 04:53:24.006749: step 2110, loss = 0.68 (1087.5 examples/sec; 0.118 sec/batch)
2016-05-26 04:53:26.556031: step 2120, loss = 0.65 (1042.0 examples/sec; 0.123 sec/batch)
2016-05-26 04:53:29.116504: step 2130, loss = 0.81 (932.6 examples/sec; 0.137 sec/batch)
2016-05-26 04:53:31.678066: step 2140, loss = 0.64 (1015.8 examples/sec; 0.126 sec/batch)
2016-05-26 04:53:34.208662: step 2150, loss = 0.49 (1007.3 examples/sec; 0.127 sec/batch)
2016-05-26 04:53:36.718631: step 2160, loss = 0.62 (992.7 examples/sec; 0.129 sec/batch)
2016-05-26 04:53:39.219987: step 2170, loss = 0.77 (1046.1 examples/sec; 0.122 sec/batch)
2016-05-26 04:53:41.752329: step 2180, loss = 0.69 (1044.2 examples/sec; 0.123 sec/batch)
2016-05-26 04:53:44.248187: step 2190, loss = 0.68 (1044.8 examples/sec; 0.123 sec/batch)
2016-05-26 04:53:46.787858: step 2200, loss = 0.61 (1052.6 examples/sec; 0.122 sec/batch)
2016-05-26 04:53:49.616808: step 2210, loss = 0.64 (1038.3 examples/sec; 0.123 sec/batch)
2016-05-26 04:53:52.144262: step 2220, loss = 0.67 (1087.4 examples/sec; 0.118 sec/batch)
2016-05-26 04:53:54.603155: step 2230, loss = 0.66 (1003.3 examples/sec; 0.128 sec/batch)
2016-05-26 04:53:57.093639: step 2240, loss = 0.74 (1051.6 examples/sec; 0.122 sec/batch)
2016-05-26 04:53:59.606070: step 2250, loss = 0.72 (1049.0 examples/sec; 0.122 sec/batch)
2016-05-26 04:54:02.092851: step 2260, loss = 0.60 (1053.9 examples/sec; 0.121 sec/batch)
2016-05-26 04:54:04.590513: step 2270, loss = 0.71 (1009.1 examples/sec; 0.127 sec/batch)
2016-05-26 04:54:07.128791: step 2280, loss = 0.79 (965.4 examples/sec; 0.133 sec/batch)
2016-05-26 04:54:09.659923: step 2290, loss = 0.55 (1032.9 examples/sec; 0.124 sec/batch)
2016-05-26 04:54:12.117455: step 2300, loss = 0.87 (1031.3 examples/sec; 0.124 sec/batch)
2016-05-26 04:54:14.901898: step 2310, loss = 0.77 (1106.1 examples/sec; 0.116 sec/batch)
2016-05-26 04:54:17.367331: step 2320, loss = 0.69 (980.5 examples/sec; 0.131 sec/batch)
2016-05-26 04:54:19.836238: step 2330, loss = 0.56 (1084.8 examples/sec; 0.118 sec/batch)
2016-05-26 04:54:22.360339: step 2340, loss = 0.63 (1045.5 examples/sec; 0.122 sec/batch)
2016-05-26 04:54:24.891883: step 2350, loss = 0.68 (1034.6 examples/sec; 0.124 sec/batch)
2016-05-26 04:54:27.381441: step 2360, loss = 0.57 (1007.2 examples/sec; 0.127 sec/batch)
2016-05-26 04:54:29.911547: step 2370, loss = 0.63 (956.4 examples/sec; 0.134 sec/batch)
2016-05-26 04:54:32.394528: step 2380, loss = 0.61 (1013.3 examples/sec; 0.126 sec/batch)
2016-05-26 04:54:34.919017: step 2390, loss = 0.56 (998.8 examples/sec; 0.128 sec/batch)
2016-05-26 04:54:37.435924: step 2400, loss = 0.64 (1029.9 examples/sec; 0.124 sec/batch)
2016-05-26 04:54:40.254909: step 2410, loss = 0.55 (1057.5 examples/sec; 0.121 sec/batch)
2016-05-26 04:54:42.774082: step 2420, loss = 0.70 (1037.7 examples/sec; 0.123 sec/batch)
2016-05-26 04:54:45.275262: step 2430, loss = 0.58 (999.2 examples/sec; 0.128 sec/batch)
2016-05-26 04:54:47.766771: step 2440, loss = 0.71 (996.1 examples/sec; 0.128 sec/batch)
2016-05-26 04:54:50.233420: step 2450, loss = 0.68 (1034.8 examples/sec; 0.124 sec/batch)
2016-05-26 04:54:52.757794: step 2460, loss = 0.56 (1054.8 examples/sec; 0.121 sec/batch)
2016-05-26 04:54:55.269048: step 2470, loss = 0.77 (1015.3 examples/sec; 0.126 sec/batch)
2016-05-26 04:54:57.695683: step 2480, loss = 0.82 (1064.2 examples/sec; 0.120 sec/batch)
2016-05-26 04:55:00.188615: step 2490, loss = 0.61 (980.6 examples/sec; 0.131 sec/batch)
2016-05-26 04:55:02.686651: step 2500, loss = 0.45 (1006.4 examples/sec; 0.127 sec/batch)
2016-05-26 04:55:05.465823: step 2510, loss = 0.69 (1004.0 examples/sec; 0.127 sec/batch)
2016-05-26 04:55:07.967339: step 2520, loss = 0.60 (1067.8 examples/sec; 0.120 sec/batch)
2016-05-26 04:55:10.480793: step 2530, loss = 0.69 (1005.7 examples/sec; 0.127 sec/batch)
2016-05-26 04:55:12.918222: step 2540, loss = 0.56 (1059.5 examples/sec; 0.121 sec/batch)
2016-05-26 04:55:15.412138: step 2550, loss = 0.72 (1012.5 examples/sec; 0.126 sec/batch)
2016-05-26 04:55:17.873294: step 2560, loss = 0.60 (1072.7 examples/sec; 0.119 sec/batch)
2016-05-26 04:55:20.377035: step 2570, loss = 0.76 (1098.4 examples/sec; 0.117 sec/batch)
2016-05-26 04:55:22.872277: step 2580, loss = 0.66 (1003.7 examples/sec; 0.128 sec/batch)
2016-05-26 04:55:25.408942: step 2590, loss = 0.59 (1006.6 examples/sec; 0.127 sec/batch)
2016-05-26 04:55:27.934106: step 2600, loss = 0.59 (1012.2 examples/sec; 0.126 sec/batch)
2016-05-26 04:55:30.745698: step 2610, loss = 0.47 (1034.7 examples/sec; 0.124 sec/batch)
2016-05-26 04:55:33.236318: step 2620, loss = 0.62 (1028.5 examples/sec; 0.124 sec/batch)
2016-05-26 04:55:35.675061: step 2630, loss = 0.58 (1033.6 examples/sec; 0.124 sec/batch)
2016-05-26 04:55:38.213946: step 2640, loss = 0.57 (1010.3 examples/sec; 0.127 sec/batch)
2016-05-26 04:55:40.710539: step 2650, loss = 0.54 (1004.3 examples/sec; 0.127 sec/batch)
2016-05-26 04:55:43.162193: step 2660, loss = 0.58 (1070.1 examples/sec; 0.120 sec/batch)
2016-05-26 04:55:45.703007: step 2670, loss = 0.72 (975.6 examples/sec; 0.131 sec/batch)
2016-05-26 04:55:48.162262: step 2680, loss = 0.81 (986.0 examples/sec; 0.130 sec/batch)
2016-05-26 04:55:50.675982: step 2690, loss = 0.56 (1062.4 examples/sec; 0.120 sec/batch)
2016-05-26 04:55:53.221798: step 2700, loss = 0.59 (1061.4 examples/sec; 0.121 sec/batch)
2016-05-26 04:55:56.037338: step 2710, loss = 0.69 (1106.5 examples/sec; 0.116 sec/batch)
2016-05-26 04:55:58.536179: step 2720, loss = 0.50 (1047.2 examples/sec; 0.122 sec/batch)
2016-05-26 04:56:01.054478: step 2730, loss = 0.61 (995.5 examples/sec; 0.129 sec/batch)
2016-05-26 04:56:03.558526: step 2740, loss = 0.57 (1013.3 examples/sec; 0.126 sec/batch)
2016-05-26 04:56:06.070428: step 2750, loss = 0.50 (959.2 examples/sec; 0.133 sec/batch)
2016-05-26 04:56:08.612297: step 2760, loss = 0.76 (972.9 examples/sec; 0.132 sec/batch)
2016-05-26 04:56:11.145894: step 2770, loss = 0.56 (976.8 examples/sec; 0.131 sec/batch)
2016-05-26 04:56:13.681273: step 2780, loss = 0.56 (1034.6 examples/sec; 0.124 sec/batch)
2016-05-26 04:56:16.160580: step 2790, loss = 0.76 (989.4 examples/sec; 0.129 sec/batch)
2016-05-26 04:56:18.679375: step 2800, loss = 0.63 (1070.7 examples/sec; 0.120 sec/batch)
2016-05-26 04:56:21.472185: step 2810, loss = 0.51 (1000.6 examples/sec; 0.128 sec/batch)
2016-05-26 04:56:23.969875: step 2820, loss = 0.64 (1041.1 examples/sec; 0.123 sec/batch)
2016-05-26 04:56:26.474272: step 2830, loss = 0.50 (1027.3 examples/sec; 0.125 sec/batch)
2016-05-26 04:56:29.009750: step 2840, loss = 0.55 (968.0 examples/sec; 0.132 sec/batch)
2016-05-26 04:56:31.502544: step 2850, loss = 0.79 (1101.2 examples/sec; 0.116 sec/batch)
2016-05-26 04:56:33.966071: step 2860, loss = 0.44 (1005.9 examples/sec; 0.127 sec/batch)
2016-05-26 04:56:36.465333: step 2870, loss = 0.57 (998.5 examples/sec; 0.128 sec/batch)
2016-05-26 04:56:38.895507: step 2880, loss = 0.67 (1096.1 examples/sec; 0.117 sec/batch)
2016-05-26 04:56:41.391825: step 2890, loss = 0.57 (1081.8 examples/sec; 0.118 sec/batch)
2016-05-26 04:56:43.847575: step 2900, loss = 0.60 (1045.8 examples/sec; 0.122 sec/batch)
2016-05-26 04:56:46.664067: step 2910, loss = 0.51 (1027.1 examples/sec; 0.125 sec/batch)
2016-05-26 04:56:49.152684: step 2920, loss = 0.64 (995.4 examples/sec; 0.129 sec/batch)
2016-05-26 04:56:51.694967: step 2930, loss = 0.59 (1017.9 examples/sec; 0.126 sec/batch)
2016-05-26 04:56:54.191617: step 2940, loss = 0.55 (979.2 examples/sec; 0.131 sec/batch)
2016-05-26 04:56:56.725057: step 2950, loss = 0.61 (997.5 examples/sec; 0.128 sec/batch)
2016-05-26 04:56:59.173090: step 2960, loss = 0.55 (1013.3 examples/sec; 0.126 sec/batch)
2016-05-26 04:57:01.721339: step 2970, loss = 0.60 (965.1 examples/sec; 0.133 sec/batch)
2016-05-26 04:57:04.245282: step 2980, loss = 0.62 (1033.4 examples/sec; 0.124 sec/batch)
2016-05-26 04:57:06.721233: step 2990, loss = 0.47 (988.6 examples/sec; 0.129 sec/batch)
2016-05-26 04:57:09.216454: step 3000, loss = 0.55 (1094.2 examples/sec; 0.117 sec/batch)
eval once
2016-05-26 04:57:50.799587: accuracy @ 1 = 0.790, 39520 / 50048 at 0
2016-05-26 04:57:53.311201: step 3010, loss = 0.33 (979.8 examples/sec; 0.131 sec/batch)
2016-05-26 04:57:55.796243: step 3020, loss = 0.49 (1103.7 examples/sec; 0.116 sec/batch)
2016-05-26 04:57:58.255861: step 3030, loss = 0.53 (1029.0 examples/sec; 0.124 sec/batch)
2016-05-26 04:58:00.747637: step 3040, loss = 0.47 (975.2 examples/sec; 0.131 sec/batch)
2016-05-26 04:58:03.278079: step 3050, loss = 0.69 (1059.2 examples/sec; 0.121 sec/batch)
2016-05-26 04:58:05.733876: step 3060, loss = 0.47 (1030.2 examples/sec; 0.124 sec/batch)
2016-05-26 04:58:08.258145: step 3070, loss = 0.43 (1077.2 examples/sec; 0.119 sec/batch)
2016-05-26 04:58:10.737627: step 3080, loss = 0.47 (1000.4 examples/sec; 0.128 sec/batch)
2016-05-26 04:58:13.225072: step 3090, loss = 0.60 (1009.8 examples/sec; 0.127 sec/batch)
2016-05-26 04:58:15.707528: step 3100, loss = 0.63 (1103.4 examples/sec; 0.116 sec/batch)
2016-05-26 04:58:18.533835: step 3110, loss = 0.53 (1087.7 examples/sec; 0.118 sec/batch)
2016-05-26 04:58:21.099426: step 3120, loss = 0.53 (1071.7 examples/sec; 0.119 sec/batch)
2016-05-26 04:58:23.528935: step 3130, loss = 0.55 (1035.4 examples/sec; 0.124 sec/batch)
2016-05-26 04:58:26.051707: step 3140, loss = 0.66 (1057.3 examples/sec; 0.121 sec/batch)
2016-05-26 04:58:28.533296: step 3150, loss = 0.47 (996.2 examples/sec; 0.128 sec/batch)
2016-05-26 04:58:30.958335: step 3160, loss = 0.55 (1043.3 examples/sec; 0.123 sec/batch)
2016-05-26 04:58:33.460726: step 3170, loss = 0.65 (1007.3 examples/sec; 0.127 sec/batch)
2016-05-26 04:58:35.996224: step 3180, loss = 0.44 (1009.4 examples/sec; 0.127 sec/batch)
2016-05-26 04:58:38.506413: step 3190, loss = 0.44 (1031.4 examples/sec; 0.124 sec/batch)
2016-05-26 04:58:41.001609: step 3200, loss = 0.55 (983.3 examples/sec; 0.130 sec/batch)
2016-05-26 04:58:43.794894: step 3210, loss = 0.45 (1066.2 examples/sec; 0.120 sec/batch)
2016-05-26 04:58:46.276640: step 3220, loss = 0.54 (1075.3 examples/sec; 0.119 sec/batch)
2016-05-26 04:58:48.861853: step 3230, loss = 0.66 (1031.8 examples/sec; 0.124 sec/batch)
2016-05-26 04:58:51.367842: step 3240, loss = 0.66 (927.5 examples/sec; 0.138 sec/batch)
2016-05-26 04:58:53.846592: step 3250, loss = 0.68 (1010.9 examples/sec; 0.127 sec/batch)
2016-05-26 04:58:56.321110: step 3260, loss = 0.66 (1039.6 examples/sec; 0.123 sec/batch)
2016-05-26 04:58:58.791748: step 3270, loss = 0.56 (976.3 examples/sec; 0.131 sec/batch)
2016-05-26 04:59:01.268913: step 3280, loss = 0.72 (962.8 examples/sec; 0.133 sec/batch)
2016-05-26 04:59:03.779448: step 3290, loss = 0.70 (1047.5 examples/sec; 0.122 sec/batch)
2016-05-26 04:59:06.308155: step 3300, loss = 0.44 (985.2 examples/sec; 0.130 sec/batch)
2016-05-26 04:59:09.128935: step 3310, loss = 0.72 (952.5 examples/sec; 0.134 sec/batch)
2016-05-26 04:59:11.656746: step 3320, loss = 0.50 (995.7 examples/sec; 0.129 sec/batch)
2016-05-26 04:59:14.125109: step 3330, loss = 0.62 (1064.2 examples/sec; 0.120 sec/batch)
2016-05-26 04:59:16.608202: step 3340, loss = 0.69 (1039.3 examples/sec; 0.123 sec/batch)
2016-05-26 04:59:19.100730: step 3350, loss = 0.57 (985.5 examples/sec; 0.130 sec/batch)
2016-05-26 04:59:21.580100: step 3360, loss = 0.43 (1010.4 examples/sec; 0.127 sec/batch)
2016-05-26 04:59:24.036942: step 3370, loss = 0.54 (1051.5 examples/sec; 0.122 sec/batch)
2016-05-26 04:59:26.486959: step 3380, loss = 0.69 (1058.9 examples/sec; 0.121 sec/batch)
2016-05-26 04:59:28.968755: step 3390, loss = 0.64 (1065.0 examples/sec; 0.120 sec/batch)
2016-05-26 04:59:31.461957: step 3400, loss = 0.42 (1021.7 examples/sec; 0.125 sec/batch)
2016-05-26 04:59:34.296949: step 3410, loss = 0.57 (1047.1 examples/sec; 0.122 sec/batch)
2016-05-26 04:59:36.822211: step 3420, loss = 0.69 (1011.5 examples/sec; 0.127 sec/batch)
2016-05-26 04:59:39.330306: step 3430, loss = 0.48 (1018.9 examples/sec; 0.126 sec/batch)
2016-05-26 04:59:41.762261: step 3440, loss = 0.63 (1071.1 examples/sec; 0.120 sec/batch)
2016-05-26 04:59:44.265653: step 3450, loss = 0.72 (1014.8 examples/sec; 0.126 sec/batch)
2016-05-26 04:59:46.794227: step 3460, loss = 0.59 (1064.2 examples/sec; 0.120 sec/batch)
2016-05-26 04:59:49.328426: step 3470, loss = 0.49 (1044.4 examples/sec; 0.123 sec/batch)
2016-05-26 04:59:51.833138: step 3480, loss = 0.64 (990.4 examples/sec; 0.129 sec/batch)
2016-05-26 04:59:54.321149: step 3490, loss = 0.48 (1045.6 examples/sec; 0.122 sec/batch)
2016-05-26 04:59:56.838685: step 3500, loss = 0.58 (1085.1 examples/sec; 0.118 sec/batch)
2016-05-26 04:59:59.630348: step 3510, loss = 0.54 (1034.9 examples/sec; 0.124 sec/batch)
2016-05-26 05:00:02.173853: step 3520, loss = 0.47 (967.6 examples/sec; 0.132 sec/batch)
2016-05-26 05:00:04.674167: step 3530, loss = 0.55 (1034.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:00:07.160938: step 3540, loss = 0.55 (1069.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:00:09.703538: step 3550, loss = 0.68 (984.6 examples/sec; 0.130 sec/batch)
2016-05-26 05:00:12.188277: step 3560, loss = 0.60 (970.7 examples/sec; 0.132 sec/batch)
2016-05-26 05:00:14.722036: step 3570, loss = 0.59 (1015.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:00:17.222374: step 3580, loss = 0.58 (1001.9 examples/sec; 0.128 sec/batch)
2016-05-26 05:00:19.701779: step 3590, loss = 0.53 (1102.4 examples/sec; 0.116 sec/batch)
2016-05-26 05:00:22.203216: step 3600, loss = 0.40 (1015.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:00:25.053286: step 3610, loss = 0.52 (954.9 examples/sec; 0.134 sec/batch)
2016-05-26 05:00:27.519156: step 3620, loss = 0.56 (1064.6 examples/sec; 0.120 sec/batch)
2016-05-26 05:00:30.012629: step 3630, loss = 0.42 (1007.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:00:32.502751: step 3640, loss = 0.51 (1017.1 examples/sec; 0.126 sec/batch)
2016-05-26 05:00:35.007704: step 3650, loss = 0.39 (1028.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:00:37.434820: step 3660, loss = 0.45 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:00:39.887830: step 3670, loss = 0.43 (1058.5 examples/sec; 0.121 sec/batch)
2016-05-26 05:00:42.363144: step 3680, loss = 0.56 (1030.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:00:44.809012: step 3690, loss = 0.50 (1023.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:00:47.294254: step 3700, loss = 0.56 (1064.1 examples/sec; 0.120 sec/batch)
2016-05-26 05:00:50.099400: step 3710, loss = 0.55 (1051.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:00:52.592049: step 3720, loss = 0.55 (1005.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:00:55.121537: step 3730, loss = 0.40 (1051.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:00:57.572232: step 3740, loss = 0.56 (1086.8 examples/sec; 0.118 sec/batch)
2016-05-26 05:01:00.082722: step 3750, loss = 0.50 (983.1 examples/sec; 0.130 sec/batch)
2016-05-26 05:01:02.611850: step 3760, loss = 0.40 (1066.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:01:05.133206: step 3770, loss = 0.62 (978.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:01:07.659059: step 3780, loss = 0.39 (1024.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:01:10.207033: step 3790, loss = 0.42 (1029.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:01:12.677877: step 3800, loss = 0.47 (992.5 examples/sec; 0.129 sec/batch)
2016-05-26 05:01:15.427340: step 3810, loss = 0.51 (1125.0 examples/sec; 0.114 sec/batch)
2016-05-26 05:01:17.888115: step 3820, loss = 0.56 (970.5 examples/sec; 0.132 sec/batch)
2016-05-26 05:01:20.440405: step 3830, loss = 0.41 (983.3 examples/sec; 0.130 sec/batch)
2016-05-26 05:01:22.948651: step 3840, loss = 0.49 (1048.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:01:25.437855: step 3850, loss = 0.58 (1098.8 examples/sec; 0.116 sec/batch)
2016-05-26 05:01:27.961670: step 3860, loss = 0.50 (1042.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:01:30.489026: step 3870, loss = 0.45 (983.2 examples/sec; 0.130 sec/batch)
2016-05-26 05:01:32.964762: step 3880, loss = 0.53 (1022.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:01:35.495120: step 3890, loss = 0.53 (1002.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:01:38.039874: step 3900, loss = 0.58 (976.1 examples/sec; 0.131 sec/batch)
2016-05-26 05:01:40.808779: step 3910, loss = 0.40 (976.1 examples/sec; 0.131 sec/batch)
2016-05-26 05:01:43.283351: step 3920, loss = 0.54 (1016.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:01:45.788210: step 3930, loss = 0.51 (984.3 examples/sec; 0.130 sec/batch)
2016-05-26 05:01:48.337962: step 3940, loss = 0.44 (1003.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:01:50.866920: step 3950, loss = 0.54 (964.2 examples/sec; 0.133 sec/batch)
2016-05-26 05:01:53.428221: step 3960, loss = 0.44 (973.9 examples/sec; 0.131 sec/batch)
2016-05-26 05:01:55.965379: step 3970, loss = 0.46 (979.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:01:58.515011: step 3980, loss = 0.50 (1024.7 examples/sec; 0.125 sec/batch)
2016-05-26 05:02:01.056705: step 3990, loss = 0.49 (1021.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:02:03.539600: step 4000, loss = 0.55 (1009.9 examples/sec; 0.127 sec/batch)
eval once
2016-05-26 05:02:45.542767: accuracy @ 1 = 0.827, 41395 / 50048 at 0
2016-05-26 05:02:48.078284: step 4010, loss = 0.65 (1010.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:02:50.556958: step 4020, loss = 0.46 (1098.8 examples/sec; 0.116 sec/batch)
2016-05-26 05:02:53.008897: step 4030, loss = 0.44 (944.4 examples/sec; 0.136 sec/batch)
2016-05-26 05:02:55.546251: step 4040, loss = 0.59 (1024.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:02:58.042999: step 4050, loss = 0.53 (984.6 examples/sec; 0.130 sec/batch)
2016-05-26 05:03:00.523142: step 4060, loss = 0.50 (1042.4 examples/sec; 0.123 sec/batch)
2016-05-26 05:03:03.025575: step 4070, loss = 0.37 (1025.2 examples/sec; 0.125 sec/batch)
2016-05-26 05:03:05.479246: step 4080, loss = 0.63 (1073.6 examples/sec; 0.119 sec/batch)
2016-05-26 05:03:07.938828: step 4090, loss = 0.41 (1029.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:03:10.448677: step 4100, loss = 0.53 (1086.7 examples/sec; 0.118 sec/batch)
2016-05-26 05:03:13.318269: step 4110, loss = 0.51 (971.5 examples/sec; 0.132 sec/batch)
2016-05-26 05:03:15.842881: step 4120, loss = 0.42 (1004.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:03:18.346787: step 4130, loss = 0.62 (1040.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:03:20.846126: step 4140, loss = 0.47 (1053.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:03:23.319921: step 4150, loss = 0.47 (1022.2 examples/sec; 0.125 sec/batch)
2016-05-26 05:03:25.801296: step 4160, loss = 0.51 (1039.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:03:28.301007: step 4170, loss = 0.57 (1016.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:03:30.806389: step 4180, loss = 0.44 (1017.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:03:33.339729: step 4190, loss = 0.50 (1078.4 examples/sec; 0.119 sec/batch)
2016-05-26 05:03:35.861004: step 4200, loss = 0.49 (994.0 examples/sec; 0.129 sec/batch)
2016-05-26 05:03:38.714019: step 4210, loss = 0.48 (962.8 examples/sec; 0.133 sec/batch)
2016-05-26 05:03:41.202572: step 4220, loss = 0.46 (952.0 examples/sec; 0.134 sec/batch)
2016-05-26 05:03:43.716173: step 4230, loss = 0.48 (986.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:03:46.187851: step 4240, loss = 0.31 (1003.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:03:48.643128: step 4250, loss = 0.51 (1022.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:03:51.106553: step 4260, loss = 0.29 (1057.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:03:53.600438: step 4270, loss = 0.65 (1080.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:03:56.091373: step 4280, loss = 0.40 (1079.5 examples/sec; 0.119 sec/batch)
2016-05-26 05:03:58.559383: step 4290, loss = 0.45 (1027.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:04:01.049848: step 4300, loss = 0.52 (998.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:04:03.915240: step 4310, loss = 0.40 (1014.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:04:06.405861: step 4320, loss = 0.35 (978.3 examples/sec; 0.131 sec/batch)
2016-05-26 05:04:08.873777: step 4330, loss = 0.47 (1052.1 examples/sec; 0.122 sec/batch)
2016-05-26 05:04:11.382581: step 4340, loss = 0.39 (1052.7 examples/sec; 0.122 sec/batch)
2016-05-26 05:04:13.915579: step 4350, loss = 0.45 (916.2 examples/sec; 0.140 sec/batch)
2016-05-26 05:04:16.414258: step 4360, loss = 0.52 (1042.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:04:18.887540: step 4370, loss = 0.53 (1001.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:04:21.336810: step 4380, loss = 0.52 (1043.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:04:23.800998: step 4390, loss = 0.39 (1050.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:04:26.344130: step 4400, loss = 0.36 (958.2 examples/sec; 0.134 sec/batch)
2016-05-26 05:04:29.199690: step 4410, loss = 0.42 (949.1 examples/sec; 0.135 sec/batch)
2016-05-26 05:04:31.713070: step 4420, loss = 0.49 (1008.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:04:34.187691: step 4430, loss = 0.44 (1065.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:04:36.704304: step 4440, loss = 0.56 (980.8 examples/sec; 0.131 sec/batch)
2016-05-26 05:04:39.210710: step 4450, loss = 0.42 (996.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:04:41.730183: step 4460, loss = 0.53 (1030.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:04:44.215895: step 4470, loss = 0.34 (1036.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:04:46.775491: step 4480, loss = 0.39 (1019.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:04:49.257774: step 4490, loss = 0.39 (1043.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:04:51.785151: step 4500, loss = 0.47 (997.1 examples/sec; 0.128 sec/batch)
2016-05-26 05:04:54.638943: step 4510, loss = 0.47 (1033.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:04:57.137903: step 4520, loss = 0.44 (1040.2 examples/sec; 0.123 sec/batch)
2016-05-26 05:04:59.680482: step 4530, loss = 0.49 (963.5 examples/sec; 0.133 sec/batch)
2016-05-26 05:05:02.202055: step 4540, loss = 0.53 (998.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:05:04.715994: step 4550, loss = 0.42 (1017.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:05:07.239187: step 4560, loss = 0.47 (993.6 examples/sec; 0.129 sec/batch)
2016-05-26 05:05:09.770212: step 4570, loss = 0.43 (1026.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:05:12.274112: step 4580, loss = 0.40 (1067.3 examples/sec; 0.120 sec/batch)
2016-05-26 05:05:14.785887: step 4590, loss = 0.45 (1017.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:05:17.252799: step 4600, loss = 0.51 (949.7 examples/sec; 0.135 sec/batch)
2016-05-26 05:05:20.071540: step 4610, loss = 0.50 (1012.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:05:22.633686: step 4620, loss = 0.49 (954.4 examples/sec; 0.134 sec/batch)
2016-05-26 05:05:25.157857: step 4630, loss = 0.32 (1077.8 examples/sec; 0.119 sec/batch)
2016-05-26 05:05:27.698467: step 4640, loss = 0.39 (997.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:05:30.224476: step 4650, loss = 0.73 (1012.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:05:32.724702: step 4660, loss = 0.54 (1080.6 examples/sec; 0.118 sec/batch)
2016-05-26 05:05:35.183113: step 4670, loss = 0.43 (1021.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:05:37.623761: step 4680, loss = 0.46 (1028.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:05:40.139718: step 4690, loss = 0.41 (1041.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:05:42.619538: step 4700, loss = 0.44 (1022.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:05:45.490288: step 4710, loss = 0.54 (978.9 examples/sec; 0.131 sec/batch)
2016-05-26 05:05:48.075787: step 4720, loss = 0.53 (1046.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:05:50.584428: step 4730, loss = 0.37 (1027.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:05:53.036082: step 4740, loss = 0.48 (1052.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:05:55.537938: step 4750, loss = 0.46 (1029.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:05:58.023668: step 4760, loss = 0.44 (1060.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:06:00.498730: step 4770, loss = 0.40 (1026.3 examples/sec; 0.125 sec/batch)
2016-05-26 05:06:03.010456: step 4780, loss = 0.38 (997.9 examples/sec; 0.128 sec/batch)
2016-05-26 05:06:05.573288: step 4790, loss = 0.60 (993.5 examples/sec; 0.129 sec/batch)
2016-05-26 05:06:08.098996: step 4800, loss = 0.38 (966.0 examples/sec; 0.132 sec/batch)
2016-05-26 05:06:10.910196: step 4810, loss = 0.48 (968.5 examples/sec; 0.132 sec/batch)
2016-05-26 05:06:13.420841: step 4820, loss = 0.47 (1056.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:06:15.907272: step 4830, loss = 0.40 (1060.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:06:18.398027: step 4840, loss = 0.47 (966.0 examples/sec; 0.133 sec/batch)
2016-05-26 05:06:20.819037: step 4850, loss = 0.53 (1109.2 examples/sec; 0.115 sec/batch)
2016-05-26 05:06:23.335000: step 4860, loss = 0.44 (970.9 examples/sec; 0.132 sec/batch)
2016-05-26 05:06:25.841696: step 4870, loss = 0.36 (1018.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:06:28.322781: step 4880, loss = 0.38 (1009.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:06:30.831796: step 4890, loss = 0.39 (1054.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:06:33.327921: step 4900, loss = 0.42 (1028.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:06:36.220129: step 4910, loss = 0.46 (994.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:06:38.714160: step 4920, loss = 0.35 (1010.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:06:41.171511: step 4930, loss = 0.43 (1036.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:06:43.668000: step 4940, loss = 0.37 (1059.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:06:46.174360: step 4950, loss = 0.42 (1031.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:06:48.627743: step 4960, loss = 0.50 (1058.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:06:51.123435: step 4970, loss = 0.36 (1000.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:06:53.607796: step 4980, loss = 0.49 (1089.1 examples/sec; 0.118 sec/batch)
2016-05-26 05:06:56.110957: step 4990, loss = 0.59 (1065.3 examples/sec; 0.120 sec/batch)
2016-05-26 05:06:58.569829: step 5000, loss = 0.44 (1077.3 examples/sec; 0.119 sec/batch)
eval once
2016-05-26 05:07:40.646354: accuracy @ 1 = 0.854, 42735 / 50048 at 0
2016-05-26 05:07:43.177727: step 5010, loss = 0.37 (1034.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:07:45.622030: step 5020, loss = 0.45 (1052.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:07:48.067668: step 5030, loss = 0.41 (1034.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:07:50.605677: step 5040, loss = 0.40 (987.2 examples/sec; 0.130 sec/batch)
2016-05-26 05:07:53.158309: step 5050, loss = 0.59 (1067.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:07:55.561280: step 5060, loss = 0.39 (1054.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:07:57.970379: step 5070, loss = 0.55 (1034.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:08:00.329589: step 5080, loss = 0.29 (1124.6 examples/sec; 0.114 sec/batch)
2016-05-26 05:08:02.726871: step 5090, loss = 0.38 (1120.2 examples/sec; 0.114 sec/batch)
2016-05-26 05:08:05.238569: step 5100, loss = 0.36 (1036.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:08:08.125643: step 5110, loss = 0.45 (1003.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:08:10.649788: step 5120, loss = 0.28 (1045.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:08:13.131744: step 5130, loss = 0.41 (1020.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:08:15.641548: step 5140, loss = 0.45 (1025.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:08:18.100590: step 5150, loss = 0.47 (1083.4 examples/sec; 0.118 sec/batch)
2016-05-26 05:08:20.605407: step 5160, loss = 0.47 (998.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:08:23.136714: step 5170, loss = 0.55 (1028.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:08:25.655375: step 5180, loss = 0.33 (1065.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:08:28.122401: step 5190, loss = 0.46 (1054.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:08:30.525039: step 5200, loss = 0.40 (1106.2 examples/sec; 0.116 sec/batch)
2016-05-26 05:08:33.309089: step 5210, loss = 0.52 (1026.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:08:35.817738: step 5220, loss = 0.30 (989.3 examples/sec; 0.129 sec/batch)
2016-05-26 05:08:38.264147: step 5230, loss = 0.38 (1075.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:08:40.778530: step 5240, loss = 0.49 (1005.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:08:43.256181: step 5250, loss = 0.61 (1040.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:08:45.780286: step 5260, loss = 0.37 (1063.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:08:48.293656: step 5270, loss = 0.44 (1016.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:08:50.736954: step 5280, loss = 0.50 (1069.6 examples/sec; 0.120 sec/batch)
2016-05-26 05:08:53.306466: step 5290, loss = 0.37 (997.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:08:55.842824: step 5300, loss = 0.34 (1088.0 examples/sec; 0.118 sec/batch)
2016-05-26 05:08:58.641830: step 5310, loss = 0.46 (1070.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:09:01.100098: step 5320, loss = 0.32 (1059.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:09:03.590939: step 5330, loss = 0.43 (1088.1 examples/sec; 0.118 sec/batch)
2016-05-26 05:09:06.049598: step 5340, loss = 0.36 (978.3 examples/sec; 0.131 sec/batch)
2016-05-26 05:09:08.540183: step 5350, loss = 0.42 (1110.5 examples/sec; 0.115 sec/batch)
2016-05-26 05:09:11.018468: step 5360, loss = 0.41 (1104.6 examples/sec; 0.116 sec/batch)
2016-05-26 05:09:13.534664: step 5370, loss = 0.48 (1110.1 examples/sec; 0.115 sec/batch)
2016-05-26 05:09:15.999416: step 5380, loss = 0.38 (1073.0 examples/sec; 0.119 sec/batch)
2016-05-26 05:09:18.466933: step 5390, loss = 0.34 (1047.7 examples/sec; 0.122 sec/batch)
2016-05-26 05:09:21.008929: step 5400, loss = 0.43 (998.9 examples/sec; 0.128 sec/batch)
2016-05-26 05:09:23.881713: step 5410, loss = 0.55 (996.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:09:26.402556: step 5420, loss = 0.53 (1074.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:09:28.897594: step 5430, loss = 0.37 (1090.0 examples/sec; 0.117 sec/batch)
2016-05-26 05:09:31.422648: step 5440, loss = 0.45 (1064.3 examples/sec; 0.120 sec/batch)
2016-05-26 05:09:33.900709: step 5450, loss = 0.41 (1067.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:09:36.373190: step 5460, loss = 0.42 (966.8 examples/sec; 0.132 sec/batch)
2016-05-26 05:09:38.866463: step 5470, loss = 0.36 (1035.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:09:41.334752: step 5480, loss = 0.36 (1009.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:09:43.849017: step 5490, loss = 0.58 (960.2 examples/sec; 0.133 sec/batch)
2016-05-26 05:09:46.377784: step 5500, loss = 0.38 (1002.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:09:49.198956: step 5510, loss = 0.40 (1047.1 examples/sec; 0.122 sec/batch)
2016-05-26 05:09:51.664765: step 5520, loss = 0.33 (1059.5 examples/sec; 0.121 sec/batch)
2016-05-26 05:09:54.176446: step 5530, loss = 0.45 (1048.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:09:56.680944: step 5540, loss = 0.43 (1018.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:09:59.152363: step 5550, loss = 0.37 (1020.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:10:01.656472: step 5560, loss = 0.36 (1059.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:10:04.198796: step 5570, loss = 0.28 (971.6 examples/sec; 0.132 sec/batch)
2016-05-26 05:10:06.678730: step 5580, loss = 0.43 (1061.3 examples/sec; 0.121 sec/batch)
2016-05-26 05:10:09.224779: step 5590, loss = 0.39 (1079.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:10:11.724772: step 5600, loss = 0.40 (977.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:10:14.532896: step 5610, loss = 0.35 (1019.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:10:16.989917: step 5620, loss = 0.48 (1065.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:10:19.493124: step 5630, loss = 0.28 (951.7 examples/sec; 0.134 sec/batch)
2016-05-26 05:10:21.969392: step 5640, loss = 0.39 (1068.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:10:24.533204: step 5650, loss = 0.45 (1035.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:10:27.020227: step 5660, loss = 0.40 (1017.1 examples/sec; 0.126 sec/batch)
2016-05-26 05:10:29.511900: step 5670, loss = 0.40 (1001.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:10:32.011002: step 5680, loss = 0.44 (1062.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:10:34.523617: step 5690, loss = 0.44 (978.1 examples/sec; 0.131 sec/batch)
2016-05-26 05:10:36.986977: step 5700, loss = 0.27 (1085.2 examples/sec; 0.118 sec/batch)
2016-05-26 05:10:39.778822: step 5710, loss = 0.23 (1003.1 examples/sec; 0.128 sec/batch)
2016-05-26 05:10:42.240302: step 5720, loss = 0.32 (982.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:10:44.722008: step 5730, loss = 0.40 (1020.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:10:47.154175: step 5740, loss = 0.28 (1033.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:10:49.625379: step 5750, loss = 0.28 (1088.6 examples/sec; 0.118 sec/batch)
2016-05-26 05:10:52.151309: step 5760, loss = 0.38 (1017.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:10:54.650278: step 5770, loss = 0.41 (1029.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:10:57.060682: step 5780, loss = 0.38 (1085.9 examples/sec; 0.118 sec/batch)
2016-05-26 05:10:59.581011: step 5790, loss = 0.31 (1020.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:11:02.036106: step 5800, loss = 0.31 (1097.6 examples/sec; 0.117 sec/batch)
2016-05-26 05:11:04.805690: step 5810, loss = 0.50 (1040.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:11:07.312621: step 5820, loss = 0.39 (982.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:11:09.796858: step 5830, loss = 0.41 (1030.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:11:12.265137: step 5840, loss = 0.45 (1000.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:11:14.755224: step 5850, loss = 0.28 (1066.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:11:17.292984: step 5860, loss = 0.30 (937.7 examples/sec; 0.137 sec/batch)
2016-05-26 05:11:19.773394: step 5870, loss = 0.38 (1048.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:11:22.303728: step 5880, loss = 0.35 (1002.9 examples/sec; 0.128 sec/batch)
2016-05-26 05:11:24.821686: step 5890, loss = 0.44 (975.3 examples/sec; 0.131 sec/batch)
2016-05-26 05:11:27.349438: step 5900, loss = 0.34 (973.2 examples/sec; 0.132 sec/batch)
2016-05-26 05:11:30.191834: step 5910, loss = 0.32 (980.9 examples/sec; 0.130 sec/batch)
2016-05-26 05:11:32.728538: step 5920, loss = 0.39 (1011.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:11:35.203712: step 5930, loss = 0.37 (1046.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:11:37.737695: step 5940, loss = 0.41 (1059.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:11:40.248087: step 5950, loss = 0.48 (973.6 examples/sec; 0.131 sec/batch)
2016-05-26 05:11:42.697074: step 5960, loss = 0.54 (1058.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:11:45.206166: step 5970, loss = 0.44 (975.4 examples/sec; 0.131 sec/batch)
2016-05-26 05:11:47.666227: step 5980, loss = 0.27 (1006.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:11:50.169285: step 5990, loss = 0.40 (1030.6 examples/sec; 0.124 sec/batch)
2016-05-26 05:11:52.652424: step 6000, loss = 0.38 (1095.4 examples/sec; 0.117 sec/batch)
eval once
2016-05-26 05:12:35.119698: accuracy @ 1 = 0.871, 43590 / 50048 at 0
2016-05-26 05:12:37.627821: step 6010, loss = 0.37 (1060.5 examples/sec; 0.121 sec/batch)
2016-05-26 05:12:40.138860: step 6020, loss = 0.44 (1032.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:12:42.666501: step 6030, loss = 0.33 (1017.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:12:45.130213: step 6040, loss = 0.35 (1035.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:12:47.610835: step 6050, loss = 0.54 (1030.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:12:50.044747: step 6060, loss = 0.35 (1037.3 examples/sec; 0.123 sec/batch)
2016-05-26 05:12:52.477785: step 6070, loss = 0.46 (1059.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:12:55.047371: step 6080, loss = 0.55 (968.7 examples/sec; 0.132 sec/batch)
2016-05-26 05:12:57.540854: step 6090, loss = 0.43 (1091.8 examples/sec; 0.117 sec/batch)
2016-05-26 05:13:00.033707: step 6100, loss = 0.36 (1041.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:13:02.849988: step 6110, loss = 0.49 (1003.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:13:05.380699: step 6120, loss = 0.31 (1025.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:13:07.841340: step 6130, loss = 0.26 (1092.8 examples/sec; 0.117 sec/batch)
2016-05-26 05:13:10.326051: step 6140, loss = 0.35 (1000.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:13:12.869804: step 6150, loss = 0.34 (983.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:13:15.342543: step 6160, loss = 0.48 (1079.7 examples/sec; 0.119 sec/batch)
2016-05-26 05:13:17.819544: step 6170, loss = 0.39 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:13:20.305136: step 6180, loss = 0.31 (1011.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:13:22.707453: step 6190, loss = 0.36 (1036.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:13:25.126054: step 6200, loss = 0.41 (1100.5 examples/sec; 0.116 sec/batch)
2016-05-26 05:13:27.915310: step 6210, loss = 0.34 (1069.6 examples/sec; 0.120 sec/batch)
2016-05-26 05:13:30.409832: step 6220, loss = 0.37 (1007.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:13:32.879053: step 6230, loss = 0.37 (1040.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:13:35.353251: step 6240, loss = 0.39 (1032.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:13:37.910152: step 6250, loss = 0.39 (996.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:13:40.414305: step 6260, loss = 0.36 (1060.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:13:42.884429: step 6270, loss = 0.29 (1028.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:13:45.406232: step 6280, loss = 0.40 (991.2 examples/sec; 0.129 sec/batch)
2016-05-26 05:13:47.921759: step 6290, loss = 0.34 (1063.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:13:50.427330: step 6300, loss = 0.37 (1020.3 examples/sec; 0.125 sec/batch)
2016-05-26 05:13:53.279445: step 6310, loss = 0.22 (1012.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:13:55.817462: step 6320, loss = 0.35 (992.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:13:58.345409: step 6330, loss = 0.44 (1013.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:14:00.826540: step 6340, loss = 0.47 (1018.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:14:03.358634: step 6350, loss = 0.48 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:14:05.871415: step 6360, loss = 0.42 (1025.7 examples/sec; 0.125 sec/batch)
2016-05-26 05:14:08.345359: step 6370, loss = 0.31 (962.0 examples/sec; 0.133 sec/batch)
2016-05-26 05:14:10.773984: step 6380, loss = 0.38 (1062.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:14:13.254617: step 6390, loss = 0.40 (982.6 examples/sec; 0.130 sec/batch)
2016-05-26 05:14:15.706579: step 6400, loss = 0.34 (1045.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:14:18.475543: step 6410, loss = 0.28 (1018.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:14:21.045010: step 6420, loss = 0.34 (1027.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:14:23.571605: step 6430, loss = 0.36 (1069.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:14:26.089698: step 6440, loss = 0.43 (1031.6 examples/sec; 0.124 sec/batch)
2016-05-26 05:14:28.603181: step 6450, loss = 0.49 (1033.9 examples/sec; 0.124 sec/batch)
2016-05-26 05:14:31.171033: step 6460, loss = 0.24 (1044.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:14:33.722338: step 6470, loss = 0.28 (1099.2 examples/sec; 0.116 sec/batch)
2016-05-26 05:14:36.207362: step 6480, loss = 0.45 (1018.1 examples/sec; 0.126 sec/batch)
2016-05-26 05:14:38.711118: step 6490, loss = 0.26 (990.5 examples/sec; 0.129 sec/batch)
2016-05-26 05:14:41.207252: step 6500, loss = 0.43 (1070.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:14:43.981459: step 6510, loss = 0.27 (1056.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:14:46.459560: step 6520, loss = 0.30 (1109.5 examples/sec; 0.115 sec/batch)
2016-05-26 05:14:48.956453: step 6530, loss = 0.30 (1021.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:14:51.408145: step 6540, loss = 0.33 (1082.8 examples/sec; 0.118 sec/batch)
2016-05-26 05:14:53.885732: step 6550, loss = 0.30 (1052.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:14:56.444970: step 6560, loss = 0.32 (1026.2 examples/sec; 0.125 sec/batch)
2016-05-26 05:14:58.890661: step 6570, loss = 0.34 (1017.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:15:01.378846: step 6580, loss = 0.36 (1019.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:15:03.827951: step 6590, loss = 0.44 (974.4 examples/sec; 0.131 sec/batch)
2016-05-26 05:15:06.338883: step 6600, loss = 0.35 (1055.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:15:09.186401: step 6610, loss = 0.44 (1031.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:15:11.667777: step 6620, loss = 0.39 (969.9 examples/sec; 0.132 sec/batch)
2016-05-26 05:15:14.150748: step 6630, loss = 0.53 (1001.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:15:16.683430: step 6640, loss = 0.30 (1040.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:15:19.175964: step 6650, loss = 0.36 (1036.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:15:21.670586: step 6660, loss = 0.30 (1047.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:15:24.146830: step 6670, loss = 0.31 (1004.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:15:26.668563: step 6680, loss = 0.34 (985.4 examples/sec; 0.130 sec/batch)
2016-05-26 05:15:29.173375: step 6690, loss = 0.31 (1025.7 examples/sec; 0.125 sec/batch)
2016-05-26 05:15:31.642453: step 6700, loss = 0.37 (1010.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:15:34.486479: step 6710, loss = 0.38 (1005.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:15:37.012848: step 6720, loss = 0.22 (1073.8 examples/sec; 0.119 sec/batch)
2016-05-26 05:15:39.487986: step 6730, loss = 0.36 (1097.6 examples/sec; 0.117 sec/batch)
2016-05-26 05:15:41.915395: step 6740, loss = 0.21 (1067.5 examples/sec; 0.120 sec/batch)
2016-05-26 05:15:44.421420: step 6750, loss = 0.29 (1081.2 examples/sec; 0.118 sec/batch)
2016-05-26 05:15:46.944255: step 6760, loss = 0.17 (1069.5 examples/sec; 0.120 sec/batch)
2016-05-26 05:15:49.409742: step 6770, loss = 0.45 (1077.4 examples/sec; 0.119 sec/batch)
2016-05-26 05:15:51.888758: step 6780, loss = 0.42 (1097.8 examples/sec; 0.117 sec/batch)
2016-05-26 05:15:54.361386: step 6790, loss = 0.35 (1020.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:15:56.852791: step 6800, loss = 0.42 (1043.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:15:59.728890: step 6810, loss = 0.23 (968.2 examples/sec; 0.132 sec/batch)
2016-05-26 05:16:02.220840: step 6820, loss = 0.47 (1042.4 examples/sec; 0.123 sec/batch)
2016-05-26 05:16:04.707987: step 6830, loss = 0.32 (957.9 examples/sec; 0.134 sec/batch)
2016-05-26 05:16:07.173155: step 6840, loss = 0.39 (1024.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:16:09.661546: step 6850, loss = 0.39 (1052.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:16:12.145686: step 6860, loss = 0.32 (1015.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:16:14.646605: step 6870, loss = 0.22 (1013.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:16:17.171186: step 6880, loss = 0.31 (979.9 examples/sec; 0.131 sec/batch)
2016-05-26 05:16:19.640516: step 6890, loss = 0.29 (1066.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:16:22.195522: step 6900, loss = 0.24 (1009.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:16:25.024830: step 6910, loss = 0.26 (975.8 examples/sec; 0.131 sec/batch)
2016-05-26 05:16:27.533749: step 6920, loss = 0.42 (1004.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:16:30.035634: step 6930, loss = 0.26 (1004.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:16:32.607396: step 6940, loss = 0.35 (958.7 examples/sec; 0.134 sec/batch)
2016-05-26 05:16:35.066022: step 6950, loss = 0.31 (1012.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:16:37.569450: step 6960, loss = 0.33 (1110.3 examples/sec; 0.115 sec/batch)
2016-05-26 05:16:40.049208: step 6970, loss = 0.35 (989.3 examples/sec; 0.129 sec/batch)
2016-05-26 05:16:42.522787: step 6980, loss = 0.35 (1053.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:16:45.008222: step 6990, loss = 0.31 (1081.5 examples/sec; 0.118 sec/batch)
2016-05-26 05:16:47.506181: step 7000, loss = 0.34 (966.4 examples/sec; 0.132 sec/batch)
eval once
2016-05-26 05:17:29.234022: accuracy @ 1 = 0.890, 44561 / 50048 at 0
2016-05-26 05:17:31.730328: step 7010, loss = 0.34 (1047.1 examples/sec; 0.122 sec/batch)
2016-05-26 05:17:34.207391: step 7020, loss = 0.38 (1024.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:17:36.596436: step 7030, loss = 0.40 (1063.3 examples/sec; 0.120 sec/batch)
2016-05-26 05:17:39.049663: step 7040, loss = 0.32 (1049.1 examples/sec; 0.122 sec/batch)
2016-05-26 05:17:41.521274: step 7050, loss = 0.26 (993.3 examples/sec; 0.129 sec/batch)
2016-05-26 05:17:43.926735: step 7060, loss = 0.27 (1052.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:17:46.473598: step 7070, loss = 0.37 (1003.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:17:48.916411: step 7080, loss = 0.25 (998.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:17:51.428939: step 7090, loss = 0.34 (1072.4 examples/sec; 0.119 sec/batch)
2016-05-26 05:17:53.940357: step 7100, loss = 0.31 (1002.1 examples/sec; 0.128 sec/batch)
2016-05-26 05:17:56.772824: step 7110, loss = 0.37 (1054.3 examples/sec; 0.121 sec/batch)
2016-05-26 05:17:59.330075: step 7120, loss = 0.34 (970.3 examples/sec; 0.132 sec/batch)
2016-05-26 05:18:01.767967: step 7130, loss = 0.25 (998.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:18:04.268481: step 7140, loss = 0.34 (1039.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:18:06.744479: step 7150, loss = 0.34 (1006.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:18:09.275285: step 7160, loss = 0.27 (1052.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:18:11.761078: step 7170, loss = 0.26 (1006.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:18:14.300754: step 7180, loss = 0.29 (1008.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:18:16.810440: step 7190, loss = 0.25 (1001.2 examples/sec; 0.128 sec/batch)
2016-05-26 05:18:19.263488: step 7200, loss = 0.30 (936.7 examples/sec; 0.137 sec/batch)
2016-05-26 05:18:22.097237: step 7210, loss = 0.33 (1009.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:18:24.588258: step 7220, loss = 0.29 (1015.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:18:27.070275: step 7230, loss = 0.13 (998.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:18:29.579261: step 7240, loss = 0.30 (1050.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:18:32.053459: step 7250, loss = 0.16 (1057.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:18:34.471114: step 7260, loss = 0.27 (1080.4 examples/sec; 0.118 sec/batch)
2016-05-26 05:18:36.977969: step 7270, loss = 0.29 (1018.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:18:39.455034: step 7280, loss = 0.32 (1040.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:18:41.930340: step 7290, loss = 0.35 (1120.4 examples/sec; 0.114 sec/batch)
2016-05-26 05:18:44.432893: step 7300, loss = 0.34 (1044.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:18:47.309527: step 7310, loss = 0.33 (972.9 examples/sec; 0.132 sec/batch)
2016-05-26 05:18:49.805458: step 7320, loss = 0.26 (989.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:18:52.286520: step 7330, loss = 0.29 (1023.2 examples/sec; 0.125 sec/batch)
2016-05-26 05:18:54.750604: step 7340, loss = 0.26 (1044.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:18:57.210691: step 7350, loss = 0.34 (1051.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:18:59.692964: step 7360, loss = 0.41 (1045.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:19:02.218533: step 7370, loss = 0.25 (962.9 examples/sec; 0.133 sec/batch)
2016-05-26 05:19:04.752957: step 7380, loss = 0.41 (1010.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:19:07.228205: step 7390, loss = 0.32 (965.0 examples/sec; 0.133 sec/batch)
2016-05-26 05:19:09.747571: step 7400, loss = 0.37 (921.9 examples/sec; 0.139 sec/batch)
2016-05-26 05:19:12.507355: step 7410, loss = 0.36 (1055.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:19:15.029735: step 7420, loss = 0.31 (1020.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:19:17.584469: step 7430, loss = 0.21 (961.6 examples/sec; 0.133 sec/batch)
2016-05-26 05:19:20.084162: step 7440, loss = 0.34 (1021.2 examples/sec; 0.125 sec/batch)
2016-05-26 05:19:22.587876: step 7450, loss = 0.33 (1051.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:19:25.075346: step 7460, loss = 0.29 (1000.9 examples/sec; 0.128 sec/batch)
2016-05-26 05:19:27.569824: step 7470, loss = 0.30 (1048.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:19:30.016402: step 7480, loss = 0.41 (1091.0 examples/sec; 0.117 sec/batch)
2016-05-26 05:19:32.531857: step 7490, loss = 0.39 (1063.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:19:35.067889: step 7500, loss = 0.25 (1032.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:19:37.919156: step 7510, loss = 0.27 (1050.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:19:40.413632: step 7520, loss = 0.27 (1013.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:19:42.857091: step 7530, loss = 0.42 (1038.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:19:45.404365: step 7540, loss = 0.17 (990.9 examples/sec; 0.129 sec/batch)
2016-05-26 05:19:47.951020: step 7550, loss = 0.25 (966.7 examples/sec; 0.132 sec/batch)
2016-05-26 05:19:50.456394: step 7560, loss = 0.19 (998.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:19:52.949993: step 7570, loss = 0.37 (1002.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:19:55.380700: step 7580, loss = 0.29 (1053.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:19:57.883866: step 7590, loss = 0.38 (1057.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:20:00.370244: step 7600, loss = 0.25 (1033.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:20:03.162360: step 7610, loss = 0.33 (973.6 examples/sec; 0.131 sec/batch)
2016-05-26 05:20:05.652033: step 7620, loss = 0.30 (1042.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:20:08.142790: step 7630, loss = 0.38 (1038.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:20:10.629544: step 7640, loss = 0.36 (1019.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:20:13.130115: step 7650, loss = 0.28 (1016.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:20:15.574932: step 7660, loss = 0.20 (1079.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:20:18.092549: step 7670, loss = 0.22 (1047.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:20:20.615238: step 7680, loss = 0.43 (1004.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:20:23.099471: step 7690, loss = 0.30 (962.4 examples/sec; 0.133 sec/batch)
2016-05-26 05:20:25.575981: step 7700, loss = 0.33 (1009.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:20:28.414532: step 7710, loss = 0.30 (1056.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:20:30.904359: step 7720, loss = 0.19 (1015.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:20:33.451968: step 7730, loss = 0.35 (1063.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:20:35.974203: step 7740, loss = 0.26 (1002.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:20:38.475754: step 7750, loss = 0.35 (1028.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:20:40.948932: step 7760, loss = 0.26 (1070.7 examples/sec; 0.120 sec/batch)
2016-05-26 05:20:43.463538: step 7770, loss = 0.25 (1023.3 examples/sec; 0.125 sec/batch)
2016-05-26 05:20:45.948979: step 7780, loss = 0.24 (973.4 examples/sec; 0.131 sec/batch)
2016-05-26 05:20:48.455410: step 7790, loss = 0.26 (1032.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:20:50.920109: step 7800, loss = 0.23 (1017.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:20:53.624010: step 7810, loss = 0.27 (1133.2 examples/sec; 0.113 sec/batch)
2016-05-26 05:20:56.071387: step 7820, loss = 0.29 (1000.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:20:58.579863: step 7830, loss = 0.22 (1010.1 examples/sec; 0.127 sec/batch)
2016-05-26 05:21:01.056374: step 7840, loss = 0.50 (965.8 examples/sec; 0.133 sec/batch)
2016-05-26 05:21:03.496076: step 7850, loss = 0.33 (1032.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:21:06.004441: step 7860, loss = 0.22 (1000.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:21:08.501931: step 7870, loss = 0.25 (1027.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:21:10.954175: step 7880, loss = 0.35 (1101.9 examples/sec; 0.116 sec/batch)
2016-05-26 05:21:13.447149: step 7890, loss = 0.52 (1010.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:21:16.029437: step 7900, loss = 0.41 (966.5 examples/sec; 0.132 sec/batch)
2016-05-26 05:21:18.819223: step 7910, loss = 0.25 (1096.4 examples/sec; 0.117 sec/batch)
2016-05-26 05:21:21.264781: step 7920, loss = 0.37 (1038.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:21:23.712027: step 7930, loss = 0.42 (1061.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:21:26.214542: step 7940, loss = 0.24 (1012.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:21:28.735648: step 7950, loss = 0.31 (1027.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:21:31.209863: step 7960, loss = 0.30 (982.0 examples/sec; 0.130 sec/batch)
2016-05-26 05:21:33.651277: step 7970, loss = 0.23 (1067.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:21:36.118284: step 7980, loss = 0.23 (1071.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:21:38.572633: step 7990, loss = 0.31 (1032.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:21:40.986907: step 8000, loss = 0.29 (1060.7 examples/sec; 0.121 sec/batch)
eval once
2016-05-26 05:22:22.548654: accuracy @ 1 = 0.898, 44925 / 50048 at 0
2016-05-26 05:22:25.022036: step 8010, loss = 0.29 (1152.3 examples/sec; 0.111 sec/batch)
2016-05-26 05:22:27.479314: step 8020, loss = 0.26 (1055.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:22:29.938680: step 8030, loss = 0.24 (985.4 examples/sec; 0.130 sec/batch)
2016-05-26 05:22:32.485602: step 8040, loss = 0.33 (977.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:22:34.928137: step 8050, loss = 0.24 (1032.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:22:37.347413: step 8060, loss = 0.28 (1081.0 examples/sec; 0.118 sec/batch)
2016-05-26 05:22:39.786802: step 8070, loss = 0.25 (1022.7 examples/sec; 0.125 sec/batch)
2016-05-26 05:22:42.268117: step 8080, loss = 0.22 (1107.4 examples/sec; 0.116 sec/batch)
2016-05-26 05:22:44.727759: step 8090, loss = 0.29 (1013.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:22:47.183261: step 8100, loss = 0.26 (1052.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:22:49.899796: step 8110, loss = 0.30 (1066.1 examples/sec; 0.120 sec/batch)
2016-05-26 05:22:52.374176: step 8120, loss = 0.36 (1072.8 examples/sec; 0.119 sec/batch)
2016-05-26 05:22:54.881766: step 8130, loss = 0.24 (1005.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:22:57.301477: step 8140, loss = 0.36 (997.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:22:59.790133: step 8150, loss = 0.23 (1017.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:23:02.249822: step 8160, loss = 0.29 (1077.0 examples/sec; 0.119 sec/batch)
2016-05-26 05:23:04.724716: step 8170, loss = 0.38 (1018.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:23:07.237174: step 8180, loss = 0.31 (966.4 examples/sec; 0.132 sec/batch)
2016-05-26 05:23:09.717803: step 8190, loss = 0.27 (963.7 examples/sec; 0.133 sec/batch)
2016-05-26 05:23:12.190330: step 8200, loss = 0.20 (1092.8 examples/sec; 0.117 sec/batch)
2016-05-26 05:23:15.001869: step 8210, loss = 0.38 (1021.3 examples/sec; 0.125 sec/batch)
2016-05-26 05:23:17.518952: step 8220, loss = 0.43 (992.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:23:19.954432: step 8230, loss = 0.26 (1058.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:23:22.370909: step 8240, loss = 0.27 (1064.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:23:24.833331: step 8250, loss = 0.31 (1017.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:23:27.308048: step 8260, loss = 0.26 (1085.2 examples/sec; 0.118 sec/batch)
2016-05-26 05:23:29.834936: step 8270, loss = 0.24 (1004.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:23:32.370966: step 8280, loss = 0.17 (1027.7 examples/sec; 0.125 sec/batch)
2016-05-26 05:23:34.849179: step 8290, loss = 0.17 (1002.2 examples/sec; 0.128 sec/batch)
2016-05-26 05:23:37.314079: step 8300, loss = 0.21 (1111.1 examples/sec; 0.115 sec/batch)
2016-05-26 05:23:40.115273: step 8310, loss = 0.29 (988.2 examples/sec; 0.130 sec/batch)
2016-05-26 05:23:42.631948: step 8320, loss = 0.31 (1082.0 examples/sec; 0.118 sec/batch)
2016-05-26 05:23:45.140345: step 8330, loss = 0.30 (1053.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:23:47.654827: step 8340, loss = 0.22 (967.3 examples/sec; 0.132 sec/batch)
2016-05-26 05:23:50.096364: step 8350, loss = 0.32 (1091.4 examples/sec; 0.117 sec/batch)
2016-05-26 05:23:52.537491: step 8360, loss = 0.37 (1045.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:23:55.001677: step 8370, loss = 0.33 (1003.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:23:57.483597: step 8380, loss = 0.32 (1030.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:23:59.974831: step 8390, loss = 0.28 (1030.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:24:02.466660: step 8400, loss = 0.29 (1016.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:24:05.247319: step 8410, loss = 0.36 (1073.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:24:07.715850: step 8420, loss = 0.22 (1068.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:24:10.197334: step 8430, loss = 0.36 (1023.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:24:12.663386: step 8440, loss = 0.21 (1014.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:24:15.155123: step 8450, loss = 0.28 (1045.1 examples/sec; 0.122 sec/batch)
2016-05-26 05:24:17.677350: step 8460, loss = 0.34 (994.0 examples/sec; 0.129 sec/batch)
2016-05-26 05:24:20.148157: step 8470, loss = 0.23 (1048.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:24:22.593368: step 8480, loss = 0.24 (1033.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:24:25.090968: step 8490, loss = 0.18 (1060.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:24:27.607336: step 8500, loss = 0.27 (1041.1 examples/sec; 0.123 sec/batch)
2016-05-26 05:24:30.363646: step 8510, loss = 0.35 (1028.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:24:32.858380: step 8520, loss = 0.24 (1000.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:24:35.357775: step 8530, loss = 0.24 (1037.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:24:37.836935: step 8540, loss = 0.27 (1039.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:24:40.276505: step 8550, loss = 0.24 (1049.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:24:42.730193: step 8560, loss = 0.39 (1032.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:24:45.184764: step 8570, loss = 0.23 (1052.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:24:47.624542: step 8580, loss = 0.17 (1063.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:24:50.081206: step 8590, loss = 0.24 (1096.1 examples/sec; 0.117 sec/batch)
2016-05-26 05:24:52.557973: step 8600, loss = 0.34 (1017.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:24:55.342530: step 8610, loss = 0.19 (1030.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:24:57.828607: step 8620, loss = 0.39 (1026.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:25:00.326566: step 8630, loss = 0.16 (1058.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:25:02.819423: step 8640, loss = 0.24 (1032.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:25:05.299303: step 8650, loss = 0.21 (959.8 examples/sec; 0.133 sec/batch)
2016-05-26 05:25:07.743194: step 8660, loss = 0.24 (1039.4 examples/sec; 0.123 sec/batch)
2016-05-26 05:25:10.244768: step 8670, loss = 0.33 (1077.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:25:12.778915: step 8680, loss = 0.29 (1019.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:25:15.245115: step 8690, loss = 0.34 (1059.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:25:17.711420: step 8700, loss = 0.31 (1038.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:25:20.512900: step 8710, loss = 0.20 (1050.7 examples/sec; 0.122 sec/batch)
2016-05-26 05:25:22.972441: step 8720, loss = 0.26 (1028.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:25:25.407071: step 8730, loss = 0.28 (1044.1 examples/sec; 0.123 sec/batch)
2016-05-26 05:25:27.913844: step 8740, loss = 0.20 (1017.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:25:30.403423: step 8750, loss = 0.26 (1049.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:25:32.874496: step 8760, loss = 0.21 (979.1 examples/sec; 0.131 sec/batch)
2016-05-26 05:25:35.339682: step 8770, loss = 0.29 (1008.1 examples/sec; 0.127 sec/batch)
2016-05-26 05:25:37.850066: step 8780, loss = 0.21 (1058.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:25:40.318811: step 8790, loss = 0.20 (1036.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:25:42.860164: step 8800, loss = 0.43 (1043.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:25:45.700243: step 8810, loss = 0.39 (1006.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:25:48.174178: step 8820, loss = 0.35 (992.2 examples/sec; 0.129 sec/batch)
2016-05-26 05:25:50.725769: step 8830, loss = 0.28 (996.0 examples/sec; 0.129 sec/batch)
2016-05-26 05:25:53.288318: step 8840, loss = 0.29 (967.3 examples/sec; 0.132 sec/batch)
2016-05-26 05:25:55.786050: step 8850, loss = 0.25 (1044.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:25:58.263463: step 8860, loss = 0.27 (1041.4 examples/sec; 0.123 sec/batch)
2016-05-26 05:26:00.741165: step 8870, loss = 0.19 (1074.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:26:03.197918: step 8880, loss = 0.23 (1055.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:26:05.662370: step 8890, loss = 0.18 (1014.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:26:08.146323: step 8900, loss = 0.19 (1020.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:26:10.911216: step 8910, loss = 0.21 (1010.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:26:13.389959: step 8920, loss = 0.24 (1079.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:26:15.894374: step 8930, loss = 0.14 (1012.1 examples/sec; 0.126 sec/batch)
2016-05-26 05:26:18.395913: step 8940, loss = 0.23 (967.6 examples/sec; 0.132 sec/batch)
2016-05-26 05:26:20.846255: step 8950, loss = 0.22 (1041.2 examples/sec; 0.123 sec/batch)
2016-05-26 05:26:23.359870: step 8960, loss = 0.30 (1000.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:26:25.905112: step 8970, loss = 0.22 (1006.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:26:28.428151: step 8980, loss = 0.29 (1036.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:26:30.935241: step 8990, loss = 0.32 (1068.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:26:33.397135: step 9000, loss = 0.19 (1008.8 examples/sec; 0.127 sec/batch)
eval once
2016-05-26 05:27:15.037695: accuracy @ 1 = 0.923, 46217 / 50048 at 0
2016-05-26 05:27:17.537975: step 9010, loss = 0.14 (1007.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:27:20.000815: step 9020, loss = 0.20 (1087.5 examples/sec; 0.118 sec/batch)
2016-05-26 05:27:22.480501: step 9030, loss = 0.26 (1031.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:27:24.991492: step 9040, loss = 0.19 (1004.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:27:27.492860: step 9050, loss = 0.17 (987.2 examples/sec; 0.130 sec/batch)
2016-05-26 05:27:30.021204: step 9060, loss = 0.24 (985.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:27:32.550541: step 9070, loss = 0.28 (992.9 examples/sec; 0.129 sec/batch)
2016-05-26 05:27:35.053153: step 9080, loss = 0.24 (1026.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:27:37.534663: step 9090, loss = 0.16 (965.4 examples/sec; 0.133 sec/batch)
2016-05-26 05:27:40.012819: step 9100, loss = 0.31 (1033.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:27:42.820720: step 9110, loss = 0.25 (999.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:27:45.310979: step 9120, loss = 0.27 (1032.6 examples/sec; 0.124 sec/batch)
2016-05-26 05:27:47.833473: step 9130, loss = 0.20 (1061.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:27:50.348242: step 9140, loss = 0.26 (1008.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:27:52.831757: step 9150, loss = 0.29 (1002.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:27:55.333543: step 9160, loss = 0.25 (1043.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:27:57.799622: step 9170, loss = 0.26 (989.2 examples/sec; 0.129 sec/batch)
2016-05-26 05:28:00.306635: step 9180, loss = 0.15 (1040.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:28:02.814840: step 9190, loss = 0.15 (1020.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:28:05.243536: step 9200, loss = 0.23 (982.8 examples/sec; 0.130 sec/batch)
2016-05-26 05:28:08.062791: step 9210, loss = 0.33 (1008.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:28:10.570091: step 9220, loss = 0.17 (1069.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:28:13.048425: step 9230, loss = 0.44 (1021.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:28:15.545059: step 9240, loss = 0.24 (1027.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:28:17.989649: step 9250, loss = 0.27 (1098.6 examples/sec; 0.117 sec/batch)
2016-05-26 05:28:20.455729: step 9260, loss = 0.35 (1030.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:28:22.946083: step 9270, loss = 0.18 (1039.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:28:25.452844: step 9280, loss = 0.19 (1027.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:28:27.931665: step 9290, loss = 0.22 (1005.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:28:30.408446: step 9300, loss = 0.27 (1064.6 examples/sec; 0.120 sec/batch)
2016-05-26 05:28:33.170260: step 9310, loss = 0.23 (985.8 examples/sec; 0.130 sec/batch)
2016-05-26 05:28:35.676481: step 9320, loss = 0.24 (1063.1 examples/sec; 0.120 sec/batch)
2016-05-26 05:28:38.134283: step 9330, loss = 0.13 (1078.0 examples/sec; 0.119 sec/batch)
2016-05-26 05:28:40.558941: step 9340, loss = 0.27 (1051.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:28:43.023246: step 9350, loss = 0.22 (977.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:28:45.474270: step 9360, loss = 0.26 (1003.9 examples/sec; 0.128 sec/batch)
2016-05-26 05:28:48.019105: step 9370, loss = 0.17 (1011.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:28:50.499615: step 9380, loss = 0.35 (1080.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:28:52.986570: step 9390, loss = 0.31 (1016.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:28:55.420931: step 9400, loss = 0.22 (1099.5 examples/sec; 0.116 sec/batch)
2016-05-26 05:28:58.208758: step 9410, loss = 0.27 (1080.3 examples/sec; 0.118 sec/batch)
2016-05-26 05:29:00.686718: step 9420, loss = 0.20 (1020.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:29:03.186400: step 9430, loss = 0.34 (988.0 examples/sec; 0.130 sec/batch)
2016-05-26 05:29:05.654728: step 9440, loss = 0.22 (1023.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:29:08.089345: step 9450, loss = 0.19 (984.4 examples/sec; 0.130 sec/batch)
2016-05-26 05:29:10.547035: step 9460, loss = 0.22 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:29:13.026770: step 9470, loss = 0.25 (1040.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:29:15.493604: step 9480, loss = 0.22 (1010.1 examples/sec; 0.127 sec/batch)
2016-05-26 05:29:17.975940: step 9490, loss = 0.24 (985.0 examples/sec; 0.130 sec/batch)
2016-05-26 05:29:20.470726: step 9500, loss = 0.25 (996.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:29:23.229669: step 9510, loss = 0.19 (1080.0 examples/sec; 0.119 sec/batch)
2016-05-26 05:29:25.680539: step 9520, loss = 0.24 (1046.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:29:28.115373: step 9530, loss = 0.33 (1075.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:29:30.593079: step 9540, loss = 0.20 (977.9 examples/sec; 0.131 sec/batch)
2016-05-26 05:29:33.105882: step 9550, loss = 0.25 (1062.7 examples/sec; 0.120 sec/batch)
2016-05-26 05:29:35.593708: step 9560, loss = 0.22 (1070.1 examples/sec; 0.120 sec/batch)
2016-05-26 05:29:38.058561: step 9570, loss = 0.19 (1007.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:29:40.553418: step 9580, loss = 0.22 (970.8 examples/sec; 0.132 sec/batch)
2016-05-26 05:29:42.976868: step 9590, loss = 0.15 (1054.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:29:45.494399: step 9600, loss = 0.34 (951.8 examples/sec; 0.134 sec/batch)
2016-05-26 05:29:48.258488: step 9610, loss = 0.24 (1048.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:29:50.728150: step 9620, loss = 0.13 (1064.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:29:53.239356: step 9630, loss = 0.16 (1055.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:29:55.682829: step 9640, loss = 0.24 (1005.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:29:58.109487: step 9650, loss = 0.17 (1090.8 examples/sec; 0.117 sec/batch)
2016-05-26 05:30:00.492546: step 9660, loss = 0.18 (1120.6 examples/sec; 0.114 sec/batch)
2016-05-26 05:30:02.968871: step 9670, loss = 0.22 (1066.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:30:05.477975: step 9680, loss = 0.19 (990.0 examples/sec; 0.129 sec/batch)
2016-05-26 05:30:07.926058: step 9690, loss = 0.19 (1059.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:30:10.452474: step 9700, loss = 0.24 (1039.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:30:13.264839: step 9710, loss = 0.17 (999.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:30:15.733825: step 9720, loss = 0.27 (1039.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:30:18.249613: step 9730, loss = 0.32 (1046.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:30:20.748214: step 9740, loss = 0.29 (1061.5 examples/sec; 0.121 sec/batch)
2016-05-26 05:30:23.226757: step 9750, loss = 0.22 (1017.2 examples/sec; 0.126 sec/batch)
2016-05-26 05:30:25.757825: step 9760, loss = 0.21 (1062.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:30:28.231544: step 9770, loss = 0.19 (1050.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:30:30.724661: step 9780, loss = 0.27 (995.4 examples/sec; 0.129 sec/batch)
2016-05-26 05:30:33.281583: step 9790, loss = 0.27 (1047.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:30:35.774576: step 9800, loss = 0.10 (1013.1 examples/sec; 0.126 sec/batch)
2016-05-26 05:30:38.549665: step 9810, loss = 0.24 (1009.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:30:41.007995: step 9820, loss = 0.10 (1029.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:30:43.430272: step 9830, loss = 0.18 (1078.5 examples/sec; 0.119 sec/batch)
2016-05-26 05:30:45.910474: step 9840, loss = 0.23 (1082.1 examples/sec; 0.118 sec/batch)
2016-05-26 05:30:48.415824: step 9850, loss = 0.23 (1041.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:30:50.901219: step 9860, loss = 0.33 (1054.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:30:53.422862: step 9870, loss = 0.22 (1028.6 examples/sec; 0.124 sec/batch)
2016-05-26 05:30:55.889493: step 9880, loss = 0.16 (980.4 examples/sec; 0.131 sec/batch)
2016-05-26 05:30:58.352852: step 9890, loss = 0.25 (1059.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:31:00.833791: step 9900, loss = 0.28 (1074.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:31:03.624235: step 9910, loss = 0.34 (1078.9 examples/sec; 0.119 sec/batch)
2016-05-26 05:31:06.097056: step 9920, loss = 0.23 (1014.1 examples/sec; 0.126 sec/batch)
2016-05-26 05:31:08.631158: step 9930, loss = 0.22 (983.8 examples/sec; 0.130 sec/batch)
2016-05-26 05:31:11.087117: step 9940, loss = 0.22 (1082.9 examples/sec; 0.118 sec/batch)
2016-05-26 05:31:13.590780: step 9950, loss = 0.17 (1004.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:31:16.061839: step 9960, loss = 0.20 (1043.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:31:18.586638: step 9970, loss = 0.20 (978.9 examples/sec; 0.131 sec/batch)
2016-05-26 05:31:21.030933: step 9980, loss = 0.21 (1078.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:31:23.469931: step 9990, loss = 0.18 (1044.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:31:25.886320: step 10000, loss = 0.18 (1078.3 examples/sec; 0.119 sec/batch)
eval once
2016-05-26 05:32:07.368486: accuracy @ 1 = 0.921, 46070 / 50048 at 0
2016-05-26 05:32:09.896454: step 10010, loss = 0.19 (1069.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:32:12.333321: step 10020, loss = 0.21 (969.0 examples/sec; 0.132 sec/batch)
2016-05-26 05:32:14.801085: step 10030, loss = 0.16 (1056.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:32:17.268815: step 10040, loss = 0.20 (965.0 examples/sec; 0.133 sec/batch)
2016-05-26 05:32:19.710897: step 10050, loss = 0.09 (978.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:32:22.221277: step 10060, loss = 0.18 (1063.4 examples/sec; 0.120 sec/batch)
2016-05-26 05:32:24.701122: step 10070, loss = 0.23 (1074.6 examples/sec; 0.119 sec/batch)
2016-05-26 05:32:27.224991: step 10080, loss = 0.19 (989.9 examples/sec; 0.129 sec/batch)
2016-05-26 05:32:29.694689: step 10090, loss = 0.15 (1020.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:32:32.220233: step 10100, loss = 0.18 (1002.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:32:35.007432: step 10110, loss = 0.16 (1020.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:32:37.509079: step 10120, loss = 0.17 (1034.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:32:39.941450: step 10130, loss = 0.18 (1066.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:32:42.425841: step 10140, loss = 0.17 (1009.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:32:44.904181: step 10150, loss = 0.14 (1049.1 examples/sec; 0.122 sec/batch)
2016-05-26 05:32:47.430553: step 10160, loss = 0.24 (1027.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:32:49.978760: step 10170, loss = 0.24 (1019.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:32:52.479045: step 10180, loss = 0.13 (1007.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:32:54.955826: step 10190, loss = 0.19 (991.2 examples/sec; 0.129 sec/batch)
2016-05-26 05:32:57.504511: step 10200, loss = 0.22 (1054.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:33:00.253429: step 10210, loss = 0.25 (1007.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:33:02.724324: step 10220, loss = 0.38 (1054.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:33:05.228339: step 10230, loss = 0.23 (1014.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:33:07.785815: step 10240, loss = 0.23 (969.0 examples/sec; 0.132 sec/batch)
2016-05-26 05:33:10.258733: step 10250, loss = 0.19 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:33:12.788926: step 10260, loss = 0.15 (963.6 examples/sec; 0.133 sec/batch)
2016-05-26 05:33:15.263112: step 10270, loss = 0.22 (1056.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:33:17.768489: step 10280, loss = 0.15 (1000.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:33:20.274005: step 10290, loss = 0.17 (950.0 examples/sec; 0.135 sec/batch)
2016-05-26 05:33:22.775635: step 10300, loss = 0.17 (1050.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:33:25.847025: step 10310, loss = 0.21 (1007.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:33:28.310195: step 10320, loss = 0.24 (1041.3 examples/sec; 0.123 sec/batch)
2016-05-26 05:33:30.827440: step 10330, loss = 0.15 (1031.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:33:33.346702: step 10340, loss = 0.32 (1016.7 examples/sec; 0.126 sec/batch)
2016-05-26 05:33:35.863048: step 10350, loss = 0.15 (1067.6 examples/sec; 0.120 sec/batch)
2016-05-26 05:33:38.367723: step 10360, loss = 0.16 (1075.6 examples/sec; 0.119 sec/batch)
2016-05-26 05:33:40.829138: step 10370, loss = 0.26 (1060.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:33:43.333675: step 10380, loss = 0.09 (1031.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:33:45.855333: step 10390, loss = 0.22 (1033.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:33:48.400481: step 10400, loss = 0.16 (945.0 examples/sec; 0.135 sec/batch)
2016-05-26 05:33:51.225565: step 10410, loss = 0.16 (1056.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:33:53.720889: step 10420, loss = 0.22 (1053.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:33:56.193804: step 10430, loss = 0.16 (1079.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:33:58.668624: step 10440, loss = 0.27 (1053.1 examples/sec; 0.122 sec/batch)
2016-05-26 05:34:01.229757: step 10450, loss = 0.12 (930.6 examples/sec; 0.138 sec/batch)
2016-05-26 05:34:03.668758: step 10460, loss = 0.15 (1071.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:34:06.184184: step 10470, loss = 0.25 (1104.5 examples/sec; 0.116 sec/batch)
2016-05-26 05:34:08.623937: step 10480, loss = 0.17 (967.4 examples/sec; 0.132 sec/batch)
2016-05-26 05:34:11.126175: step 10490, loss = 0.23 (1061.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:34:13.559325: step 10500, loss = 0.20 (1067.1 examples/sec; 0.120 sec/batch)
2016-05-26 05:34:16.298366: step 10510, loss = 0.22 (1021.7 examples/sec; 0.125 sec/batch)
2016-05-26 05:34:18.727742: step 10520, loss = 0.12 (1073.5 examples/sec; 0.119 sec/batch)
2016-05-26 05:34:21.215179: step 10530, loss = 0.11 (1002.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:34:23.710615: step 10540, loss = 0.15 (960.8 examples/sec; 0.133 sec/batch)
2016-05-26 05:34:26.148242: step 10550, loss = 0.17 (1079.5 examples/sec; 0.119 sec/batch)
2016-05-26 05:34:28.621171: step 10560, loss = 0.13 (1028.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:34:31.111791: step 10570, loss = 0.23 (1070.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:34:33.587336: step 10580, loss = 0.08 (1032.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:34:36.051277: step 10590, loss = 0.17 (1062.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:34:38.554675: step 10600, loss = 0.14 (1007.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:34:41.330612: step 10610, loss = 0.19 (1043.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:34:43.819806: step 10620, loss = 0.10 (974.1 examples/sec; 0.131 sec/batch)
2016-05-26 05:34:46.280845: step 10630, loss = 0.16 (1029.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:34:48.748659: step 10640, loss = 0.20 (999.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:34:51.225032: step 10650, loss = 0.32 (1042.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:34:53.670181: step 10660, loss = 0.12 (1026.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:34:56.201516: step 10670, loss = 0.20 (1058.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:34:58.641904: step 10680, loss = 0.17 (1021.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:35:01.126616: step 10690, loss = 0.17 (1067.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:35:03.561472: step 10700, loss = 0.20 (1074.8 examples/sec; 0.119 sec/batch)
2016-05-26 05:35:06.428327: step 10710, loss = 0.16 (967.9 examples/sec; 0.132 sec/batch)
2016-05-26 05:35:08.896725: step 10720, loss = 0.13 (1019.1 examples/sec; 0.126 sec/batch)
2016-05-26 05:35:11.379259: step 10730, loss = 0.25 (1065.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:35:13.867448: step 10740, loss = 0.22 (1022.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:35:16.324195: step 10750, loss = 0.18 (1029.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:35:18.827877: step 10760, loss = 0.27 (989.8 examples/sec; 0.129 sec/batch)
2016-05-26 05:35:21.301428: step 10770, loss = 0.19 (986.1 examples/sec; 0.130 sec/batch)
2016-05-26 05:35:23.813042: step 10780, loss = 0.20 (1075.7 examples/sec; 0.119 sec/batch)
2016-05-26 05:35:26.270354: step 10790, loss = 0.15 (1003.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:35:28.808085: step 10800, loss = 0.16 (970.8 examples/sec; 0.132 sec/batch)
2016-05-26 05:35:31.547573: step 10810, loss = 0.18 (1068.7 examples/sec; 0.120 sec/batch)
2016-05-26 05:35:34.070069: step 10820, loss = 0.28 (996.1 examples/sec; 0.128 sec/batch)
2016-05-26 05:35:36.569315: step 10830, loss = 0.27 (1024.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:35:39.071569: step 10840, loss = 0.18 (1020.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:35:41.556603: step 10850, loss = 0.23 (1011.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:35:44.035817: step 10860, loss = 0.21 (1037.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:35:46.527400: step 10870, loss = 0.18 (981.0 examples/sec; 0.130 sec/batch)
2016-05-26 05:35:49.069413: step 10880, loss = 0.12 (998.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:35:51.553498: step 10890, loss = 0.21 (1058.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:35:54.069275: step 10900, loss = 0.18 (957.1 examples/sec; 0.134 sec/batch)
2016-05-26 05:35:56.921788: step 10910, loss = 0.13 (999.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:35:59.463334: step 10920, loss = 0.23 (987.7 examples/sec; 0.130 sec/batch)
2016-05-26 05:36:01.893334: step 10930, loss = 0.18 (1012.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:36:04.415804: step 10940, loss = 0.20 (1021.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:36:06.872048: step 10950, loss = 0.11 (1039.1 examples/sec; 0.123 sec/batch)
2016-05-26 05:36:09.304530: step 10960, loss = 0.20 (1039.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:36:11.756966: step 10970, loss = 0.14 (1041.3 examples/sec; 0.123 sec/batch)
2016-05-26 05:36:14.282901: step 10980, loss = 0.22 (1016.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:36:16.795653: step 10990, loss = 0.27 (1035.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:36:19.273444: step 11000, loss = 0.20 (977.1 examples/sec; 0.131 sec/batch)
eval once
2016-05-26 05:37:00.948877: accuracy @ 1 = 0.939, 46976 / 50048 at 0
2016-05-26 05:37:03.446352: step 11010, loss = 0.20 (992.5 examples/sec; 0.129 sec/batch)
2016-05-26 05:37:05.884638: step 11020, loss = 0.14 (1089.8 examples/sec; 0.117 sec/batch)
2016-05-26 05:37:08.351335: step 11030, loss = 0.17 (1001.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:37:10.820773: step 11040, loss = 0.17 (1102.1 examples/sec; 0.116 sec/batch)
2016-05-26 05:37:13.250663: step 11050, loss = 0.07 (1041.3 examples/sec; 0.123 sec/batch)
2016-05-26 05:37:15.678812: step 11060, loss = 0.13 (997.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:37:18.158019: step 11070, loss = 0.29 (1047.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:37:20.609453: step 11080, loss = 0.15 (1043.2 examples/sec; 0.123 sec/batch)
2016-05-26 05:37:23.097964: step 11090, loss = 0.14 (993.5 examples/sec; 0.129 sec/batch)
2016-05-26 05:37:25.545699: step 11100, loss = 0.20 (1085.6 examples/sec; 0.118 sec/batch)
2016-05-26 05:37:28.320830: step 11110, loss = 0.27 (1051.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:37:30.827759: step 11120, loss = 0.19 (990.9 examples/sec; 0.129 sec/batch)
2016-05-26 05:37:33.331727: step 11130, loss = 0.21 (976.4 examples/sec; 0.131 sec/batch)
2016-05-26 05:37:35.857841: step 11140, loss = 0.13 (957.6 examples/sec; 0.134 sec/batch)
2016-05-26 05:37:38.330535: step 11150, loss = 0.17 (973.8 examples/sec; 0.131 sec/batch)
2016-05-26 05:37:40.821473: step 11160, loss = 0.18 (1027.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:37:43.265279: step 11170, loss = 0.08 (1055.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:37:45.747013: step 11180, loss = 0.15 (1019.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:37:48.236687: step 11190, loss = 0.20 (1005.1 examples/sec; 0.127 sec/batch)
2016-05-26 05:37:50.712077: step 11200, loss = 0.12 (1035.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:37:53.553145: step 11210, loss = 0.22 (1024.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:37:56.014305: step 11220, loss = 0.12 (1011.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:37:58.490800: step 11230, loss = 0.13 (1002.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:38:00.960811: step 11240, loss = 0.27 (961.8 examples/sec; 0.133 sec/batch)
2016-05-26 05:38:03.449079: step 11250, loss = 0.25 (1014.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:38:05.922982: step 11260, loss = 0.18 (1002.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:38:08.369760: step 11270, loss = 0.14 (1063.6 examples/sec; 0.120 sec/batch)
2016-05-26 05:38:10.818915: step 11280, loss = 0.13 (1065.5 examples/sec; 0.120 sec/batch)
2016-05-26 05:38:13.317118: step 11290, loss = 0.21 (1023.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:38:15.716883: step 11300, loss = 0.18 (1104.0 examples/sec; 0.116 sec/batch)
2016-05-26 05:38:18.502385: step 11310, loss = 0.29 (975.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:38:21.000052: step 11320, loss = 0.33 (977.2 examples/sec; 0.131 sec/batch)
2016-05-26 05:38:23.501984: step 11330, loss = 0.10 (1051.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:38:26.009864: step 11340, loss = 0.15 (984.9 examples/sec; 0.130 sec/batch)
2016-05-26 05:38:28.554140: step 11350, loss = 0.10 (960.9 examples/sec; 0.133 sec/batch)
2016-05-26 05:38:31.057409: step 11360, loss = 0.11 (1008.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:38:33.523038: step 11370, loss = 0.23 (1073.8 examples/sec; 0.119 sec/batch)
2016-05-26 05:38:36.031620: step 11380, loss = 0.25 (972.3 examples/sec; 0.132 sec/batch)
2016-05-26 05:38:38.516267: step 11390, loss = 0.25 (1071.7 examples/sec; 0.119 sec/batch)
2016-05-26 05:38:40.965578: step 11400, loss = 0.29 (1057.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:38:43.767400: step 11410, loss = 0.17 (1014.0 examples/sec; 0.126 sec/batch)
2016-05-26 05:38:46.276107: step 11420, loss = 0.18 (1017.7 examples/sec; 0.126 sec/batch)
2016-05-26 05:38:48.847260: step 11430, loss = 0.14 (991.1 examples/sec; 0.129 sec/batch)
2016-05-26 05:38:51.294252: step 11440, loss = 0.17 (1047.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:38:53.736645: step 11450, loss = 0.18 (1034.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:38:56.197646: step 11460, loss = 0.17 (1033.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:38:58.738117: step 11470, loss = 0.18 (956.9 examples/sec; 0.134 sec/batch)
2016-05-26 05:39:01.204905: step 11480, loss = 0.18 (971.1 examples/sec; 0.132 sec/batch)
2016-05-26 05:39:03.685265: step 11490, loss = 0.17 (1019.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:39:06.162212: step 11500, loss = 0.17 (1033.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:39:08.954487: step 11510, loss = 0.14 (1002.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:39:11.442233: step 11520, loss = 0.18 (1050.7 examples/sec; 0.122 sec/batch)
2016-05-26 05:39:13.914135: step 11530, loss = 0.15 (1043.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:39:16.380317: step 11540, loss = 0.06 (1042.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:39:18.853203: step 11550, loss = 0.12 (1032.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:39:21.307090: step 11560, loss = 0.18 (1000.3 examples/sec; 0.128 sec/batch)
2016-05-26 05:39:23.788086: step 11570, loss = 0.19 (1028.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:39:26.370855: step 11580, loss = 0.15 (999.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:39:28.894198: step 11590, loss = 0.08 (1047.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:39:31.382617: step 11600, loss = 0.19 (1055.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:39:34.139744: step 11610, loss = 0.19 (988.8 examples/sec; 0.129 sec/batch)
2016-05-26 05:39:36.663064: step 11620, loss = 0.11 (1005.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:39:39.107209: step 11630, loss = 0.19 (1080.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:39:41.592861: step 11640, loss = 0.19 (1030.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:39:44.092553: step 11650, loss = 0.32 (1045.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:39:46.620505: step 11660, loss = 0.11 (1059.3 examples/sec; 0.121 sec/batch)
2016-05-26 05:39:49.154747: step 11670, loss = 0.14 (1081.7 examples/sec; 0.118 sec/batch)
2016-05-26 05:39:51.662407: step 11680, loss = 0.20 (1009.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:39:54.196842: step 11690, loss = 0.15 (951.9 examples/sec; 0.134 sec/batch)
2016-05-26 05:39:56.656497: step 11700, loss = 0.11 (1073.6 examples/sec; 0.119 sec/batch)
2016-05-26 05:39:59.439406: step 11710, loss = 0.15 (1016.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:40:01.961452: step 11720, loss = 0.09 (1006.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:40:04.477363: step 11730, loss = 0.15 (1037.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:40:06.955823: step 11740, loss = 0.22 (1021.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:40:09.446158: step 11750, loss = 0.13 (1049.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:40:11.914436: step 11760, loss = 0.11 (1051.7 examples/sec; 0.122 sec/batch)
2016-05-26 05:40:14.395565: step 11770, loss = 0.19 (1068.7 examples/sec; 0.120 sec/batch)
2016-05-26 05:40:16.870584: step 11780, loss = 0.16 (972.6 examples/sec; 0.132 sec/batch)
2016-05-26 05:40:19.392620: step 11790, loss = 0.11 (993.0 examples/sec; 0.129 sec/batch)
2016-05-26 05:40:21.862387: step 11800, loss = 0.17 (1069.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:40:24.697140: step 11810, loss = 0.15 (1087.0 examples/sec; 0.118 sec/batch)
2016-05-26 05:40:27.174626: step 11820, loss = 0.16 (977.8 examples/sec; 0.131 sec/batch)
2016-05-26 05:40:29.666774: step 11830, loss = 0.21 (1054.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:40:32.180910: step 11840, loss = 0.15 (1046.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:40:34.614067: step 11850, loss = 0.15 (1040.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:40:37.109016: step 11860, loss = 0.13 (992.8 examples/sec; 0.129 sec/batch)
2016-05-26 05:40:39.591874: step 11870, loss = 0.21 (1004.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:40:42.066854: step 11880, loss = 0.12 (1067.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:40:44.567333: step 11890, loss = 0.13 (965.6 examples/sec; 0.133 sec/batch)
2016-05-26 05:40:47.052617: step 11900, loss = 0.15 (1018.2 examples/sec; 0.126 sec/batch)
2016-05-26 05:40:49.830401: step 11910, loss = 0.10 (1000.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:40:52.309061: step 11920, loss = 0.16 (1003.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:40:54.806373: step 11930, loss = 0.15 (1051.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:40:57.303976: step 11940, loss = 0.10 (1016.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:40:59.834628: step 11950, loss = 0.11 (993.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:41:02.280556: step 11960, loss = 0.16 (1007.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:41:04.708300: step 11970, loss = 0.17 (1115.7 examples/sec; 0.115 sec/batch)
2016-05-26 05:41:07.233458: step 11980, loss = 0.13 (1051.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:41:09.683969: step 11990, loss = 0.08 (1086.8 examples/sec; 0.118 sec/batch)
2016-05-26 05:41:12.146676: step 12000, loss = 0.11 (977.6 examples/sec; 0.131 sec/batch)
eval once
2016-05-26 05:41:54.067906: accuracy @ 1 = 0.949, 47515 / 50048 at 0
2016-05-26 05:41:56.555354: step 12010, loss = 0.14 (1074.7 examples/sec; 0.119 sec/batch)
2016-05-26 05:41:59.055077: step 12020, loss = 0.12 (1032.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:42:01.538144: step 12030, loss = 0.16 (1027.7 examples/sec; 0.125 sec/batch)
2016-05-26 05:42:03.998968: step 12040, loss = 0.19 (1031.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:42:06.457878: step 12050, loss = 0.15 (1056.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:42:08.946856: step 12060, loss = 0.19 (1006.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:42:11.419140: step 12070, loss = 0.20 (1071.1 examples/sec; 0.120 sec/batch)
2016-05-26 05:42:13.920703: step 12080, loss = 0.15 (1039.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:42:16.362655: step 12090, loss = 0.09 (1054.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:42:18.847806: step 12100, loss = 0.10 (1027.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:42:21.624037: step 12110, loss = 0.12 (1082.9 examples/sec; 0.118 sec/batch)
2016-05-26 05:42:24.120472: step 12120, loss = 0.21 (999.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:42:26.665851: step 12130, loss = 0.14 (930.1 examples/sec; 0.138 sec/batch)
2016-05-26 05:42:29.162916: step 12140, loss = 0.12 (1032.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:42:31.662025: step 12150, loss = 0.18 (1050.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:42:34.150522: step 12160, loss = 0.12 (1025.3 examples/sec; 0.125 sec/batch)
2016-05-26 05:42:36.630203: step 12170, loss = 0.15 (1026.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:42:39.103666: step 12180, loss = 0.17 (1025.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:42:41.642451: step 12190, loss = 0.11 (1010.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:42:44.129980: step 12200, loss = 0.10 (1038.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:42:46.936340: step 12210, loss = 0.15 (1069.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:42:49.430295: step 12220, loss = 0.18 (988.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:42:51.901198: step 12230, loss = 0.24 (1082.1 examples/sec; 0.118 sec/batch)
2016-05-26 05:42:54.376155: step 12240, loss = 0.19 (1063.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:42:56.949197: step 12250, loss = 0.14 (1026.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:42:59.436450: step 12260, loss = 0.15 (1047.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:43:01.934348: step 12270, loss = 0.10 (968.2 examples/sec; 0.132 sec/batch)
2016-05-26 05:43:04.441212: step 12280, loss = 0.08 (1022.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:43:06.955789: step 12290, loss = 0.21 (1048.7 examples/sec; 0.122 sec/batch)
2016-05-26 05:43:09.450970: step 12300, loss = 0.16 (1024.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:43:12.271844: step 12310, loss = 0.14 (1017.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:43:14.808993: step 12320, loss = 0.14 (1030.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:43:17.278618: step 12330, loss = 0.12 (1038.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:43:19.760535: step 12340, loss = 0.14 (1075.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:43:22.203111: step 12350, loss = 0.14 (1036.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:43:24.711800: step 12360, loss = 0.11 (1005.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:43:27.164448: step 12370, loss = 0.11 (1037.1 examples/sec; 0.123 sec/batch)
2016-05-26 05:43:29.651132: step 12380, loss = 0.16 (1068.3 examples/sec; 0.120 sec/batch)
2016-05-26 05:43:32.064512: step 12390, loss = 0.19 (1031.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:43:34.526107: step 12400, loss = 0.19 (1097.3 examples/sec; 0.117 sec/batch)
2016-05-26 05:43:37.297370: step 12410, loss = 0.28 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:43:39.825392: step 12420, loss = 0.14 (1030.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:43:42.331748: step 12430, loss = 0.09 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 05:43:44.803476: step 12440, loss = 0.15 (1021.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:43:47.323337: step 12450, loss = 0.16 (1041.2 examples/sec; 0.123 sec/batch)
2016-05-26 05:43:49.850504: step 12460, loss = 0.24 (1044.1 examples/sec; 0.123 sec/batch)
2016-05-26 05:43:52.325913: step 12470, loss = 0.12 (1073.6 examples/sec; 0.119 sec/batch)
2016-05-26 05:43:54.770567: step 12480, loss = 0.08 (1023.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:43:57.198545: step 12490, loss = 0.14 (1090.3 examples/sec; 0.117 sec/batch)
2016-05-26 05:43:59.710657: step 12500, loss = 0.08 (1003.1 examples/sec; 0.128 sec/batch)
2016-05-26 05:44:02.466162: step 12510, loss = 0.13 (1034.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:44:04.953813: step 12520, loss = 0.18 (1078.0 examples/sec; 0.119 sec/batch)
2016-05-26 05:44:07.414871: step 12530, loss = 0.16 (1052.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:44:09.886241: step 12540, loss = 0.14 (1045.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:44:12.397287: step 12550, loss = 0.11 (997.2 examples/sec; 0.128 sec/batch)
2016-05-26 05:44:14.906333: step 12560, loss = 0.11 (988.4 examples/sec; 0.130 sec/batch)
2016-05-26 05:44:17.372225: step 12570, loss = 0.19 (1024.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:44:19.838523: step 12580, loss = 0.11 (1032.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:44:22.319865: step 12590, loss = 0.11 (1057.3 examples/sec; 0.121 sec/batch)
2016-05-26 05:44:24.790590: step 12600, loss = 0.11 (1081.6 examples/sec; 0.118 sec/batch)
2016-05-26 05:44:27.572740: step 12610, loss = 0.14 (988.8 examples/sec; 0.129 sec/batch)
2016-05-26 05:44:30.081878: step 12620, loss = 0.07 (967.6 examples/sec; 0.132 sec/batch)
2016-05-26 05:44:32.587166: step 12630, loss = 0.13 (1028.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:44:35.102810: step 12640, loss = 0.18 (970.4 examples/sec; 0.132 sec/batch)
2016-05-26 05:44:37.546363: step 12650, loss = 0.19 (1079.0 examples/sec; 0.119 sec/batch)
2016-05-26 05:44:40.042312: step 12660, loss = 0.11 (974.7 examples/sec; 0.131 sec/batch)
2016-05-26 05:44:42.587246: step 12670, loss = 0.14 (1025.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:44:45.073370: step 12680, loss = 0.14 (1059.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:44:47.509281: step 12690, loss = 0.14 (1045.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:44:49.992117: step 12700, loss = 0.10 (1087.2 examples/sec; 0.118 sec/batch)
2016-05-26 05:44:52.785141: step 12710, loss = 0.25 (1001.0 examples/sec; 0.128 sec/batch)
2016-05-26 05:44:55.273505: step 12720, loss = 0.13 (978.4 examples/sec; 0.131 sec/batch)
2016-05-26 05:44:57.808899: step 12730, loss = 0.17 (999.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:45:00.316209: step 12740, loss = 0.15 (1049.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:45:02.805296: step 12750, loss = 0.14 (1059.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:45:05.300485: step 12760, loss = 0.17 (1019.7 examples/sec; 0.126 sec/batch)
2016-05-26 05:45:07.779432: step 12770, loss = 0.10 (1013.8 examples/sec; 0.126 sec/batch)
2016-05-26 05:45:10.281631: step 12780, loss = 0.13 (1078.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:45:12.830453: step 12790, loss = 0.16 (1001.1 examples/sec; 0.128 sec/batch)
2016-05-26 05:45:15.364517: step 12800, loss = 0.10 (1004.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:45:18.150188: step 12810, loss = 0.20 (1014.5 examples/sec; 0.126 sec/batch)
2016-05-26 05:45:20.678608: step 12820, loss = 0.24 (1058.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:45:23.222569: step 12830, loss = 0.09 (977.1 examples/sec; 0.131 sec/batch)
2016-05-26 05:45:25.679034: step 12840, loss = 0.15 (1023.2 examples/sec; 0.125 sec/batch)
2016-05-26 05:45:28.180140: step 12850, loss = 0.14 (1023.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:45:30.672160: step 12860, loss = 0.08 (1030.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:45:33.162375: step 12870, loss = 0.19 (1055.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:45:35.622471: step 12880, loss = 0.09 (1095.1 examples/sec; 0.117 sec/batch)
2016-05-26 05:45:38.066631: step 12890, loss = 0.11 (1030.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:45:40.534833: step 12900, loss = 0.08 (981.2 examples/sec; 0.130 sec/batch)
2016-05-26 05:45:43.333523: step 12910, loss = 0.13 (1012.2 examples/sec; 0.126 sec/batch)
2016-05-26 05:45:45.821199: step 12920, loss = 0.16 (960.5 examples/sec; 0.133 sec/batch)
2016-05-26 05:45:48.358224: step 12930, loss = 0.15 (1029.3 examples/sec; 0.124 sec/batch)
2016-05-26 05:45:50.898763: step 12940, loss = 0.11 (989.2 examples/sec; 0.129 sec/batch)
2016-05-26 05:45:53.427467: step 12950, loss = 0.15 (958.9 examples/sec; 0.133 sec/batch)
2016-05-26 05:45:55.933381: step 12960, loss = 0.11 (1024.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:45:58.394694: step 12970, loss = 0.15 (1052.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:46:00.833941: step 12980, loss = 0.06 (998.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:46:03.349677: step 12990, loss = 0.07 (1004.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:46:05.793524: step 13000, loss = 0.09 (1044.7 examples/sec; 0.123 sec/batch)
eval once
2016-05-26 05:46:47.536295: accuracy @ 1 = 0.958, 47934 / 50048 at 0
2016-05-26 05:46:50.050882: step 13010, loss = 0.05 (1006.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:46:52.533888: step 13020, loss = 0.10 (1009.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:46:54.987262: step 13030, loss = 0.16 (1045.7 examples/sec; 0.122 sec/batch)
2016-05-26 05:46:57.427273: step 13040, loss = 0.16 (1054.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:46:59.926256: step 13050, loss = 0.18 (985.6 examples/sec; 0.130 sec/batch)
2016-05-26 05:47:02.433472: step 13060, loss = 0.15 (1037.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:47:04.907802: step 13070, loss = 0.09 (1072.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:47:07.393803: step 13080, loss = 0.20 (996.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:47:09.911320: step 13090, loss = 0.10 (973.0 examples/sec; 0.132 sec/batch)
2016-05-26 05:47:12.412015: step 13100, loss = 0.19 (987.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:47:15.216952: step 13110, loss = 0.09 (997.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:47:17.699187: step 13120, loss = 0.11 (988.5 examples/sec; 0.129 sec/batch)
2016-05-26 05:47:20.161960: step 13130, loss = 0.08 (985.1 examples/sec; 0.130 sec/batch)
2016-05-26 05:47:22.664568: step 13140, loss = 0.09 (1044.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:47:25.136801: step 13150, loss = 0.09 (998.7 examples/sec; 0.128 sec/batch)
2016-05-26 05:47:27.597292: step 13160, loss = 0.13 (1020.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:47:30.079090: step 13170, loss = 0.12 (1029.6 examples/sec; 0.124 sec/batch)
2016-05-26 05:47:32.579781: step 13180, loss = 0.09 (1024.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:47:35.082372: step 13190, loss = 0.09 (1014.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:47:37.549660: step 13200, loss = 0.10 (1044.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:47:40.349711: step 13210, loss = 0.12 (1030.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:47:42.833109: step 13220, loss = 0.16 (1040.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:47:45.312056: step 13230, loss = 0.15 (1056.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:47:47.818043: step 13240, loss = 0.10 (1056.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:47:50.273226: step 13250, loss = 0.12 (1065.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:47:52.805780: step 13260, loss = 0.17 (1020.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:47:55.284262: step 13270, loss = 0.10 (1037.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:47:57.757487: step 13280, loss = 0.08 (1014.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:48:00.220011: step 13290, loss = 0.14 (1003.1 examples/sec; 0.128 sec/batch)
2016-05-26 05:48:02.673268: step 13300, loss = 0.14 (1059.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:48:05.477764: step 13310, loss = 0.17 (1061.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:48:07.997430: step 13320, loss = 0.09 (1072.8 examples/sec; 0.119 sec/batch)
2016-05-26 05:48:10.470180: step 13330, loss = 0.13 (1065.7 examples/sec; 0.120 sec/batch)
2016-05-26 05:48:12.975878: step 13340, loss = 0.08 (1056.6 examples/sec; 0.121 sec/batch)
2016-05-26 05:48:15.459644: step 13350, loss = 0.13 (1073.6 examples/sec; 0.119 sec/batch)
2016-05-26 05:48:17.971089: step 13360, loss = 0.18 (1071.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:48:20.461755: step 13370, loss = 0.11 (1050.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:48:22.989361: step 13380, loss = 0.15 (1016.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:48:25.481196: step 13390, loss = 0.08 (1009.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:48:27.917073: step 13400, loss = 0.12 (1049.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:48:30.720468: step 13410, loss = 0.14 (1008.1 examples/sec; 0.127 sec/batch)
2016-05-26 05:48:33.213626: step 13420, loss = 0.15 (974.5 examples/sec; 0.131 sec/batch)
2016-05-26 05:48:35.722308: step 13430, loss = 0.07 (991.9 examples/sec; 0.129 sec/batch)
2016-05-26 05:48:38.201716: step 13440, loss = 0.09 (1022.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:48:40.665718: step 13450, loss = 0.09 (990.6 examples/sec; 0.129 sec/batch)
2016-05-26 05:48:43.130461: step 13460, loss = 0.10 (1115.1 examples/sec; 0.115 sec/batch)
2016-05-26 05:48:45.582620: step 13470, loss = 0.07 (1059.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:48:48.130675: step 13480, loss = 0.10 (1026.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:48:50.653487: step 13490, loss = 0.06 (977.1 examples/sec; 0.131 sec/batch)
2016-05-26 05:48:53.153258: step 13500, loss = 0.06 (994.8 examples/sec; 0.129 sec/batch)
2016-05-26 05:48:55.986213: step 13510, loss = 0.08 (1012.2 examples/sec; 0.126 sec/batch)
2016-05-26 05:48:58.492810: step 13520, loss = 0.16 (1004.1 examples/sec; 0.127 sec/batch)
2016-05-26 05:49:01.044325: step 13530, loss = 0.12 (1023.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:49:03.497004: step 13540, loss = 0.10 (1010.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:49:05.924947: step 13550, loss = 0.13 (1085.8 examples/sec; 0.118 sec/batch)
2016-05-26 05:49:08.449574: step 13560, loss = 0.10 (1057.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:49:10.953612: step 13570, loss = 0.10 (998.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:49:13.440649: step 13580, loss = 0.11 (1072.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:49:15.909164: step 13590, loss = 0.10 (1024.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:49:18.361192: step 13600, loss = 0.19 (1044.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:49:21.246107: step 13610, loss = 0.09 (958.0 examples/sec; 0.134 sec/batch)
2016-05-26 05:49:23.763393: step 13620, loss = 0.11 (981.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:49:26.258937: step 13630, loss = 0.16 (1015.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:49:28.763838: step 13640, loss = 0.10 (1022.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:49:31.310283: step 13650, loss = 0.06 (1001.6 examples/sec; 0.128 sec/batch)
2016-05-26 05:49:33.807529: step 13660, loss = 0.09 (1086.2 examples/sec; 0.118 sec/batch)
2016-05-26 05:49:36.271953: step 13670, loss = 0.19 (1053.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:49:38.757120: step 13680, loss = 0.11 (1100.1 examples/sec; 0.116 sec/batch)
2016-05-26 05:49:41.208948: step 13690, loss = 0.07 (1005.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:49:43.713199: step 13700, loss = 0.11 (1064.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:49:46.500637: step 13710, loss = 0.15 (1030.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:49:49.002003: step 13720, loss = 0.12 (1054.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:49:51.487808: step 13730, loss = 0.09 (1076.7 examples/sec; 0.119 sec/batch)
2016-05-26 05:49:53.980845: step 13740, loss = 0.09 (1083.4 examples/sec; 0.118 sec/batch)
2016-05-26 05:49:56.466379: step 13750, loss = 0.10 (986.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:49:58.967426: step 13760, loss = 0.09 (983.0 examples/sec; 0.130 sec/batch)
2016-05-26 05:50:01.463792: step 13770, loss = 0.07 (984.9 examples/sec; 0.130 sec/batch)
2016-05-26 05:50:03.983774: step 13780, loss = 0.13 (963.9 examples/sec; 0.133 sec/batch)
2016-05-26 05:50:06.513103: step 13790, loss = 0.14 (966.5 examples/sec; 0.132 sec/batch)
2016-05-26 05:50:08.984337: step 13800, loss = 0.13 (1035.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:50:11.748693: step 13810, loss = 0.12 (1052.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:50:14.189529: step 13820, loss = 0.14 (957.1 examples/sec; 0.134 sec/batch)
2016-05-26 05:50:16.672229: step 13830, loss = 0.05 (1008.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:50:19.225828: step 13840, loss = 0.10 (971.1 examples/sec; 0.132 sec/batch)
2016-05-26 05:50:21.745811: step 13850, loss = 0.08 (1027.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:50:24.194595: step 13860, loss = 0.10 (1028.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:50:26.663823: step 13870, loss = 0.08 (1004.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:50:29.132363: step 13880, loss = 0.07 (1092.9 examples/sec; 0.117 sec/batch)
2016-05-26 05:50:31.646842: step 13890, loss = 0.14 (1065.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:50:34.119756: step 13900, loss = 0.11 (1042.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:50:36.893066: step 13910, loss = 0.09 (1077.3 examples/sec; 0.119 sec/batch)
2016-05-26 05:50:39.391569: step 13920, loss = 0.13 (1072.2 examples/sec; 0.119 sec/batch)
2016-05-26 05:50:41.873733: step 13930, loss = 0.10 (1090.6 examples/sec; 0.117 sec/batch)
2016-05-26 05:50:44.358542: step 13940, loss = 0.08 (1002.8 examples/sec; 0.128 sec/batch)
2016-05-26 05:50:46.835139: step 13950, loss = 0.10 (951.7 examples/sec; 0.135 sec/batch)
2016-05-26 05:50:49.308400: step 13960, loss = 0.10 (1061.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:50:51.852039: step 13970, loss = 0.06 (998.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:50:54.345133: step 13980, loss = 0.15 (1068.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:50:56.789409: step 13990, loss = 0.09 (1050.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:50:59.263807: step 14000, loss = 0.16 (1087.6 examples/sec; 0.118 sec/batch)
eval once
2016-05-26 05:51:40.875920: accuracy @ 1 = 0.961, 48091 / 50048 at 0
2016-05-26 05:51:43.430973: step 14010, loss = 0.09 (1063.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:51:45.913928: step 14020, loss = 0.10 (1101.6 examples/sec; 0.116 sec/batch)
2016-05-26 05:51:48.403862: step 14030, loss = 0.10 (1019.2 examples/sec; 0.126 sec/batch)
2016-05-26 05:51:50.869258: step 14040, loss = 0.11 (958.7 examples/sec; 0.134 sec/batch)
2016-05-26 05:51:53.349158: step 14050, loss = 0.09 (1026.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:51:55.816226: step 14060, loss = 0.06 (988.0 examples/sec; 0.130 sec/batch)
2016-05-26 05:51:58.252415: step 14070, loss = 0.11 (1104.3 examples/sec; 0.116 sec/batch)
2016-05-26 05:52:00.735837: step 14080, loss = 0.16 (1008.1 examples/sec; 0.127 sec/batch)
2016-05-26 05:52:03.246081: step 14090, loss = 0.11 (1062.3 examples/sec; 0.120 sec/batch)
2016-05-26 05:52:05.676698: step 14100, loss = 0.08 (1097.0 examples/sec; 0.117 sec/batch)
2016-05-26 05:52:08.454989: step 14110, loss = 0.05 (951.2 examples/sec; 0.135 sec/batch)
2016-05-26 05:52:10.984314: step 14120, loss = 0.12 (963.7 examples/sec; 0.133 sec/batch)
2016-05-26 05:52:13.434250: step 14130, loss = 0.16 (1124.7 examples/sec; 0.114 sec/batch)
2016-05-26 05:52:15.888387: step 14140, loss = 0.09 (1046.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:52:18.355968: step 14150, loss = 0.10 (1037.9 examples/sec; 0.123 sec/batch)
2016-05-26 05:52:20.884611: step 14160, loss = 0.16 (1057.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:52:23.328018: step 14170, loss = 0.11 (1032.5 examples/sec; 0.124 sec/batch)
2016-05-26 05:52:25.840329: step 14180, loss = 0.18 (1066.9 examples/sec; 0.120 sec/batch)
2016-05-26 05:52:28.337104: step 14190, loss = 0.12 (999.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:52:30.802176: step 14200, loss = 0.11 (1009.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:52:33.574599: step 14210, loss = 0.09 (1024.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:52:36.097215: step 14220, loss = 0.15 (1063.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:52:38.493647: step 14230, loss = 0.09 (1113.1 examples/sec; 0.115 sec/batch)
2016-05-26 05:52:40.908758: step 14240, loss = 0.09 (1077.9 examples/sec; 0.119 sec/batch)
2016-05-26 05:52:43.373968: step 14250, loss = 0.11 (1037.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:52:45.845466: step 14260, loss = 0.06 (1006.9 examples/sec; 0.127 sec/batch)
2016-05-26 05:52:48.295403: step 14270, loss = 0.11 (1079.5 examples/sec; 0.119 sec/batch)
2016-05-26 05:52:50.809414: step 14280, loss = 0.08 (981.1 examples/sec; 0.130 sec/batch)
2016-05-26 05:52:53.267381: step 14290, loss = 0.13 (1055.0 examples/sec; 0.121 sec/batch)
2016-05-26 05:52:55.770470: step 14300, loss = 0.09 (1084.7 examples/sec; 0.118 sec/batch)
2016-05-26 05:52:58.579300: step 14310, loss = 0.12 (1049.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:53:01.071094: step 14320, loss = 0.11 (992.9 examples/sec; 0.129 sec/batch)
2016-05-26 05:53:03.545747: step 14330, loss = 0.06 (967.6 examples/sec; 0.132 sec/batch)
2016-05-26 05:53:06.010567: step 14340, loss = 0.04 (1056.9 examples/sec; 0.121 sec/batch)
2016-05-26 05:53:08.484549: step 14350, loss = 0.11 (997.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:53:11.078704: step 14360, loss = 0.10 (996.1 examples/sec; 0.129 sec/batch)
2016-05-26 05:53:13.627551: step 14370, loss = 0.08 (1031.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:53:16.172809: step 14380, loss = 0.08 (1043.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:53:18.748994: step 14390, loss = 0.12 (1014.7 examples/sec; 0.126 sec/batch)
2016-05-26 05:53:21.212531: step 14400, loss = 0.14 (978.4 examples/sec; 0.131 sec/batch)
2016-05-26 05:53:24.007046: step 14410, loss = 0.08 (1056.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:53:26.492748: step 14420, loss = 0.15 (1054.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:53:28.919775: step 14430, loss = 0.09 (1078.7 examples/sec; 0.119 sec/batch)
2016-05-26 05:53:31.370124: step 14440, loss = 0.13 (1006.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:53:33.829423: step 14450, loss = 0.11 (1041.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:53:36.293525: step 14460, loss = 0.10 (962.6 examples/sec; 0.133 sec/batch)
2016-05-26 05:53:38.812199: step 14470, loss = 0.03 (1011.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:53:41.298005: step 14480, loss = 0.11 (1033.1 examples/sec; 0.124 sec/batch)
2016-05-26 05:53:43.829994: step 14490, loss = 0.10 (1040.4 examples/sec; 0.123 sec/batch)
2016-05-26 05:53:46.322688: step 14500, loss = 0.04 (1006.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:53:49.132167: step 14510, loss = 0.10 (1055.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:53:51.584002: step 14520, loss = 0.10 (1017.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:53:54.082752: step 14530, loss = 0.11 (999.4 examples/sec; 0.128 sec/batch)
2016-05-26 05:53:56.553672: step 14540, loss = 0.07 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:53:59.034893: step 14550, loss = 0.11 (1070.6 examples/sec; 0.120 sec/batch)
2016-05-26 05:54:01.538177: step 14560, loss = 0.16 (1093.4 examples/sec; 0.117 sec/batch)
2016-05-26 05:54:04.021842: step 14570, loss = 0.10 (1086.3 examples/sec; 0.118 sec/batch)
2016-05-26 05:54:06.544448: step 14580, loss = 0.16 (989.3 examples/sec; 0.129 sec/batch)
2016-05-26 05:54:09.064133: step 14590, loss = 0.02 (986.9 examples/sec; 0.130 sec/batch)
2016-05-26 05:54:11.550593: step 14600, loss = 0.14 (1033.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:54:14.359721: step 14610, loss = 0.11 (1103.7 examples/sec; 0.116 sec/batch)
2016-05-26 05:54:16.865951: step 14620, loss = 0.06 (993.2 examples/sec; 0.129 sec/batch)
2016-05-26 05:54:19.297079: step 14630, loss = 0.04 (1026.1 examples/sec; 0.125 sec/batch)
2016-05-26 05:54:21.824893: step 14640, loss = 0.05 (1054.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:54:24.354509: step 14650, loss = 0.07 (1012.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:54:26.867870: step 14660, loss = 0.07 (1011.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:54:29.391164: step 14670, loss = 0.08 (1038.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:54:31.886232: step 14680, loss = 0.18 (1084.5 examples/sec; 0.118 sec/batch)
2016-05-26 05:54:34.329733: step 14690, loss = 0.09 (1068.2 examples/sec; 0.120 sec/batch)
2016-05-26 05:54:36.835867: step 14700, loss = 0.16 (1057.8 examples/sec; 0.121 sec/batch)
2016-05-26 05:54:39.610430: step 14710, loss = 0.11 (1027.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:54:42.092453: step 14720, loss = 0.15 (1015.7 examples/sec; 0.126 sec/batch)
2016-05-26 05:54:44.545832: step 14730, loss = 0.14 (985.0 examples/sec; 0.130 sec/batch)
2016-05-26 05:54:47.077730: step 14740, loss = 0.08 (989.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:54:49.572459: step 14750, loss = 0.09 (1072.1 examples/sec; 0.119 sec/batch)
2016-05-26 05:54:52.069226: step 14760, loss = 0.09 (1023.8 examples/sec; 0.125 sec/batch)
2016-05-26 05:54:54.573163: step 14770, loss = 0.15 (1002.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:54:57.062861: step 14780, loss = 0.06 (1057.4 examples/sec; 0.121 sec/batch)
2016-05-26 05:54:59.578830: step 14790, loss = 0.09 (1013.6 examples/sec; 0.126 sec/batch)
2016-05-26 05:55:02.034858: step 14800, loss = 0.06 (1029.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:55:04.851991: step 14810, loss = 0.08 (994.5 examples/sec; 0.129 sec/batch)
2016-05-26 05:55:07.344906: step 14820, loss = 0.09 (1025.2 examples/sec; 0.125 sec/batch)
2016-05-26 05:55:09.888095: step 14830, loss = 0.07 (991.7 examples/sec; 0.129 sec/batch)
2016-05-26 05:55:12.351229: step 14840, loss = 0.07 (1052.4 examples/sec; 0.122 sec/batch)
2016-05-26 05:55:14.835819: step 14850, loss = 0.08 (984.7 examples/sec; 0.130 sec/batch)
2016-05-26 05:55:17.342427: step 14860, loss = 0.08 (944.7 examples/sec; 0.135 sec/batch)
2016-05-26 05:55:19.805491: step 14870, loss = 0.14 (1035.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:55:22.255669: step 14880, loss = 0.10 (1090.9 examples/sec; 0.117 sec/batch)
2016-05-26 05:55:24.715687: step 14890, loss = 0.15 (1062.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:55:27.217789: step 14900, loss = 0.08 (1044.0 examples/sec; 0.123 sec/batch)
2016-05-26 05:55:30.041095: step 14910, loss = 0.05 (1009.4 examples/sec; 0.127 sec/batch)
2016-05-26 05:55:32.493930: step 14920, loss = 0.04 (1106.8 examples/sec; 0.116 sec/batch)
2016-05-26 05:55:34.961756: step 14930, loss = 0.10 (1020.9 examples/sec; 0.125 sec/batch)
2016-05-26 05:55:37.450670: step 14940, loss = 0.09 (994.0 examples/sec; 0.129 sec/batch)
2016-05-26 05:55:39.973535: step 14950, loss = 0.10 (1009.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:55:42.458107: step 14960, loss = 0.14 (1033.8 examples/sec; 0.124 sec/batch)
2016-05-26 05:55:44.925642: step 14970, loss = 0.05 (1006.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:55:47.428678: step 14980, loss = 0.09 (1028.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:55:49.950609: step 14990, loss = 0.10 (971.6 examples/sec; 0.132 sec/batch)
2016-05-26 05:55:52.333479: step 15000, loss = 0.07 (1048.0 examples/sec; 0.122 sec/batch)
eval once
2016-05-26 05:56:34.420722: accuracy @ 1 = 0.973, 48702 / 50048 at 0
2016-05-26 05:56:36.913335: step 15010, loss = 0.08 (1010.2 examples/sec; 0.127 sec/batch)
2016-05-26 05:56:39.370864: step 15020, loss = 0.06 (1031.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:56:41.829999: step 15030, loss = 0.07 (1096.6 examples/sec; 0.117 sec/batch)
2016-05-26 05:56:44.331572: step 15040, loss = 0.06 (1027.5 examples/sec; 0.125 sec/batch)
2016-05-26 05:56:46.751243: step 15050, loss = 0.18 (1012.9 examples/sec; 0.126 sec/batch)
2016-05-26 05:56:49.248962: step 15060, loss = 0.09 (1070.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:56:51.752890: step 15070, loss = 0.07 (1043.4 examples/sec; 0.123 sec/batch)
2016-05-26 05:56:54.206361: step 15080, loss = 0.07 (1097.3 examples/sec; 0.117 sec/batch)
2016-05-26 05:56:56.632089: step 15090, loss = 0.07 (1006.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:56:59.130858: step 15100, loss = 0.07 (959.6 examples/sec; 0.133 sec/batch)
2016-05-26 05:57:01.922241: step 15110, loss = 0.07 (1047.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:57:04.381254: step 15120, loss = 0.14 (1031.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:57:06.862185: step 15130, loss = 0.10 (991.2 examples/sec; 0.129 sec/batch)
2016-05-26 05:57:09.333327: step 15140, loss = 0.10 (1030.7 examples/sec; 0.124 sec/batch)
2016-05-26 05:57:11.783722: step 15150, loss = 0.05 (1024.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:57:14.247562: step 15160, loss = 0.17 (1011.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:57:16.762196: step 15170, loss = 0.04 (1023.3 examples/sec; 0.125 sec/batch)
2016-05-26 05:57:19.190063: step 15180, loss = 0.05 (1075.9 examples/sec; 0.119 sec/batch)
2016-05-26 05:57:21.612553: step 15190, loss = 0.12 (1081.3 examples/sec; 0.118 sec/batch)
2016-05-26 05:57:24.153441: step 15200, loss = 0.07 (1007.6 examples/sec; 0.127 sec/batch)
2016-05-26 05:57:26.998539: step 15210, loss = 0.07 (1040.4 examples/sec; 0.123 sec/batch)
2016-05-26 05:57:29.495660: step 15220, loss = 0.09 (1052.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:57:31.978794: step 15230, loss = 0.07 (1022.4 examples/sec; 0.125 sec/batch)
2016-05-26 05:57:34.512274: step 15240, loss = 0.12 (1004.8 examples/sec; 0.127 sec/batch)
2016-05-26 05:57:36.986133: step 15250, loss = 0.10 (1070.7 examples/sec; 0.120 sec/batch)
2016-05-26 05:57:39.471253: step 15260, loss = 0.10 (1046.3 examples/sec; 0.122 sec/batch)
2016-05-26 05:57:41.982656: step 15270, loss = 0.08 (1007.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:57:44.479307: step 15280, loss = 0.08 (1000.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:57:46.975703: step 15290, loss = 0.10 (1007.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:57:49.453609: step 15300, loss = 0.07 (1056.3 examples/sec; 0.121 sec/batch)
2016-05-26 05:57:52.300742: step 15310, loss = 0.04 (960.9 examples/sec; 0.133 sec/batch)
2016-05-26 05:57:54.748319: step 15320, loss = 0.04 (1010.3 examples/sec; 0.127 sec/batch)
2016-05-26 05:57:57.293894: step 15330, loss = 0.08 (1048.0 examples/sec; 0.122 sec/batch)
2016-05-26 05:57:59.805063: step 15340, loss = 0.11 (1005.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:58:02.317031: step 15350, loss = 0.06 (1037.2 examples/sec; 0.123 sec/batch)
2016-05-26 05:58:04.856378: step 15360, loss = 0.10 (982.2 examples/sec; 0.130 sec/batch)
2016-05-26 05:58:07.310719: step 15370, loss = 0.12 (1035.2 examples/sec; 0.124 sec/batch)
2016-05-26 05:58:09.764148: step 15380, loss = 0.10 (1089.0 examples/sec; 0.118 sec/batch)
2016-05-26 05:58:12.267632: step 15390, loss = 0.05 (1089.4 examples/sec; 0.117 sec/batch)
2016-05-26 05:58:14.813867: step 15400, loss = 0.11 (973.5 examples/sec; 0.131 sec/batch)
2016-05-26 05:58:17.660659: step 15410, loss = 0.05 (1013.4 examples/sec; 0.126 sec/batch)
2016-05-26 05:58:20.111648: step 15420, loss = 0.06 (972.7 examples/sec; 0.132 sec/batch)
2016-05-26 05:58:22.588075: step 15430, loss = 0.11 (1079.7 examples/sec; 0.119 sec/batch)
2016-05-26 05:58:25.090075: step 15440, loss = 0.08 (1008.7 examples/sec; 0.127 sec/batch)
2016-05-26 05:58:27.584195: step 15450, loss = 0.08 (1005.5 examples/sec; 0.127 sec/batch)
2016-05-26 05:58:30.099451: step 15460, loss = 0.08 (1040.6 examples/sec; 0.123 sec/batch)
2016-05-26 05:58:32.639047: step 15470, loss = 0.11 (998.5 examples/sec; 0.128 sec/batch)
2016-05-26 05:58:35.124196: step 15480, loss = 0.18 (1034.0 examples/sec; 0.124 sec/batch)
2016-05-26 05:58:37.572375: step 15490, loss = 0.02 (1042.3 examples/sec; 0.123 sec/batch)
2016-05-26 05:58:40.096193: step 15500, loss = 0.06 (1049.5 examples/sec; 0.122 sec/batch)
2016-05-26 05:58:42.878252: step 15510, loss = 0.05 (1009.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:58:45.424703: step 15520, loss = 0.09 (1020.0 examples/sec; 0.125 sec/batch)
2016-05-26 05:58:47.938295: step 15530, loss = 0.08 (1077.5 examples/sec; 0.119 sec/batch)
2016-05-26 05:58:50.419484: step 15540, loss = 0.05 (1041.2 examples/sec; 0.123 sec/batch)
2016-05-26 05:58:52.936504: step 15550, loss = 0.09 (1021.6 examples/sec; 0.125 sec/batch)
2016-05-26 05:58:55.405519: step 15560, loss = 0.09 (1068.8 examples/sec; 0.120 sec/batch)
2016-05-26 05:58:57.870896: step 15570, loss = 0.06 (1071.0 examples/sec; 0.120 sec/batch)
2016-05-26 05:59:00.356641: step 15580, loss = 0.10 (1037.3 examples/sec; 0.123 sec/batch)
2016-05-26 05:59:02.833729: step 15590, loss = 0.06 (1045.6 examples/sec; 0.122 sec/batch)
2016-05-26 05:59:05.332155: step 15600, loss = 0.13 (1046.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:59:08.151703: step 15610, loss = 0.06 (1081.6 examples/sec; 0.118 sec/batch)
2016-05-26 05:59:10.676497: step 15620, loss = 0.11 (1055.7 examples/sec; 0.121 sec/batch)
2016-05-26 05:59:13.145368: step 15630, loss = 0.08 (1056.1 examples/sec; 0.121 sec/batch)
2016-05-26 05:59:15.689366: step 15640, loss = 0.09 (993.0 examples/sec; 0.129 sec/batch)
2016-05-26 05:59:18.170682: step 15650, loss = 0.06 (1029.6 examples/sec; 0.124 sec/batch)
2016-05-26 05:59:20.703148: step 15660, loss = 0.06 (1082.4 examples/sec; 0.118 sec/batch)
2016-05-26 05:59:23.231783: step 15670, loss = 0.05 (986.5 examples/sec; 0.130 sec/batch)
2016-05-26 05:59:25.714858: step 15680, loss = 0.09 (1042.5 examples/sec; 0.123 sec/batch)
2016-05-26 05:59:28.172580: step 15690, loss = 0.14 (1068.1 examples/sec; 0.120 sec/batch)
2016-05-26 05:59:30.646456: step 15700, loss = 0.08 (947.0 examples/sec; 0.135 sec/batch)
2016-05-26 05:59:33.443133: step 15710, loss = 0.09 (1086.0 examples/sec; 0.118 sec/batch)
2016-05-26 05:59:35.861123: step 15720, loss = 0.03 (1046.9 examples/sec; 0.122 sec/batch)
2016-05-26 05:59:38.376507: step 15730, loss = 0.15 (1034.4 examples/sec; 0.124 sec/batch)
2016-05-26 05:59:40.896176: step 15740, loss = 0.08 (1013.3 examples/sec; 0.126 sec/batch)
2016-05-26 05:59:43.426493: step 15750, loss = 0.14 (1048.2 examples/sec; 0.122 sec/batch)
2016-05-26 05:59:45.983983: step 15760, loss = 0.06 (1009.0 examples/sec; 0.127 sec/batch)
2016-05-26 05:59:48.437953: step 15770, loss = 0.05 (1056.2 examples/sec; 0.121 sec/batch)
2016-05-26 05:59:50.903760: step 15780, loss = 0.12 (1089.2 examples/sec; 0.118 sec/batch)
2016-05-26 05:59:53.388608: step 15790, loss = 0.08 (1038.8 examples/sec; 0.123 sec/batch)
2016-05-26 05:59:55.858456: step 15800, loss = 0.06 (1046.8 examples/sec; 0.122 sec/batch)
2016-05-26 05:59:58.643180: step 15810, loss = 0.06 (1014.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:00:01.115829: step 15820, loss = 0.10 (1031.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:00:03.632718: step 15830, loss = 0.06 (1008.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:00:06.120728: step 15840, loss = 0.05 (1047.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:00:08.588932: step 15850, loss = 0.05 (1024.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:00:11.085567: step 15860, loss = 0.07 (1012.2 examples/sec; 0.126 sec/batch)
2016-05-26 06:00:13.564485: step 15870, loss = 0.07 (1089.6 examples/sec; 0.117 sec/batch)
2016-05-26 06:00:16.075293: step 15880, loss = 0.04 (1028.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:00:18.613548: step 15890, loss = 0.09 (977.1 examples/sec; 0.131 sec/batch)
2016-05-26 06:00:21.104061: step 15900, loss = 0.11 (1006.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:00:23.933440: step 15910, loss = 0.13 (976.0 examples/sec; 0.131 sec/batch)
2016-05-26 06:00:26.411438: step 15920, loss = 0.06 (1102.0 examples/sec; 0.116 sec/batch)
2016-05-26 06:00:28.897297: step 15930, loss = 0.05 (1066.6 examples/sec; 0.120 sec/batch)
2016-05-26 06:00:31.434176: step 15940, loss = 0.03 (1007.6 examples/sec; 0.127 sec/batch)
2016-05-26 06:00:33.927178: step 15950, loss = 0.08 (1026.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:00:36.391149: step 15960, loss = 0.10 (1030.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:00:38.855469: step 15970, loss = 0.04 (1022.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:00:41.367584: step 15980, loss = 0.03 (1009.0 examples/sec; 0.127 sec/batch)
2016-05-26 06:00:43.858882: step 15990, loss = 0.10 (1012.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:00:46.349169: step 16000, loss = 0.05 (1036.9 examples/sec; 0.123 sec/batch)
eval once
2016-05-26 06:01:28.374860: accuracy @ 1 = 0.978, 48949 / 50048 at 0
2016-05-26 06:01:30.906223: step 16010, loss = 0.06 (1026.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:01:33.461219: step 16020, loss = 0.04 (1017.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:01:35.931804: step 16030, loss = 0.07 (1066.9 examples/sec; 0.120 sec/batch)
2016-05-26 06:01:38.418855: step 16040, loss = 0.15 (1068.2 examples/sec; 0.120 sec/batch)
2016-05-26 06:01:40.938562: step 16050, loss = 0.04 (986.3 examples/sec; 0.130 sec/batch)
2016-05-26 06:01:43.416862: step 16060, loss = 0.05 (1034.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:01:45.850276: step 16070, loss = 0.06 (1086.6 examples/sec; 0.118 sec/batch)
2016-05-26 06:01:48.336646: step 16080, loss = 0.09 (1003.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:01:50.858687: step 16090, loss = 0.09 (1049.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:01:53.365051: step 16100, loss = 0.07 (975.5 examples/sec; 0.131 sec/batch)
2016-05-26 06:01:56.273247: step 16110, loss = 0.06 (1004.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:01:58.788407: step 16120, loss = 0.11 (992.4 examples/sec; 0.129 sec/batch)
2016-05-26 06:02:01.232857: step 16130, loss = 0.08 (1060.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:02:03.700082: step 16140, loss = 0.09 (1047.9 examples/sec; 0.122 sec/batch)
2016-05-26 06:02:06.175017: step 16150, loss = 0.04 (996.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:02:08.687571: step 16160, loss = 0.09 (1013.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:02:11.181952: step 16170, loss = 0.07 (1037.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:02:13.666448: step 16180, loss = 0.04 (1001.2 examples/sec; 0.128 sec/batch)
2016-05-26 06:02:16.112380: step 16190, loss = 0.16 (1039.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:02:18.633982: step 16200, loss = 0.05 (986.0 examples/sec; 0.130 sec/batch)
2016-05-26 06:02:21.446459: step 16210, loss = 0.05 (1019.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:02:23.866436: step 16220, loss = 0.03 (1047.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:02:26.416838: step 16230, loss = 0.07 (991.5 examples/sec; 0.129 sec/batch)
2016-05-26 06:02:28.965197: step 16240, loss = 0.04 (1000.1 examples/sec; 0.128 sec/batch)
2016-05-26 06:02:31.446449: step 16250, loss = 0.05 (1041.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:02:33.935904: step 16260, loss = 0.04 (1076.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:02:36.437193: step 16270, loss = 0.06 (975.5 examples/sec; 0.131 sec/batch)
2016-05-26 06:02:38.873671: step 16280, loss = 0.04 (1021.2 examples/sec; 0.125 sec/batch)
2016-05-26 06:02:41.319189: step 16290, loss = 0.02 (1051.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:02:43.793518: step 16300, loss = 0.11 (1032.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:02:46.632555: step 16310, loss = 0.06 (995.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:02:49.065894: step 16320, loss = 0.04 (1116.8 examples/sec; 0.115 sec/batch)
2016-05-26 06:02:51.523045: step 16330, loss = 0.04 (1086.7 examples/sec; 0.118 sec/batch)
2016-05-26 06:02:54.036884: step 16340, loss = 0.05 (1068.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:02:56.521197: step 16350, loss = 0.04 (1054.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:02:59.038142: step 16360, loss = 0.05 (1015.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:03:01.557728: step 16370, loss = 0.10 (977.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:03:04.052413: step 16380, loss = 0.09 (1040.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:03:06.529193: step 16390, loss = 0.08 (1057.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:03:09.029656: step 16400, loss = 0.08 (962.7 examples/sec; 0.133 sec/batch)
2016-05-26 06:03:11.833004: step 16410, loss = 0.07 (1040.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:03:14.337866: step 16420, loss = 0.06 (1065.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:03:16.880797: step 16430, loss = 0.06 (1005.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:03:19.327818: step 16440, loss = 0.09 (1052.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:03:21.835074: step 16450, loss = 0.05 (1090.0 examples/sec; 0.117 sec/batch)
2016-05-26 06:03:24.328868: step 16460, loss = 0.05 (1016.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:03:26.845403: step 16470, loss = 0.05 (1050.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:03:29.317243: step 16480, loss = 0.05 (1082.1 examples/sec; 0.118 sec/batch)
2016-05-26 06:03:31.791092: step 16490, loss = 0.08 (1048.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:03:34.283467: step 16500, loss = 0.06 (1067.2 examples/sec; 0.120 sec/batch)
2016-05-26 06:03:37.092560: step 16510, loss = 0.12 (970.7 examples/sec; 0.132 sec/batch)
2016-05-26 06:03:39.584303: step 16520, loss = 0.05 (1005.3 examples/sec; 0.127 sec/batch)
2016-05-26 06:03:42.055784: step 16530, loss = 0.03 (1065.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:03:44.524771: step 16540, loss = 0.08 (1083.0 examples/sec; 0.118 sec/batch)
2016-05-26 06:03:47.039877: step 16550, loss = 0.16 (1007.4 examples/sec; 0.127 sec/batch)
2016-05-26 06:03:49.576394: step 16560, loss = 0.04 (1023.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:03:52.058813: step 16570, loss = 0.06 (992.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:03:54.585458: step 16580, loss = 0.04 (1035.1 examples/sec; 0.124 sec/batch)
2016-05-26 06:03:57.064026: step 16590, loss = 0.03 (1031.1 examples/sec; 0.124 sec/batch)
2016-05-26 06:03:59.532006: step 16600, loss = 0.04 (1014.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:04:02.389433: step 16610, loss = 0.08 (1019.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:04:04.889222: step 16620, loss = 0.07 (1056.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:04:07.359404: step 16630, loss = 0.07 (996.1 examples/sec; 0.128 sec/batch)
2016-05-26 06:04:09.854213: step 16640, loss = 0.07 (1085.0 examples/sec; 0.118 sec/batch)
2016-05-26 06:04:12.364294: step 16650, loss = 0.09 (1017.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:04:14.881782: step 16660, loss = 0.05 (1062.6 examples/sec; 0.120 sec/batch)
2016-05-26 06:04:17.318219: step 16670, loss = 0.06 (1035.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:04:19.758955: step 16680, loss = 0.01 (997.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:04:22.191681: step 16690, loss = 0.06 (1019.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:04:24.659262: step 16700, loss = 0.02 (1037.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:04:27.423149: step 16710, loss = 0.04 (1070.0 examples/sec; 0.120 sec/batch)
2016-05-26 06:04:29.927549: step 16720, loss = 0.06 (962.5 examples/sec; 0.133 sec/batch)
2016-05-26 06:04:32.475577: step 16730, loss = 0.12 (1055.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:04:34.976288: step 16740, loss = 0.09 (1084.7 examples/sec; 0.118 sec/batch)
2016-05-26 06:04:37.482858: step 16750, loss = 0.08 (1057.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:04:40.021383: step 16760, loss = 0.03 (1056.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:04:42.505343: step 16770, loss = 0.06 (1091.7 examples/sec; 0.117 sec/batch)
2016-05-26 06:04:44.975481: step 16780, loss = 0.05 (1023.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:04:47.497827: step 16790, loss = 0.05 (1041.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:04:50.005447: step 16800, loss = 0.06 (995.5 examples/sec; 0.129 sec/batch)
2016-05-26 06:04:52.816894: step 16810, loss = 0.12 (947.4 examples/sec; 0.135 sec/batch)
2016-05-26 06:04:55.277797: step 16820, loss = 0.14 (1087.6 examples/sec; 0.118 sec/batch)
2016-05-26 06:04:57.748831: step 16830, loss = 0.04 (1082.8 examples/sec; 0.118 sec/batch)
2016-05-26 06:05:00.265309: step 16840, loss = 0.06 (1004.0 examples/sec; 0.127 sec/batch)
2016-05-26 06:05:02.733533: step 16850, loss = 0.07 (1063.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:05:05.241851: step 16860, loss = 0.08 (1053.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:05:07.755464: step 16870, loss = 0.05 (1020.1 examples/sec; 0.125 sec/batch)
2016-05-26 06:05:10.302463: step 16880, loss = 0.07 (1080.7 examples/sec; 0.118 sec/batch)
2016-05-26 06:05:12.706193: step 16890, loss = 0.11 (1074.7 examples/sec; 0.119 sec/batch)
2016-05-26 06:05:15.208205: step 16900, loss = 0.04 (1046.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:05:17.958125: step 16910, loss = 0.06 (1048.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:05:20.411983: step 16920, loss = 0.06 (1067.3 examples/sec; 0.120 sec/batch)
2016-05-26 06:05:22.934421: step 16930, loss = 0.03 (1020.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:05:25.407197: step 16940, loss = 0.07 (1002.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:05:27.873586: step 16950, loss = 0.04 (1039.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:05:30.382265: step 16960, loss = 0.08 (1024.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:05:32.904975: step 16970, loss = 0.06 (946.2 examples/sec; 0.135 sec/batch)
2016-05-26 06:05:35.390748: step 16980, loss = 0.04 (1079.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:05:37.886772: step 16990, loss = 0.04 (1079.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:05:40.357789: step 17000, loss = 0.04 (1056.4 examples/sec; 0.121 sec/batch)
eval once
2016-05-26 06:06:22.004167: accuracy @ 1 = 0.981, 49109 / 50048 at 0
2016-05-26 06:06:24.525553: step 17010, loss = 0.02 (1068.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:06:27.032880: step 17020, loss = 0.04 (999.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:06:29.500898: step 17030, loss = 0.10 (1078.9 examples/sec; 0.119 sec/batch)
2016-05-26 06:06:31.953998: step 17040, loss = 0.03 (999.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:06:34.385836: step 17050, loss = 0.06 (1089.5 examples/sec; 0.117 sec/batch)
2016-05-26 06:06:36.896480: step 17060, loss = 0.13 (1025.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:06:39.408990: step 17070, loss = 0.03 (1059.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:06:41.935521: step 17080, loss = 0.09 (989.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:06:44.380263: step 17090, loss = 0.09 (1079.7 examples/sec; 0.119 sec/batch)
2016-05-26 06:06:46.861758: step 17100, loss = 0.02 (1044.0 examples/sec; 0.123 sec/batch)
2016-05-26 06:06:49.652947: step 17110, loss = 0.04 (1057.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:06:52.123955: step 17120, loss = 0.02 (1026.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:06:54.590825: step 17130, loss = 0.07 (1046.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:06:57.108841: step 17140, loss = 0.13 (988.0 examples/sec; 0.130 sec/batch)
2016-05-26 06:06:59.601801: step 17150, loss = 0.15 (1044.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:07:02.076425: step 17160, loss = 0.03 (986.1 examples/sec; 0.130 sec/batch)
2016-05-26 06:07:04.549878: step 17170, loss = 0.05 (990.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:07:07.024686: step 17180, loss = 0.03 (1036.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:07:09.555357: step 17190, loss = 0.04 (996.1 examples/sec; 0.128 sec/batch)
2016-05-26 06:07:12.039879: step 17200, loss = 0.05 (1044.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:07:14.840781: step 17210, loss = 0.08 (1034.4 examples/sec; 0.124 sec/batch)
2016-05-26 06:07:17.320104: step 17220, loss = 0.07 (1050.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:07:19.772340: step 17230, loss = 0.08 (1095.9 examples/sec; 0.117 sec/batch)
2016-05-26 06:07:22.286245: step 17240, loss = 0.13 (1080.1 examples/sec; 0.119 sec/batch)
2016-05-26 06:07:24.776374: step 17250, loss = 0.06 (1062.3 examples/sec; 0.120 sec/batch)
2016-05-26 06:07:27.320484: step 17260, loss = 0.07 (976.4 examples/sec; 0.131 sec/batch)
2016-05-26 06:07:29.873511: step 17270, loss = 0.03 (978.3 examples/sec; 0.131 sec/batch)
2016-05-26 06:07:32.366495: step 17280, loss = 0.12 (956.8 examples/sec; 0.134 sec/batch)
2016-05-26 06:07:34.835659: step 17290, loss = 0.03 (1049.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:07:37.301127: step 17300, loss = 0.06 (1015.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:07:40.122892: step 17310, loss = 0.05 (1024.2 examples/sec; 0.125 sec/batch)
2016-05-26 06:07:42.604047: step 17320, loss = 0.06 (1053.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:07:45.058166: step 17330, loss = 0.05 (1075.9 examples/sec; 0.119 sec/batch)
2016-05-26 06:07:47.550850: step 17340, loss = 0.05 (992.9 examples/sec; 0.129 sec/batch)
2016-05-26 06:07:50.065702: step 17350, loss = 0.07 (1069.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:07:52.593246: step 17360, loss = 0.04 (1023.3 examples/sec; 0.125 sec/batch)
2016-05-26 06:07:55.096663: step 17370, loss = 0.05 (1023.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:07:57.596948: step 17380, loss = 0.06 (1026.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:08:00.092648: step 17390, loss = 0.04 (1050.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:08:02.550187: step 17400, loss = 0.02 (1038.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:08:05.352173: step 17410, loss = 0.05 (984.8 examples/sec; 0.130 sec/batch)
2016-05-26 06:08:07.850686: step 17420, loss = 0.11 (1028.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:08:10.293085: step 17430, loss = 0.08 (1065.2 examples/sec; 0.120 sec/batch)
2016-05-26 06:08:12.755877: step 17440, loss = 0.06 (1081.1 examples/sec; 0.118 sec/batch)
2016-05-26 06:08:15.224890: step 17450, loss = 0.02 (1024.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:08:17.718988: step 17460, loss = 0.11 (958.5 examples/sec; 0.134 sec/batch)
2016-05-26 06:08:20.240217: step 17470, loss = 0.09 (1106.8 examples/sec; 0.116 sec/batch)
2016-05-26 06:08:22.695272: step 17480, loss = 0.05 (993.2 examples/sec; 0.129 sec/batch)
2016-05-26 06:08:25.191646: step 17490, loss = 0.08 (1054.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:08:27.666234: step 17500, loss = 0.05 (1058.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:08:30.432081: step 17510, loss = 0.05 (1074.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:08:32.878113: step 17520, loss = 0.10 (1102.9 examples/sec; 0.116 sec/batch)
2016-05-26 06:08:35.324438: step 17530, loss = 0.03 (1043.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:08:37.837208: step 17540, loss = 0.04 (1051.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:08:40.358372: step 17550, loss = 0.01 (951.2 examples/sec; 0.135 sec/batch)
2016-05-26 06:08:42.834235: step 17560, loss = 0.03 (1042.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:08:45.323125: step 17570, loss = 0.02 (999.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:08:47.831727: step 17580, loss = 0.08 (980.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:08:50.286805: step 17590, loss = 0.06 (1008.0 examples/sec; 0.127 sec/batch)
2016-05-26 06:08:52.779281: step 17600, loss = 0.05 (1063.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:08:55.582500: step 17610, loss = 0.03 (989.7 examples/sec; 0.129 sec/batch)
2016-05-26 06:08:58.061731: step 17620, loss = 0.07 (1032.5 examples/sec; 0.124 sec/batch)
2016-05-26 06:09:00.591520: step 17630, loss = 0.05 (979.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:09:03.091285: step 17640, loss = 0.08 (1061.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:09:05.573327: step 17650, loss = 0.07 (1014.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:09:08.059838: step 17660, loss = 0.05 (999.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:09:10.513547: step 17670, loss = 0.04 (1061.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:09:12.999352: step 17680, loss = 0.07 (1121.7 examples/sec; 0.114 sec/batch)
2016-05-26 06:09:15.472238: step 17690, loss = 0.08 (971.4 examples/sec; 0.132 sec/batch)
2016-05-26 06:09:17.940252: step 17700, loss = 0.04 (1066.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:09:20.740992: step 17710, loss = 0.17 (1019.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:09:23.237769: step 17720, loss = 0.06 (1046.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:09:25.755600: step 17730, loss = 0.03 (1009.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:09:28.277662: step 17740, loss = 0.09 (1034.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:09:30.758458: step 17750, loss = 0.10 (1071.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:09:33.268240: step 17760, loss = 0.03 (982.9 examples/sec; 0.130 sec/batch)
2016-05-26 06:09:35.732511: step 17770, loss = 0.02 (960.6 examples/sec; 0.133 sec/batch)
2016-05-26 06:09:38.205266: step 17780, loss = 0.03 (1056.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:09:40.721817: step 17790, loss = 0.03 (1051.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:09:43.221740: step 17800, loss = 0.05 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:09:46.081816: step 17810, loss = 0.04 (1005.0 examples/sec; 0.127 sec/batch)
2016-05-26 06:09:48.541121: step 17820, loss = 0.03 (1020.3 examples/sec; 0.125 sec/batch)
2016-05-26 06:09:50.984608: step 17830, loss = 0.06 (1070.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:09:53.476642: step 17840, loss = 0.04 (1098.6 examples/sec; 0.117 sec/batch)
2016-05-26 06:09:55.921704: step 17850, loss = 0.08 (1075.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:09:58.360123: step 17860, loss = 0.04 (996.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:10:00.847128: step 17870, loss = 0.05 (995.7 examples/sec; 0.129 sec/batch)
2016-05-26 06:10:03.354444: step 17880, loss = 0.04 (1042.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:10:05.869198: step 17890, loss = 0.05 (1006.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:10:08.378812: step 17900, loss = 0.06 (988.3 examples/sec; 0.130 sec/batch)
2016-05-26 06:10:11.190509: step 17910, loss = 0.06 (998.2 examples/sec; 0.128 sec/batch)
2016-05-26 06:10:13.673793: step 17920, loss = 0.03 (1085.9 examples/sec; 0.118 sec/batch)
2016-05-26 06:10:16.147172: step 17930, loss = 0.02 (1008.7 examples/sec; 0.127 sec/batch)
2016-05-26 06:10:18.589208: step 17940, loss = 0.11 (1059.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:10:21.068371: step 17950, loss = 0.03 (1053.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:10:23.600288: step 17960, loss = 0.02 (960.2 examples/sec; 0.133 sec/batch)
2016-05-26 06:10:26.070418: step 17970, loss = 0.06 (1072.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:10:28.572219: step 17980, loss = 0.08 (1008.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:10:31.073396: step 17990, loss = 0.04 (1001.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:10:33.623149: step 18000, loss = 0.03 (1046.5 examples/sec; 0.122 sec/batch)
eval once
2016-05-26 06:11:15.099777: accuracy @ 1 = 0.983, 49183 / 50048 at 0
2016-05-26 06:11:17.627524: step 18010, loss = 0.06 (1034.0 examples/sec; 0.124 sec/batch)
2016-05-26 06:11:20.139410: step 18020, loss = 0.02 (1013.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:11:22.596681: step 18030, loss = 0.02 (972.2 examples/sec; 0.132 sec/batch)
2016-05-26 06:11:25.044684: step 18040, loss = 0.02 (1009.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:11:27.515612: step 18050, loss = 0.09 (978.2 examples/sec; 0.131 sec/batch)
2016-05-26 06:11:29.953136: step 18060, loss = 0.06 (1071.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:11:32.490065: step 18070, loss = 0.02 (980.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:11:34.975008: step 18080, loss = 0.02 (1035.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:11:37.500401: step 18090, loss = 0.08 (1040.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:11:40.054699: step 18100, loss = 0.07 (969.5 examples/sec; 0.132 sec/batch)
2016-05-26 06:11:42.851247: step 18110, loss = 0.03 (1078.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:11:45.311371: step 18120, loss = 0.06 (1004.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:11:47.761717: step 18130, loss = 0.04 (1016.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:11:50.217966: step 18140, loss = 0.01 (1051.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:11:52.732778: step 18150, loss = 0.04 (1000.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:11:55.257045: step 18160, loss = 0.02 (1096.9 examples/sec; 0.117 sec/batch)
2016-05-26 06:11:57.739074: step 18170, loss = 0.07 (1023.1 examples/sec; 0.125 sec/batch)
2016-05-26 06:12:00.261591: step 18180, loss = 0.09 (1025.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:12:02.751220: step 18190, loss = 0.03 (1059.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:12:05.273494: step 18200, loss = 0.04 (1014.7 examples/sec; 0.126 sec/batch)
2016-05-26 06:12:08.078419: step 18210, loss = 0.09 (984.7 examples/sec; 0.130 sec/batch)
2016-05-26 06:12:10.574705: step 18220, loss = 0.04 (1019.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:12:13.106288: step 18230, loss = 0.03 (1006.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:12:15.639132: step 18240, loss = 0.01 (988.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:12:18.178585: step 18250, loss = 0.06 (1010.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:12:20.648512: step 18260, loss = 0.03 (1052.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:12:23.175490: step 18270, loss = 0.08 (1098.8 examples/sec; 0.116 sec/batch)
2016-05-26 06:12:25.670468: step 18280, loss = 0.01 (1027.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:12:28.157289: step 18290, loss = 0.03 (1010.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:12:30.737550: step 18300, loss = 0.04 (965.3 examples/sec; 0.133 sec/batch)
2016-05-26 06:12:33.489498: step 18310, loss = 0.03 (1045.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:12:35.974007: step 18320, loss = 0.04 (1070.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:12:38.484372: step 18330, loss = 0.03 (985.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:12:40.964692: step 18340, loss = 0.10 (976.2 examples/sec; 0.131 sec/batch)
2016-05-26 06:12:43.493857: step 18350, loss = 0.11 (1037.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:12:45.988538: step 18360, loss = 0.02 (1055.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:12:48.506401: step 18370, loss = 0.06 (1001.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:12:51.015830: step 18380, loss = 0.04 (968.5 examples/sec; 0.132 sec/batch)
2016-05-26 06:12:53.495641: step 18390, loss = 0.01 (1063.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:12:55.967227: step 18400, loss = 0.03 (1065.9 examples/sec; 0.120 sec/batch)
2016-05-26 06:12:58.786825: step 18410, loss = 0.05 (1051.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:13:01.256477: step 18420, loss = 0.04 (1059.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:13:03.749041: step 18430, loss = 0.06 (1026.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:13:06.228565: step 18440, loss = 0.04 (1074.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:13:08.712257: step 18450, loss = 0.04 (1057.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:13:11.164214: step 18460, loss = 0.07 (1067.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:13:13.626355: step 18470, loss = 0.03 (1005.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:13:16.165634: step 18480, loss = 0.06 (1022.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:13:18.680215: step 18490, loss = 0.06 (969.3 examples/sec; 0.132 sec/batch)
2016-05-26 06:13:21.132335: step 18500, loss = 0.07 (1013.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:13:23.981085: step 18510, loss = 0.05 (1053.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:13:26.505737: step 18520, loss = 0.07 (1031.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:13:29.001775: step 18530, loss = 0.02 (1000.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:13:31.579591: step 18540, loss = 0.02 (999.2 examples/sec; 0.128 sec/batch)
2016-05-26 06:13:34.027383: step 18550, loss = 0.08 (1073.1 examples/sec; 0.119 sec/batch)
2016-05-26 06:13:36.501285: step 18560, loss = 0.07 (1074.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:13:39.044422: step 18570, loss = 0.02 (1021.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:13:41.464577: step 18580, loss = 0.04 (1077.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:13:43.945614: step 18590, loss = 0.06 (1029.5 examples/sec; 0.124 sec/batch)
2016-05-26 06:13:46.464411: step 18600, loss = 0.04 (1043.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:13:49.316306: step 18610, loss = 0.01 (1078.4 examples/sec; 0.119 sec/batch)
2016-05-26 06:13:51.853848: step 18620, loss = 0.09 (1058.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:13:54.246183: step 18630, loss = 0.11 (1056.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:13:56.713799: step 18640, loss = 0.11 (1019.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:13:59.186916: step 18650, loss = 0.02 (1048.3 examples/sec; 0.122 sec/batch)
2016-05-26 06:14:01.665138: step 18660, loss = 0.07 (1009.6 examples/sec; 0.127 sec/batch)
2016-05-26 06:14:04.140599: step 18670, loss = 0.04 (1044.0 examples/sec; 0.123 sec/batch)
2016-05-26 06:14:06.620110: step 18680, loss = 0.08 (1008.6 examples/sec; 0.127 sec/batch)
2016-05-26 06:14:09.018879: step 18690, loss = 0.02 (1079.7 examples/sec; 0.119 sec/batch)
2016-05-26 06:14:11.444872: step 18700, loss = 0.05 (1066.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:14:14.286237: step 18710, loss = 0.01 (1034.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:14:16.739812: step 18720, loss = 0.02 (1032.1 examples/sec; 0.124 sec/batch)
2016-05-26 06:14:19.244180: step 18730, loss = 0.05 (1028.4 examples/sec; 0.124 sec/batch)
2016-05-26 06:14:21.776004: step 18740, loss = 0.05 (1011.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:14:24.303350: step 18750, loss = 0.02 (1003.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:14:26.733751: step 18760, loss = 0.08 (1041.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:14:29.227160: step 18770, loss = 0.01 (986.7 examples/sec; 0.130 sec/batch)
2016-05-26 06:14:31.750729: step 18780, loss = 0.02 (1032.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:14:34.268681: step 18790, loss = 0.03 (954.3 examples/sec; 0.134 sec/batch)
2016-05-26 06:14:36.781588: step 18800, loss = 0.06 (1039.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:14:39.604747: step 18810, loss = 0.02 (1021.2 examples/sec; 0.125 sec/batch)
2016-05-26 06:14:42.096467: step 18820, loss = 0.07 (977.6 examples/sec; 0.131 sec/batch)
2016-05-26 06:14:44.593517: step 18830, loss = 0.08 (1060.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:14:47.106307: step 18840, loss = 0.03 (1017.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:14:49.611223: step 18850, loss = 0.06 (1029.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:14:52.077861: step 18860, loss = 0.03 (1116.4 examples/sec; 0.115 sec/batch)
2016-05-26 06:14:54.566661: step 18870, loss = 0.09 (1070.2 examples/sec; 0.120 sec/batch)
2016-05-26 06:14:57.103049: step 18880, loss = 0.04 (984.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:14:59.623121: step 18890, loss = 0.05 (1003.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:15:02.163368: step 18900, loss = 0.04 (1022.6 examples/sec; 0.125 sec/batch)
2016-05-26 06:15:04.913686: step 18910, loss = 0.05 (1067.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:15:07.459606: step 18920, loss = 0.13 (1009.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:15:09.987063: step 18930, loss = 0.04 (1008.6 examples/sec; 0.127 sec/batch)
2016-05-26 06:15:12.475543: step 18940, loss = 0.03 (1081.7 examples/sec; 0.118 sec/batch)
2016-05-26 06:15:14.970576: step 18950, loss = 0.05 (996.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:15:17.475699: step 18960, loss = 0.02 (1001.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:15:19.966092: step 18970, loss = 0.09 (975.5 examples/sec; 0.131 sec/batch)
2016-05-26 06:15:22.523244: step 18980, loss = 0.05 (956.1 examples/sec; 0.134 sec/batch)
2016-05-26 06:15:25.005363: step 18990, loss = 0.05 (1008.5 examples/sec; 0.127 sec/batch)
2016-05-26 06:15:27.545902: step 19000, loss = 0.05 (995.8 examples/sec; 0.129 sec/batch)
eval once
2016-05-26 06:16:09.619364: accuracy @ 1 = 0.982, 49146 / 50048 at 0
2016-05-26 06:16:12.160202: step 19010, loss = 0.02 (1005.7 examples/sec; 0.127 sec/batch)
2016-05-26 06:16:14.657373: step 19020, loss = 0.07 (1001.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:16:17.068007: step 19030, loss = 0.03 (1025.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:16:19.476610: step 19040, loss = 0.03 (1103.8 examples/sec; 0.116 sec/batch)
2016-05-26 06:16:21.919452: step 19050, loss = 0.03 (1014.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:16:24.384938: step 19060, loss = 0.08 (1022.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:16:26.830810: step 19070, loss = 0.02 (1043.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:16:29.328499: step 19080, loss = 0.07 (1046.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:16:31.817942: step 19090, loss = 0.03 (1016.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:16:34.284299: step 19100, loss = 0.03 (1000.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:16:37.110442: step 19110, loss = 0.05 (1011.5 examples/sec; 0.127 sec/batch)
2016-05-26 06:16:39.542382: step 19120, loss = 0.08 (1078.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:16:42.027282: step 19130, loss = 0.05 (1065.6 examples/sec; 0.120 sec/batch)
2016-05-26 06:16:44.533692: step 19140, loss = 0.02 (990.4 examples/sec; 0.129 sec/batch)
2016-05-26 06:16:47.017472: step 19150, loss = 0.03 (1012.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:16:49.488442: step 19160, loss = 0.01 (988.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:16:51.943307: step 19170, loss = 0.03 (1078.4 examples/sec; 0.119 sec/batch)
2016-05-26 06:16:54.378435: step 19180, loss = 0.06 (1071.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:16:56.853045: step 19190, loss = 0.03 (982.8 examples/sec; 0.130 sec/batch)
2016-05-26 06:16:59.348369: step 19200, loss = 0.03 (987.8 examples/sec; 0.130 sec/batch)
2016-05-26 06:17:02.166933: step 19210, loss = 0.05 (1052.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:17:04.691258: step 19220, loss = 0.08 (1000.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:17:07.173015: step 19230, loss = 0.06 (1094.8 examples/sec; 0.117 sec/batch)
2016-05-26 06:17:09.624839: step 19240, loss = 0.01 (1056.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:17:12.120928: step 19250, loss = 0.12 (1085.0 examples/sec; 0.118 sec/batch)
2016-05-26 06:17:14.657990: step 19260, loss = 0.10 (1012.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:17:17.177017: step 19270, loss = 0.05 (996.4 examples/sec; 0.128 sec/batch)
2016-05-26 06:17:19.672967: step 19280, loss = 0.04 (1049.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:17:22.198587: step 19290, loss = 0.01 (1048.0 examples/sec; 0.122 sec/batch)
2016-05-26 06:17:24.666319: step 19300, loss = 0.12 (1044.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:17:27.470664: step 19310, loss = 0.02 (1035.0 examples/sec; 0.124 sec/batch)
2016-05-26 06:17:30.042168: step 19320, loss = 0.01 (1009.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:17:32.557096: step 19330, loss = 0.05 (968.8 examples/sec; 0.132 sec/batch)
2016-05-26 06:17:35.068048: step 19340, loss = 0.05 (990.2 examples/sec; 0.129 sec/batch)
2016-05-26 06:17:37.547428: step 19350, loss = 0.03 (1078.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:17:40.045752: step 19360, loss = 0.01 (1023.6 examples/sec; 0.125 sec/batch)
2016-05-26 06:17:42.509665: step 19370, loss = 0.03 (1028.2 examples/sec; 0.124 sec/batch)
2016-05-26 06:17:44.968602: step 19380, loss = 0.07 (1106.2 examples/sec; 0.116 sec/batch)
2016-05-26 06:17:47.427027: step 19390, loss = 0.02 (1099.4 examples/sec; 0.116 sec/batch)
2016-05-26 06:17:49.924292: step 19400, loss = 0.08 (1021.6 examples/sec; 0.125 sec/batch)
2016-05-26 06:17:52.717425: step 19410, loss = 0.09 (1023.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:17:55.180860: step 19420, loss = 0.05 (1074.1 examples/sec; 0.119 sec/batch)
2016-05-26 06:17:57.653257: step 19430, loss = 0.05 (1024.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:18:00.136962: step 19440, loss = 0.02 (1040.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:18:02.620595: step 19450, loss = 0.01 (1037.0 examples/sec; 0.123 sec/batch)
2016-05-26 06:18:05.145918: step 19460, loss = 0.02 (988.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:18:07.690599: step 19470, loss = 0.04 (1029.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:18:10.185580: step 19480, loss = 0.03 (1054.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:18:12.715588: step 19490, loss = 0.07 (982.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:18:15.178663: step 19500, loss = 0.02 (1016.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:18:17.959685: step 19510, loss = 0.02 (1085.4 examples/sec; 0.118 sec/batch)
2016-05-26 06:18:20.459039: step 19520, loss = 0.07 (1051.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:18:22.932515: step 19530, loss = 0.09 (1081.6 examples/sec; 0.118 sec/batch)
2016-05-26 06:18:25.436655: step 19540, loss = 0.07 (967.7 examples/sec; 0.132 sec/batch)
2016-05-26 06:18:27.961970: step 19550, loss = 0.05 (1062.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:18:30.477975: step 19560, loss = 0.10 (971.6 examples/sec; 0.132 sec/batch)
2016-05-26 06:18:32.938588: step 19570, loss = 0.10 (998.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:18:35.398580: step 19580, loss = 0.08 (1076.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:18:37.820666: step 19590, loss = 0.05 (1000.1 examples/sec; 0.128 sec/batch)
2016-05-26 06:18:40.366652: step 19600, loss = 0.13 (954.2 examples/sec; 0.134 sec/batch)
2016-05-26 06:18:43.222909: step 19610, loss = 0.07 (1041.0 examples/sec; 0.123 sec/batch)
2016-05-26 06:18:45.697738: step 19620, loss = 0.04 (1033.0 examples/sec; 0.124 sec/batch)
2016-05-26 06:18:48.151422: step 19630, loss = 0.03 (1050.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:18:50.595201: step 19640, loss = 0.03 (1077.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:18:53.088787: step 19650, loss = 0.02 (1074.4 examples/sec; 0.119 sec/batch)
2016-05-26 06:18:55.591948: step 19660, loss = 0.11 (1018.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:18:58.120054: step 19670, loss = 0.08 (1016.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:19:00.609940: step 19680, loss = 0.03 (1018.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:19:03.107063: step 19690, loss = 0.02 (1032.4 examples/sec; 0.124 sec/batch)
2016-05-26 06:19:05.578546: step 19700, loss = 0.07 (1018.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:19:08.360366: step 19710, loss = 0.04 (950.5 examples/sec; 0.135 sec/batch)
2016-05-26 06:19:10.887908: step 19720, loss = 0.06 (1044.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:19:13.389019: step 19730, loss = 0.03 (982.6 examples/sec; 0.130 sec/batch)
2016-05-26 06:19:15.881817: step 19740, loss = 0.08 (986.7 examples/sec; 0.130 sec/batch)
2016-05-26 06:19:18.375778: step 19750, loss = 0.04 (983.1 examples/sec; 0.130 sec/batch)
2016-05-26 06:19:20.878499: step 19760, loss = 0.03 (1072.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:19:23.361389: step 19770, loss = 0.02 (963.9 examples/sec; 0.133 sec/batch)
2016-05-26 06:19:25.852652: step 19780, loss = 0.04 (1042.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:19:28.345922: step 19790, loss = 0.06 (1061.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:19:30.818987: step 19800, loss = 0.05 (985.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:19:33.563861: step 19810, loss = 0.02 (1016.2 examples/sec; 0.126 sec/batch)
2016-05-26 06:19:36.054966: step 19820, loss = 0.04 (992.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:19:38.561789: step 19830, loss = 0.02 (1012.7 examples/sec; 0.126 sec/batch)
2016-05-26 06:19:40.965114: step 19840, loss = 0.02 (1052.0 examples/sec; 0.122 sec/batch)
2016-05-26 06:19:43.417533: step 19850, loss = 0.06 (1077.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:19:45.880033: step 19860, loss = 0.02 (1018.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:19:48.358068: step 19870, loss = 0.08 (1029.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:19:50.824808: step 19880, loss = 0.03 (1013.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:19:53.327985: step 19890, loss = 0.01 (1053.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:19:55.844125: step 19900, loss = 0.03 (1032.5 examples/sec; 0.124 sec/batch)
2016-05-26 06:19:58.663310: step 19910, loss = 0.03 (1016.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:20:01.232636: step 19920, loss = 0.02 (1053.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:20:03.793364: step 19930, loss = 0.08 (974.9 examples/sec; 0.131 sec/batch)
2016-05-26 06:20:06.300440: step 19940, loss = 0.05 (993.2 examples/sec; 0.129 sec/batch)
2016-05-26 06:20:08.813284: step 19950, loss = 0.03 (1037.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:20:11.269550: step 19960, loss = 0.04 (1056.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:20:13.766863: step 19970, loss = 0.03 (1069.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:20:16.301142: step 19980, loss = 0.05 (966.4 examples/sec; 0.132 sec/batch)
2016-05-26 06:20:18.829229: step 19990, loss = 0.04 (1010.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:20:21.328867: step 20000, loss = 0.03 (986.8 examples/sec; 0.130 sec/batch)
eval once
2016-05-26 06:21:03.326714: accuracy @ 1 = 0.984, 49255 / 50048 at 0
2016-05-26 06:21:05.822143: step 20010, loss = 0.05 (1075.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:21:08.283080: step 20020, loss = 0.06 (1071.7 examples/sec; 0.119 sec/batch)
2016-05-26 06:21:10.762355: step 20030, loss = 0.06 (994.9 examples/sec; 0.129 sec/batch)
2016-05-26 06:21:13.209713: step 20040, loss = 0.04 (1049.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:21:15.672121: step 20050, loss = 0.14 (1086.9 examples/sec; 0.118 sec/batch)
2016-05-26 06:21:18.178758: step 20060, loss = 0.05 (1019.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:21:20.679180: step 20070, loss = 0.02 (1093.0 examples/sec; 0.117 sec/batch)
2016-05-26 06:21:23.212654: step 20080, loss = 0.01 (956.2 examples/sec; 0.134 sec/batch)
2016-05-26 06:21:25.748655: step 20090, loss = 0.03 (969.0 examples/sec; 0.132 sec/batch)
2016-05-26 06:21:28.249118: step 20100, loss = 0.02 (1081.9 examples/sec; 0.118 sec/batch)
2016-05-26 06:21:31.068285: step 20110, loss = 0.01 (1027.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:21:33.602510: step 20120, loss = 0.04 (1052.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:21:36.121973: step 20130, loss = 0.07 (992.9 examples/sec; 0.129 sec/batch)
2016-05-26 06:21:38.660573: step 20140, loss = 0.02 (987.6 examples/sec; 0.130 sec/batch)
2016-05-26 06:21:41.110694: step 20150, loss = 0.02 (1066.0 examples/sec; 0.120 sec/batch)
2016-05-26 06:21:43.583437: step 20160, loss = 0.06 (999.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:21:46.092862: step 20170, loss = 0.06 (1058.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:21:48.551153: step 20180, loss = 0.04 (1016.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:21:51.042634: step 20190, loss = 0.02 (1054.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:21:53.526038: step 20200, loss = 0.03 (1040.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:21:56.323793: step 20210, loss = 0.03 (1009.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:21:58.771445: step 20220, loss = 0.03 (1017.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:22:01.270508: step 20230, loss = 0.04 (1104.5 examples/sec; 0.116 sec/batch)
2016-05-26 06:22:03.827656: step 20240, loss = 0.04 (987.6 examples/sec; 0.130 sec/batch)
2016-05-26 06:22:06.362916: step 20250, loss = 0.06 (1039.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:22:08.852671: step 20260, loss = 0.05 (1050.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:22:11.370902: step 20270, loss = 0.09 (1005.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:22:13.944019: step 20280, loss = 0.04 (997.4 examples/sec; 0.128 sec/batch)
2016-05-26 06:22:16.408494: step 20290, loss = 0.02 (1072.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:22:18.871951: step 20300, loss = 0.07 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:22:21.678669: step 20310, loss = 0.06 (1070.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:22:24.150429: step 20320, loss = 0.02 (980.1 examples/sec; 0.131 sec/batch)
2016-05-26 06:22:26.687040: step 20330, loss = 0.02 (1077.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:22:29.195040: step 20340, loss = 0.05 (1030.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:22:31.650762: step 20350, loss = 0.05 (1055.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:22:34.186871: step 20360, loss = 0.02 (1069.6 examples/sec; 0.120 sec/batch)
2016-05-26 06:22:36.708962: step 20370, loss = 0.07 (972.7 examples/sec; 0.132 sec/batch)
2016-05-26 06:22:39.199907: step 20380, loss = 0.02 (1008.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:22:41.679411: step 20390, loss = 0.04 (1062.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:22:44.201677: step 20400, loss = 0.02 (1022.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:22:47.009361: step 20410, loss = 0.04 (992.8 examples/sec; 0.129 sec/batch)
2016-05-26 06:22:49.506892: step 20420, loss = 0.05 (1076.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:22:52.014525: step 20430, loss = 0.05 (981.1 examples/sec; 0.130 sec/batch)
2016-05-26 06:22:54.500191: step 20440, loss = 0.01 (972.2 examples/sec; 0.132 sec/batch)
2016-05-26 06:22:56.958632: step 20450, loss = 0.07 (1011.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:22:59.478926: step 20460, loss = 0.04 (1069.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:23:01.996935: step 20470, loss = 0.02 (1027.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:23:04.448847: step 20480, loss = 0.02 (1002.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:23:06.939248: step 20490, loss = 0.02 (1007.0 examples/sec; 0.127 sec/batch)
2016-05-26 06:23:09.402023: step 20500, loss = 0.02 (1014.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:23:12.101656: step 20510, loss = 0.02 (1040.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:23:14.590343: step 20520, loss = 0.03 (1042.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:23:17.055656: step 20530, loss = 0.01 (998.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:23:19.535371: step 20540, loss = 0.06 (1072.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:23:22.049271: step 20550, loss = 0.01 (1066.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:23:24.581120: step 20560, loss = 0.02 (989.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:23:27.100016: step 20570, loss = 0.03 (1044.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:23:29.608131: step 20580, loss = 0.03 (1044.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:23:32.087141: step 20590, loss = 0.01 (1050.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:23:34.576520: step 20600, loss = 0.04 (997.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:23:37.404828: step 20610, loss = 0.04 (1042.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:23:39.899807: step 20620, loss = 0.04 (1001.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:23:42.368398: step 20630, loss = 0.02 (1022.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:23:44.862415: step 20640, loss = 0.04 (1016.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:23:47.327150: step 20650, loss = 0.06 (1058.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:23:49.829740: step 20660, loss = 0.04 (1098.2 examples/sec; 0.117 sec/batch)
2016-05-26 06:23:52.322030: step 20670, loss = 0.03 (1045.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:23:54.809358: step 20680, loss = 0.04 (979.9 examples/sec; 0.131 sec/batch)
2016-05-26 06:23:57.220755: step 20690, loss = 0.03 (1045.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:23:59.716751: step 20700, loss = 0.04 (987.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:24:02.563292: step 20710, loss = 0.02 (1015.7 examples/sec; 0.126 sec/batch)
2016-05-26 06:24:05.113414: step 20720, loss = 0.08 (961.7 examples/sec; 0.133 sec/batch)
2016-05-26 06:24:07.633218: step 20730, loss = 0.08 (998.2 examples/sec; 0.128 sec/batch)
2016-05-26 06:24:10.151997: step 20740, loss = 0.03 (978.6 examples/sec; 0.131 sec/batch)
2016-05-26 06:24:12.690140: step 20750, loss = 0.07 (980.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:24:15.150704: step 20760, loss = 0.04 (1044.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:24:17.643743: step 20770, loss = 0.02 (1046.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:24:20.114614: step 20780, loss = 0.08 (1079.7 examples/sec; 0.119 sec/batch)
2016-05-26 06:24:22.566805: step 20790, loss = 0.03 (1064.9 examples/sec; 0.120 sec/batch)
2016-05-26 06:24:25.111673: step 20800, loss = 0.03 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:24:27.908301: step 20810, loss = 0.02 (1003.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:24:30.410672: step 20820, loss = 0.04 (1073.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:24:32.910489: step 20830, loss = 0.02 (1026.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:24:35.364172: step 20840, loss = 0.01 (1013.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:24:37.878378: step 20850, loss = 0.02 (983.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:24:40.394496: step 20860, loss = 0.06 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:24:42.876316: step 20870, loss = 0.01 (1043.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:24:45.386710: step 20880, loss = 0.02 (989.2 examples/sec; 0.129 sec/batch)
2016-05-26 06:24:47.863091: step 20890, loss = 0.02 (1016.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:24:50.377744: step 20900, loss = 0.01 (1002.1 examples/sec; 0.128 sec/batch)
2016-05-26 06:24:53.141863: step 20910, loss = 0.03 (1025.5 examples/sec; 0.125 sec/batch)
2016-05-26 06:24:55.610205: step 20920, loss = 0.03 (1033.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:24:58.125747: step 20930, loss = 0.03 (991.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:25:00.687100: step 20940, loss = 0.01 (912.4 examples/sec; 0.140 sec/batch)
2016-05-26 06:25:03.197559: step 20950, loss = 0.02 (993.1 examples/sec; 0.129 sec/batch)
2016-05-26 06:25:05.697264: step 20960, loss = 0.04 (1032.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:25:08.232915: step 20970, loss = 0.01 (1008.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:25:10.715134: step 20980, loss = 0.03 (1075.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:25:13.196495: step 20990, loss = 0.01 (1079.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:25:15.681702: step 21000, loss = 0.04 (1008.5 examples/sec; 0.127 sec/batch)
eval once
2016-05-26 06:25:57.499756: accuracy @ 1 = 0.991, 49584 / 50048 at 0
2016-05-26 06:26:00.038619: step 21010, loss = 0.07 (1014.2 examples/sec; 0.126 sec/batch)
2016-05-26 06:26:02.479532: step 21020, loss = 0.03 (1073.1 examples/sec; 0.119 sec/batch)
2016-05-26 06:26:04.973763: step 21030, loss = 0.01 (1031.2 examples/sec; 0.124 sec/batch)
2016-05-26 06:26:07.443434: step 21040, loss = 0.06 (992.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:26:09.866532: step 21050, loss = 0.06 (1081.2 examples/sec; 0.118 sec/batch)
2016-05-26 06:26:12.305429: step 21060, loss = 0.06 (1051.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:26:14.779093: step 21070, loss = 0.03 (997.4 examples/sec; 0.128 sec/batch)
2016-05-26 06:26:17.350855: step 21080, loss = 0.06 (1005.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:26:19.738204: step 21090, loss = 0.04 (1071.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:26:22.197660: step 21100, loss = 0.02 (987.3 examples/sec; 0.130 sec/batch)
2016-05-26 06:26:25.015444: step 21110, loss = 0.04 (1008.7 examples/sec; 0.127 sec/batch)
2016-05-26 06:26:27.456323: step 21120, loss = 0.02 (1049.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:26:29.915480: step 21130, loss = 0.01 (1000.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:26:32.346822: step 21140, loss = 0.04 (1095.4 examples/sec; 0.117 sec/batch)
2016-05-26 06:26:34.765536: step 21150, loss = 0.03 (1073.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:26:37.248683: step 21160, loss = 0.02 (1027.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:26:39.763291: step 21170, loss = 0.04 (1047.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:26:42.295101: step 21180, loss = 0.04 (1003.4 examples/sec; 0.128 sec/batch)
2016-05-26 06:26:44.801914: step 21190, loss = 0.01 (1064.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:26:47.281977: step 21200, loss = 0.05 (1053.3 examples/sec; 0.122 sec/batch)
2016-05-26 06:26:50.155781: step 21210, loss = 0.04 (984.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:26:52.656903: step 21220, loss = 0.02 (1015.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:26:55.164911: step 21230, loss = 0.02 (978.3 examples/sec; 0.131 sec/batch)
2016-05-26 06:26:57.671791: step 21240, loss = 0.01 (1047.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:27:00.127906: step 21250, loss = 0.02 (1019.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:27:02.611788: step 21260, loss = 0.05 (1094.6 examples/sec; 0.117 sec/batch)
2016-05-26 06:27:05.146400: step 21270, loss = 0.03 (1040.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:27:07.659313: step 21280, loss = 0.03 (1064.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:27:10.195817: step 21290, loss = 0.02 (1031.3 examples/sec; 0.124 sec/batch)
2016-05-26 06:27:12.689212: step 21300, loss = 0.02 (1090.4 examples/sec; 0.117 sec/batch)
2016-05-26 06:27:15.466989: step 21310, loss = 0.03 (1101.3 examples/sec; 0.116 sec/batch)
2016-05-26 06:27:17.947238: step 21320, loss = 0.05 (1033.3 examples/sec; 0.124 sec/batch)
2016-05-26 06:27:20.430839: step 21330, loss = 0.01 (1050.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:27:22.907506: step 21340, loss = 0.01 (1061.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:27:25.392402: step 21350, loss = 0.01 (1024.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:27:27.913188: step 21360, loss = 0.02 (1003.2 examples/sec; 0.128 sec/batch)
2016-05-26 06:27:30.426963: step 21370, loss = 0.01 (997.1 examples/sec; 0.128 sec/batch)
2016-05-26 06:27:32.915974: step 21380, loss = 0.03 (1022.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:27:35.446763: step 21390, loss = 0.02 (1029.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:27:37.900924: step 21400, loss = 0.02 (1065.0 examples/sec; 0.120 sec/batch)
2016-05-26 06:27:40.680692: step 21410, loss = 0.01 (1003.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:27:43.178508: step 21420, loss = 0.01 (1005.3 examples/sec; 0.127 sec/batch)
2016-05-26 06:27:45.667422: step 21430, loss = 0.07 (1056.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:27:48.163260: step 21440, loss = 0.05 (1064.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:27:50.632089: step 21450, loss = 0.03 (1021.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:27:53.150638: step 21460, loss = 0.01 (1038.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:27:55.616002: step 21470, loss = 0.02 (1050.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:27:58.115475: step 21480, loss = 0.01 (1000.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:28:00.623865: step 21490, loss = 0.02 (1001.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:28:03.118088: step 21500, loss = 0.02 (993.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:28:06.194854: step 21510, loss = 0.05 (1064.3 examples/sec; 0.120 sec/batch)
2016-05-26 06:28:08.638871: step 21520, loss = 0.03 (1021.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:28:11.113897: step 21530, loss = 0.01 (962.7 examples/sec; 0.133 sec/batch)
2016-05-26 06:28:13.535684: step 21540, loss = 0.03 (1079.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:28:15.978144: step 21550, loss = 0.01 (1036.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:28:18.445732: step 21560, loss = 0.07 (1014.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:28:20.902447: step 21570, loss = 0.01 (1004.3 examples/sec; 0.127 sec/batch)
2016-05-26 06:28:23.412318: step 21580, loss = 0.02 (1016.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:28:25.897639: step 21590, loss = 0.01 (1092.5 examples/sec; 0.117 sec/batch)
2016-05-26 06:28:28.347997: step 21600, loss = 0.05 (1011.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:28:31.110439: step 21610, loss = 0.02 (1051.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:28:33.575679: step 21620, loss = 0.01 (1080.2 examples/sec; 0.118 sec/batch)
2016-05-26 06:28:36.008171: step 21630, loss = 0.02 (1065.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:28:38.451241: step 21640, loss = 0.03 (979.2 examples/sec; 0.131 sec/batch)
2016-05-26 06:28:40.961878: step 21650, loss = 0.02 (1027.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:28:43.467649: step 21660, loss = 0.02 (958.5 examples/sec; 0.134 sec/batch)
2016-05-26 06:28:45.989487: step 21670, loss = 0.01 (1012.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:28:48.458961: step 21680, loss = 0.02 (1053.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:28:50.940650: step 21690, loss = 0.06 (1023.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:28:53.429809: step 21700, loss = 0.01 (967.0 examples/sec; 0.132 sec/batch)
2016-05-26 06:28:56.217526: step 21710, loss = 0.04 (1092.2 examples/sec; 0.117 sec/batch)
2016-05-26 06:28:58.614006: step 21720, loss = 0.02 (1100.7 examples/sec; 0.116 sec/batch)
2016-05-26 06:29:01.098093: step 21730, loss = 0.02 (1053.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:29:03.525832: step 21740, loss = 0.08 (1093.1 examples/sec; 0.117 sec/batch)
2016-05-26 06:29:05.985962: step 21750, loss = 0.04 (1075.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:29:08.484326: step 21760, loss = 0.02 (1017.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:29:10.942933: step 21770, loss = 0.03 (1054.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:29:13.418431: step 21780, loss = 0.02 (1080.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:29:15.879106: step 21790, loss = 0.03 (1018.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:29:18.363070: step 21800, loss = 0.06 (1111.1 examples/sec; 0.115 sec/batch)
2016-05-26 06:29:21.167527: step 21810, loss = 0.01 (1010.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:29:23.626989: step 21820, loss = 0.02 (1036.2 examples/sec; 0.124 sec/batch)
2016-05-26 06:29:26.043412: step 21830, loss = 0.04 (1075.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:29:28.565123: step 21840, loss = 0.03 (986.1 examples/sec; 0.130 sec/batch)
2016-05-26 06:29:31.009043: step 21850, loss = 0.02 (1034.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:29:33.411417: step 21860, loss = 0.03 (1085.2 examples/sec; 0.118 sec/batch)
2016-05-26 06:29:35.863656: step 21870, loss = 0.02 (1013.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:29:38.301599: step 21880, loss = 0.01 (1043.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:29:40.816242: step 21890, loss = 0.03 (1062.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:29:43.290680: step 21900, loss = 0.02 (1057.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:29:46.059186: step 21910, loss = 0.07 (1036.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:29:48.548409: step 21920, loss = 0.02 (1063.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:29:51.019674: step 21930, loss = 0.01 (1045.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:29:53.460343: step 21940, loss = 0.03 (1012.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:29:55.945576: step 21950, loss = 0.03 (1061.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:29:58.407513: step 21960, loss = 0.02 (1023.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:30:00.852448: step 21970, loss = 0.04 (1049.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:30:03.306084: step 21980, loss = 0.03 (1030.2 examples/sec; 0.124 sec/batch)
2016-05-26 06:30:05.806172: step 21990, loss = 0.04 (1058.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:30:08.281803: step 22000, loss = 0.01 (1002.4 examples/sec; 0.128 sec/batch)
eval once
2016-05-26 06:30:49.196140: accuracy @ 1 = 0.992, 49660 / 50048 at 0
2016-05-26 06:30:51.758044: step 22010, loss = 0.02 (1060.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:30:54.209952: step 22020, loss = 0.02 (1048.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:30:56.644333: step 22030, loss = 0.03 (1070.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:30:59.086242: step 22040, loss = 0.01 (1051.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:31:01.568147: step 22050, loss = 0.01 (993.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:31:04.022527: step 22060, loss = 0.02 (1001.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:31:06.462414: step 22070, loss = 0.06 (1063.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:31:08.903554: step 22080, loss = 0.05 (1057.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:31:11.375938: step 22090, loss = 0.02 (1023.1 examples/sec; 0.125 sec/batch)
2016-05-26 06:31:13.836025: step 22100, loss = 0.01 (1062.6 examples/sec; 0.120 sec/batch)
2016-05-26 06:31:16.598302: step 22110, loss = 0.02 (1055.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:31:19.036381: step 22120, loss = 0.01 (997.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:31:21.491808: step 22130, loss = 0.07 (1057.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:31:23.925382: step 22140, loss = 0.02 (1030.0 examples/sec; 0.124 sec/batch)
2016-05-26 06:31:26.402817: step 22150, loss = 0.02 (1039.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:31:28.874765: step 22160, loss = 0.04 (1007.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:31:31.341532: step 22170, loss = 0.03 (1085.7 examples/sec; 0.118 sec/batch)
2016-05-26 06:31:33.799438: step 22180, loss = 0.03 (996.1 examples/sec; 0.129 sec/batch)
2016-05-26 06:31:36.270345: step 22190, loss = 0.01 (1023.5 examples/sec; 0.125 sec/batch)
2016-05-26 06:31:38.727670: step 22200, loss = 0.02 (985.9 examples/sec; 0.130 sec/batch)
2016-05-26 06:31:41.588772: step 22210, loss = 0.07 (982.5 examples/sec; 0.130 sec/batch)
2016-05-26 06:31:44.090095: step 22220, loss = 0.07 (1057.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:31:46.544159: step 22230, loss = 0.04 (1101.3 examples/sec; 0.116 sec/batch)
2016-05-26 06:31:49.058703: step 22240, loss = 0.03 (999.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:31:51.566518: step 22250, loss = 0.06 (994.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:31:54.035378: step 22260, loss = 0.05 (1038.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:31:56.527541: step 22270, loss = 0.02 (1049.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:31:58.990626: step 22280, loss = 0.08 (1003.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:32:01.501060: step 22290, loss = 0.03 (1052.9 examples/sec; 0.122 sec/batch)
2016-05-26 06:32:03.988822: step 22300, loss = 0.02 (1100.0 examples/sec; 0.116 sec/batch)
2016-05-26 06:32:06.798414: step 22310, loss = 0.09 (994.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:32:09.336380: step 22320, loss = 0.02 (1004.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:32:11.892575: step 22330, loss = 0.05 (983.1 examples/sec; 0.130 sec/batch)
2016-05-26 06:32:14.379115: step 22340, loss = 0.05 (1043.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:32:16.868931: step 22350, loss = 0.03 (1021.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:32:19.358095: step 22360, loss = 0.02 (1049.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:32:21.863362: step 22370, loss = 0.05 (946.6 examples/sec; 0.135 sec/batch)
2016-05-26 06:32:24.365717: step 22380, loss = 0.03 (1018.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:32:26.824668: step 22390, loss = 0.04 (1037.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:32:29.312146: step 22400, loss = 0.01 (1026.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:32:32.152369: step 22410, loss = 0.01 (1043.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:32:34.632048: step 22420, loss = 0.03 (968.6 examples/sec; 0.132 sec/batch)
2016-05-26 06:32:37.095058: step 22430, loss = 0.02 (1028.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:32:39.582796: step 22440, loss = 0.01 (1023.2 examples/sec; 0.125 sec/batch)
2016-05-26 06:32:42.071651: step 22450, loss = 0.03 (1038.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:32:44.521258: step 22460, loss = 0.02 (1074.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:32:47.017670: step 22470, loss = 0.05 (1014.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:32:49.540878: step 22480, loss = 0.04 (1007.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:32:52.069230: step 22490, loss = 0.01 (966.2 examples/sec; 0.132 sec/batch)
2016-05-26 06:32:54.581992: step 22500, loss = 0.05 (992.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:32:57.313324: step 22510, loss = 0.01 (1091.9 examples/sec; 0.117 sec/batch)
2016-05-26 06:32:59.809376: step 22520, loss = 0.03 (1028.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:33:02.310763: step 22530, loss = 0.07 (1033.5 examples/sec; 0.124 sec/batch)
2016-05-26 06:33:04.765998: step 22540, loss = 0.02 (1015.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:33:07.270365: step 22550, loss = 0.04 (1037.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:33:09.754032: step 22560, loss = 0.02 (1066.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:33:12.250632: step 22570, loss = 0.03 (997.4 examples/sec; 0.128 sec/batch)
2016-05-26 06:33:14.714415: step 22580, loss = 0.01 (1045.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:33:17.173443: step 22590, loss = 0.01 (1036.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:33:19.680665: step 22600, loss = 0.04 (1028.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:33:22.444818: step 22610, loss = 0.03 (1068.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:33:24.910051: step 22620, loss = 0.02 (994.5 examples/sec; 0.129 sec/batch)
2016-05-26 06:33:27.359710: step 22630, loss = 0.02 (1092.9 examples/sec; 0.117 sec/batch)
2016-05-26 06:33:29.866324: step 22640, loss = 0.01 (1017.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:33:32.355462: step 22650, loss = 0.00 (1061.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:33:34.831618: step 22660, loss = 0.02 (1070.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:33:37.322223: step 22670, loss = 0.01 (1030.3 examples/sec; 0.124 sec/batch)
2016-05-26 06:33:39.798946: step 22680, loss = 0.02 (1010.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:33:42.305823: step 22690, loss = 0.04 (1001.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:33:44.817071: step 22700, loss = 0.05 (1064.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:33:47.540953: step 22710, loss = 0.03 (1050.0 examples/sec; 0.122 sec/batch)
2016-05-26 06:33:50.010703: step 22720, loss = 0.03 (1052.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:33:52.461097: step 22730, loss = 0.02 (1047.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:33:54.946654: step 22740, loss = 0.04 (1108.5 examples/sec; 0.115 sec/batch)
2016-05-26 06:33:57.407596: step 22750, loss = 0.04 (1111.3 examples/sec; 0.115 sec/batch)
2016-05-26 06:33:59.924957: step 22760, loss = 0.02 (935.7 examples/sec; 0.137 sec/batch)
2016-05-26 06:34:02.446662: step 22770, loss = 0.06 (993.1 examples/sec; 0.129 sec/batch)
2016-05-26 06:34:04.896386: step 22780, loss = 0.04 (1047.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:34:07.371015: step 22790, loss = 0.02 (1038.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:34:09.827255: step 22800, loss = 0.06 (985.0 examples/sec; 0.130 sec/batch)
2016-05-26 06:34:12.709983: step 22810, loss = 0.03 (994.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:34:15.236825: step 22820, loss = 0.02 (1010.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:34:17.712535: step 22830, loss = 0.04 (1043.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:34:20.193703: step 22840, loss = 0.03 (1061.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:34:22.712584: step 22850, loss = 0.01 (968.5 examples/sec; 0.132 sec/batch)
2016-05-26 06:34:25.213975: step 22860, loss = 0.02 (1060.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:34:27.692135: step 22870, loss = 0.04 (1080.3 examples/sec; 0.118 sec/batch)
2016-05-26 06:34:30.214452: step 22880, loss = 0.05 (988.0 examples/sec; 0.130 sec/batch)
2016-05-26 06:34:32.714104: step 22890, loss = 0.03 (974.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:34:35.205217: step 22900, loss = 0.03 (1100.0 examples/sec; 0.116 sec/batch)
2016-05-26 06:34:38.008881: step 22910, loss = 0.02 (1041.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:34:40.459656: step 22920, loss = 0.06 (1065.2 examples/sec; 0.120 sec/batch)
2016-05-26 06:34:42.953294: step 22930, loss = 0.01 (992.8 examples/sec; 0.129 sec/batch)
2016-05-26 06:34:45.457663: step 22940, loss = 0.00 (989.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:34:47.922875: step 22950, loss = 0.04 (1018.2 examples/sec; 0.126 sec/batch)
2016-05-26 06:34:50.402727: step 22960, loss = 0.05 (1023.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:34:52.860745: step 22970, loss = 0.03 (1053.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:34:55.311572: step 22980, loss = 0.03 (1025.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:34:57.806965: step 22990, loss = 0.00 (969.2 examples/sec; 0.132 sec/batch)
2016-05-26 06:35:00.306310: step 23000, loss = 0.06 (997.1 examples/sec; 0.128 sec/batch)
eval once
2016-05-26 06:35:42.053459: accuracy @ 1 = 0.990, 49539 / 50048 at 0
2016-05-26 06:35:44.538323: step 23010, loss = 0.06 (999.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:35:47.010914: step 23020, loss = 0.02 (969.3 examples/sec; 0.132 sec/batch)
2016-05-26 06:35:49.463973: step 23030, loss = 0.03 (1119.5 examples/sec; 0.114 sec/batch)
2016-05-26 06:35:51.916233: step 23040, loss = 0.01 (1075.7 examples/sec; 0.119 sec/batch)
2016-05-26 06:35:54.341677: step 23050, loss = 0.06 (1064.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:35:56.768465: step 23060, loss = 0.06 (1099.5 examples/sec; 0.116 sec/batch)
2016-05-26 06:35:59.227610: step 23070, loss = 0.04 (1054.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:36:01.654231: step 23080, loss = 0.03 (1064.3 examples/sec; 0.120 sec/batch)
2016-05-26 06:36:04.135963: step 23090, loss = 0.06 (1009.3 examples/sec; 0.127 sec/batch)
2016-05-26 06:36:06.591617: step 23100, loss = 0.04 (1045.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:36:09.370110: step 23110, loss = 0.02 (1056.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:36:11.798878: step 23120, loss = 0.01 (1082.3 examples/sec; 0.118 sec/batch)
2016-05-26 06:36:14.302865: step 23130, loss = 0.01 (1025.2 examples/sec; 0.125 sec/batch)
2016-05-26 06:36:16.761470: step 23140, loss = 0.01 (1113.0 examples/sec; 0.115 sec/batch)
2016-05-26 06:36:19.246146: step 23150, loss = 0.01 (1037.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:36:21.610570: step 23160, loss = 0.02 (1120.0 examples/sec; 0.114 sec/batch)
2016-05-26 06:36:24.070842: step 23170, loss = 0.01 (1073.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:36:26.548212: step 23180, loss = 0.02 (1111.3 examples/sec; 0.115 sec/batch)
2016-05-26 06:36:29.050985: step 23190, loss = 0.01 (1000.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:36:31.600068: step 23200, loss = 0.02 (992.7 examples/sec; 0.129 sec/batch)
2016-05-26 06:36:34.391403: step 23210, loss = 0.04 (1015.6 examples/sec; 0.126 sec/batch)
2016-05-26 06:36:36.908674: step 23220, loss = 0.04 (970.2 examples/sec; 0.132 sec/batch)
2016-05-26 06:36:39.419594: step 23230, loss = 0.01 (1024.1 examples/sec; 0.125 sec/batch)
2016-05-26 06:36:41.899465: step 23240, loss = 0.04 (1058.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:36:44.393422: step 23250, loss = 0.03 (978.8 examples/sec; 0.131 sec/batch)
2016-05-26 06:36:46.873719: step 23260, loss = 0.09 (997.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:36:49.394600: step 23270, loss = 0.06 (1007.3 examples/sec; 0.127 sec/batch)
2016-05-26 06:36:51.854999: step 23280, loss = 0.02 (1066.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:36:54.325033: step 23290, loss = 0.03 (1064.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:36:56.798486: step 23300, loss = 0.01 (994.7 examples/sec; 0.129 sec/batch)
2016-05-26 06:36:59.508486: step 23310, loss = 0.03 (1061.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:37:01.988783: step 23320, loss = 0.05 (991.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:37:04.444532: step 23330, loss = 0.01 (987.6 examples/sec; 0.130 sec/batch)
2016-05-26 06:37:06.921902: step 23340, loss = 0.02 (1041.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:37:09.413753: step 23350, loss = 0.03 (1043.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:37:11.937769: step 23360, loss = 0.01 (1018.7 examples/sec; 0.126 sec/batch)
2016-05-26 06:37:14.423680: step 23370, loss = 0.01 (1072.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:37:16.883363: step 23380, loss = 0.02 (1056.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:37:19.353615: step 23390, loss = 0.00 (1040.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:37:21.851761: step 23400, loss = 0.03 (1033.4 examples/sec; 0.124 sec/batch)
2016-05-26 06:37:24.689159: step 23410, loss = 0.04 (1003.4 examples/sec; 0.128 sec/batch)
2016-05-26 06:37:27.156144: step 23420, loss = 0.07 (1040.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:37:29.676529: step 23430, loss = 0.02 (1047.0 examples/sec; 0.122 sec/batch)
2016-05-26 06:37:32.141891: step 23440, loss = 0.01 (994.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:37:34.574073: step 23450, loss = 0.02 (1077.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:37:37.018819: step 23460, loss = 0.02 (1074.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:37:39.523945: step 23470, loss = 0.02 (987.7 examples/sec; 0.130 sec/batch)
2016-05-26 06:37:42.011534: step 23480, loss = 0.02 (1060.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:37:44.534251: step 23490, loss = 0.03 (1003.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:37:47.039949: step 23500, loss = 0.04 (972.4 examples/sec; 0.132 sec/batch)
2016-05-26 06:37:49.814266: step 23510, loss = 0.02 (968.6 examples/sec; 0.132 sec/batch)
2016-05-26 06:37:52.275462: step 23520, loss = 0.05 (1073.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:37:54.757115: step 23530, loss = 0.03 (1074.4 examples/sec; 0.119 sec/batch)
2016-05-26 06:37:57.277402: step 23540, loss = 0.03 (1039.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:37:59.719266: step 23550, loss = 0.04 (1065.3 examples/sec; 0.120 sec/batch)
2016-05-26 06:38:02.150185: step 23560, loss = 0.04 (962.9 examples/sec; 0.133 sec/batch)
2016-05-26 06:38:04.619185: step 23570, loss = 0.02 (984.9 examples/sec; 0.130 sec/batch)
2016-05-26 06:38:07.071103: step 23580, loss = 0.09 (1006.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:38:09.529707: step 23590, loss = 0.04 (1057.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:38:12.016485: step 23600, loss = 0.07 (952.7 examples/sec; 0.134 sec/batch)
2016-05-26 06:38:14.869445: step 23610, loss = 0.04 (1051.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:38:17.346679: step 23620, loss = 0.05 (1048.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:38:19.827450: step 23630, loss = 0.02 (1025.6 examples/sec; 0.125 sec/batch)
2016-05-26 06:38:22.268480: step 23640, loss = 0.07 (1055.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:38:24.725358: step 23650, loss = 0.01 (1063.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:38:27.180613: step 23660, loss = 0.05 (1070.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:38:29.641353: step 23670, loss = 0.06 (1097.1 examples/sec; 0.117 sec/batch)
2016-05-26 06:38:32.170784: step 23680, loss = 0.05 (974.1 examples/sec; 0.131 sec/batch)
2016-05-26 06:38:34.623250: step 23690, loss = 0.03 (1046.3 examples/sec; 0.122 sec/batch)
2016-05-26 06:38:37.122027: step 23700, loss = 0.02 (1045.0 examples/sec; 0.122 sec/batch)
2016-05-26 06:38:39.902016: step 23710, loss = 0.02 (1062.9 examples/sec; 0.120 sec/batch)
2016-05-26 06:38:42.381097: step 23720, loss = 0.06 (1120.1 examples/sec; 0.114 sec/batch)
2016-05-26 06:38:44.889729: step 23730, loss = 0.02 (1071.0 examples/sec; 0.120 sec/batch)
2016-05-26 06:38:47.339897: step 23740, loss = 0.01 (1072.3 examples/sec; 0.119 sec/batch)
2016-05-26 06:38:49.840803: step 23750, loss = 0.02 (1093.9 examples/sec; 0.117 sec/batch)
2016-05-26 06:38:52.342764: step 23760, loss = 0.01 (1043.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:38:54.819731: step 23770, loss = 0.01 (1042.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:38:57.304161: step 23780, loss = 0.03 (992.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:38:59.785257: step 23790, loss = 0.01 (1029.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:39:02.238321: step 23800, loss = 0.01 (1016.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:39:05.093014: step 23810, loss = 0.04 (1043.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:39:07.541900: step 23820, loss = 0.03 (1033.5 examples/sec; 0.124 sec/batch)
2016-05-26 06:39:10.042163: step 23830, loss = 0.03 (1041.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:39:12.529534: step 23840, loss = 0.02 (1085.6 examples/sec; 0.118 sec/batch)
2016-05-26 06:39:15.034693: step 23850, loss = 0.01 (1060.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:39:17.501518: step 23860, loss = 0.02 (1072.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:39:19.969932: step 23870, loss = 0.01 (1014.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:39:22.477910: step 23880, loss = 0.01 (982.3 examples/sec; 0.130 sec/batch)
2016-05-26 06:39:24.954649: step 23890, loss = 0.03 (1052.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:39:27.414956: step 23900, loss = 0.05 (1072.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:39:30.195802: step 23910, loss = 0.02 (1027.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:39:32.650886: step 23920, loss = 0.04 (1044.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:39:35.124205: step 23930, loss = 0.03 (1025.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:39:37.585734: step 23940, loss = 0.01 (1045.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:39:40.067217: step 23950, loss = 0.01 (1068.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:39:42.517614: step 23960, loss = 0.02 (1069.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:39:45.006006: step 23970, loss = 0.02 (1013.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:39:47.473576: step 23980, loss = 0.02 (990.7 examples/sec; 0.129 sec/batch)
2016-05-26 06:39:49.971913: step 23990, loss = 0.02 (1045.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:39:52.468871: step 24000, loss = 0.01 (1036.7 examples/sec; 0.123 sec/batch)
eval once
2016-05-26 06:40:34.530243: accuracy @ 1 = 0.993, 49675 / 50048 at 0
2016-05-26 06:40:37.102786: step 24010, loss = 0.04 (984.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:40:39.556954: step 24020, loss = 0.02 (992.1 examples/sec; 0.129 sec/batch)
2016-05-26 06:40:42.038191: step 24030, loss = 0.01 (1103.8 examples/sec; 0.116 sec/batch)
2016-05-26 06:40:44.457252: step 24040, loss = 0.01 (1055.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:40:46.846510: step 24050, loss = 0.02 (1089.7 examples/sec; 0.117 sec/batch)
2016-05-26 06:40:49.335183: step 24060, loss = 0.03 (989.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:40:51.822497: step 24070, loss = 0.00 (1017.2 examples/sec; 0.126 sec/batch)
2016-05-26 06:40:54.295611: step 24080, loss = 0.03 (1004.8 examples/sec; 0.127 sec/batch)
2016-05-26 06:40:56.793903: step 24090, loss = 0.03 (1090.0 examples/sec; 0.117 sec/batch)
2016-05-26 06:40:59.241922: step 24100, loss = 0.05 (1082.6 examples/sec; 0.118 sec/batch)
2016-05-26 06:41:02.002449: step 24110, loss = 0.00 (998.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:41:04.450610: step 24120, loss = 0.02 (1048.3 examples/sec; 0.122 sec/batch)
2016-05-26 06:41:06.934118: step 24130, loss = 0.05 (978.1 examples/sec; 0.131 sec/batch)
2016-05-26 06:41:09.369015: step 24140, loss = 0.01 (1080.2 examples/sec; 0.118 sec/batch)
2016-05-26 06:41:11.933642: step 24150, loss = 0.01 (1034.3 examples/sec; 0.124 sec/batch)
2016-05-26 06:41:14.430093: step 24160, loss = 0.02 (1059.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:41:16.908808: step 24170, loss = 0.07 (1057.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:41:19.450269: step 24180, loss = 0.05 (1016.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:41:21.960606: step 24190, loss = 0.02 (997.8 examples/sec; 0.128 sec/batch)
2016-05-26 06:41:24.434764: step 24200, loss = 0.02 (990.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:41:27.253194: step 24210, loss = 0.03 (969.7 examples/sec; 0.132 sec/batch)
2016-05-26 06:41:29.693461: step 24220, loss = 0.07 (1077.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:41:32.209133: step 24230, loss = 0.03 (1032.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:41:34.671206: step 24240, loss = 0.01 (1052.0 examples/sec; 0.122 sec/batch)
2016-05-26 06:41:37.208247: step 24250, loss = 0.02 (963.8 examples/sec; 0.133 sec/batch)
2016-05-26 06:41:39.765377: step 24260, loss = 0.10 (965.8 examples/sec; 0.133 sec/batch)
2016-05-26 06:41:42.225559: step 24270, loss = 0.02 (1089.1 examples/sec; 0.118 sec/batch)
2016-05-26 06:41:44.713748: step 24280, loss = 0.03 (988.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:41:47.150170: step 24290, loss = 0.01 (1093.4 examples/sec; 0.117 sec/batch)
2016-05-26 06:41:49.659825: step 24300, loss = 0.02 (1043.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:41:52.502708: step 24310, loss = 0.02 (1044.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:41:54.988769: step 24320, loss = 0.06 (982.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:41:57.459157: step 24330, loss = 0.05 (1077.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:41:59.917359: step 24340, loss = 0.01 (1080.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:42:02.420525: step 24350, loss = 0.02 (1042.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:42:04.963040: step 24360, loss = 0.02 (1035.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:42:07.451774: step 24370, loss = 0.01 (1030.1 examples/sec; 0.124 sec/batch)
2016-05-26 06:42:09.940416: step 24380, loss = 0.02 (1071.0 examples/sec; 0.120 sec/batch)
2016-05-26 06:42:12.428506: step 24390, loss = 0.01 (1039.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:42:14.944168: step 24400, loss = 0.02 (983.3 examples/sec; 0.130 sec/batch)
2016-05-26 06:42:17.760308: step 24410, loss = 0.01 (1016.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:42:20.255718: step 24420, loss = 0.02 (1001.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:42:22.692926: step 24430, loss = 0.04 (1059.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:42:25.135162: step 24440, loss = 0.02 (1039.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:42:27.633940: step 24450, loss = 0.01 (1005.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:42:30.141338: step 24460, loss = 0.01 (949.3 examples/sec; 0.135 sec/batch)
2016-05-26 06:42:32.588105: step 24470, loss = 0.01 (1070.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:42:35.105851: step 24480, loss = 0.01 (996.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:42:37.629101: step 24490, loss = 0.01 (1030.3 examples/sec; 0.124 sec/batch)
2016-05-26 06:42:40.100068: step 24500, loss = 0.06 (999.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:42:42.874277: step 24510, loss = 0.02 (1025.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:42:45.334213: step 24520, loss = 0.02 (981.8 examples/sec; 0.130 sec/batch)
2016-05-26 06:42:47.804156: step 24530, loss = 0.02 (1032.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:42:50.277181: step 24540, loss = 0.05 (1021.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:42:52.781522: step 24550, loss = 0.02 (1008.4 examples/sec; 0.127 sec/batch)
2016-05-26 06:42:55.250212: step 24560, loss = 0.03 (1050.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:42:57.750942: step 24570, loss = 0.01 (994.2 examples/sec; 0.129 sec/batch)
2016-05-26 06:43:00.229973: step 24580, loss = 0.01 (1015.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:43:02.736680: step 24590, loss = 0.04 (1011.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:43:05.201331: step 24600, loss = 0.02 (1012.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:43:07.943784: step 24610, loss = 0.02 (1033.5 examples/sec; 0.124 sec/batch)
2016-05-26 06:43:10.418269: step 24620, loss = 0.06 (1041.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:43:12.894134: step 24630, loss = 0.02 (976.4 examples/sec; 0.131 sec/batch)
2016-05-26 06:43:15.366020: step 24640, loss = 0.03 (1099.8 examples/sec; 0.116 sec/batch)
2016-05-26 06:43:17.866322: step 24650, loss = 0.05 (967.5 examples/sec; 0.132 sec/batch)
2016-05-26 06:43:20.333137: step 24660, loss = 0.02 (1010.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:43:22.818111: step 24670, loss = 0.03 (1006.4 examples/sec; 0.127 sec/batch)
2016-05-26 06:43:25.328380: step 24680, loss = 0.03 (1041.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:43:27.791172: step 24690, loss = 0.01 (1102.2 examples/sec; 0.116 sec/batch)
2016-05-26 06:43:30.237427: step 24700, loss = 0.01 (1071.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:43:33.032182: step 24710, loss = 0.01 (1021.2 examples/sec; 0.125 sec/batch)
2016-05-26 06:43:35.481902: step 24720, loss = 0.01 (1078.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:43:37.920745: step 24730, loss = 0.02 (1019.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:43:40.424274: step 24740, loss = 0.01 (1056.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:43:42.921223: step 24750, loss = 0.08 (1072.2 examples/sec; 0.119 sec/batch)
2016-05-26 06:43:45.402382: step 24760, loss = 0.04 (1050.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:43:47.934162: step 24770, loss = 0.02 (1042.1 examples/sec; 0.123 sec/batch)
2016-05-26 06:43:50.424082: step 24780, loss = 0.05 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:43:52.919556: step 24790, loss = 0.09 (1010.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:43:55.456887: step 24800, loss = 0.02 (994.1 examples/sec; 0.129 sec/batch)
2016-05-26 06:43:58.262415: step 24810, loss = 0.01 (1052.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:44:00.764906: step 24820, loss = 0.02 (998.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:44:03.259307: step 24830, loss = 0.01 (1010.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:44:05.717203: step 24840, loss = 0.07 (1031.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:44:08.200135: step 24850, loss = 0.04 (1053.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:44:10.649841: step 24860, loss = 0.02 (1057.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:44:13.144417: step 24870, loss = 0.01 (1042.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:44:15.634372: step 24880, loss = 0.02 (1054.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:44:18.121471: step 24890, loss = 0.03 (1017.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:44:20.606771: step 24900, loss = 0.03 (1025.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:44:23.383154: step 24910, loss = 0.01 (1055.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:44:25.829416: step 24920, loss = 0.04 (1037.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:44:28.323978: step 24930, loss = 0.02 (1004.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:44:30.831849: step 24940, loss = 0.01 (1077.4 examples/sec; 0.119 sec/batch)
2016-05-26 06:44:33.296539: step 24950, loss = 0.04 (1075.5 examples/sec; 0.119 sec/batch)
2016-05-26 06:44:35.750242: step 24960, loss = 0.01 (1047.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:44:38.244771: step 24970, loss = 0.03 (997.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:44:40.713975: step 24980, loss = 0.01 (1016.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:44:43.219359: step 24990, loss = 0.02 (993.9 examples/sec; 0.129 sec/batch)
2016-05-26 06:44:45.682335: step 25000, loss = 0.02 (1038.8 examples/sec; 0.123 sec/batch)
eval once
2016-05-26 06:45:27.009623: accuracy @ 1 = 0.994, 49771 / 50048 at 0
2016-05-26 06:45:29.561802: step 25010, loss = 0.02 (1083.8 examples/sec; 0.118 sec/batch)
2016-05-26 06:45:32.015292: step 25020, loss = 0.06 (1031.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:45:34.415802: step 25030, loss = 0.03 (1091.5 examples/sec; 0.117 sec/batch)
2016-05-26 06:45:36.836818: step 25040, loss = 0.02 (1018.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:45:39.459071: step 25050, loss = 0.01 (1039.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:45:41.908000: step 25060, loss = 0.06 (1060.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:45:44.388940: step 25070, loss = 0.01 (1014.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:45:46.857754: step 25080, loss = 0.06 (1048.3 examples/sec; 0.122 sec/batch)
2016-05-26 06:45:49.326101: step 25090, loss = 0.01 (1059.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:45:51.788598: step 25100, loss = 0.01 (1000.1 examples/sec; 0.128 sec/batch)
2016-05-26 06:45:54.555192: step 25110, loss = 0.03 (1016.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:45:57.001404: step 25120, loss = 0.05 (1027.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:45:59.463561: step 25130, loss = 0.06 (1004.5 examples/sec; 0.127 sec/batch)
2016-05-26 06:46:01.967848: step 25140, loss = 0.03 (1053.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:46:04.455415: step 25150, loss = 0.02 (995.8 examples/sec; 0.129 sec/batch)
2016-05-26 06:46:06.989038: step 25160, loss = 0.03 (970.0 examples/sec; 0.132 sec/batch)
2016-05-26 06:46:09.475648: step 25170, loss = 0.01 (1042.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:46:11.957741: step 25180, loss = 0.02 (1051.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:46:14.428088: step 25190, loss = 0.02 (1082.4 examples/sec; 0.118 sec/batch)
2016-05-26 06:46:16.909910: step 25200, loss = 0.01 (1004.5 examples/sec; 0.127 sec/batch)
2016-05-26 06:46:19.682618: step 25210, loss = 0.02 (1074.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:46:22.176378: step 25220, loss = 0.01 (986.4 examples/sec; 0.130 sec/batch)
2016-05-26 06:46:24.663866: step 25230, loss = 0.07 (1092.5 examples/sec; 0.117 sec/batch)
2016-05-26 06:46:27.129909: step 25240, loss = 0.02 (1022.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:46:29.646700: step 25250, loss = 0.02 (989.7 examples/sec; 0.129 sec/batch)
2016-05-26 06:46:32.106265: step 25260, loss = 0.01 (991.8 examples/sec; 0.129 sec/batch)
2016-05-26 06:46:34.554724: step 25270, loss = 0.03 (1055.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:46:37.079006: step 25280, loss = 0.01 (996.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:46:39.588771: step 25290, loss = 0.02 (1036.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:46:42.033968: step 25300, loss = 0.01 (1069.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:46:44.818022: step 25310, loss = 0.01 (987.5 examples/sec; 0.130 sec/batch)
2016-05-26 06:46:47.319726: step 25320, loss = 0.04 (1046.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:46:49.807703: step 25330, loss = 0.01 (1060.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:46:52.310642: step 25340, loss = 0.04 (1034.4 examples/sec; 0.124 sec/batch)
2016-05-26 06:46:54.757006: step 25350, loss = 0.02 (1054.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:46:57.191432: step 25360, loss = 0.04 (1055.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:46:59.698753: step 25370, loss = 0.02 (1113.8 examples/sec; 0.115 sec/batch)
2016-05-26 06:47:02.163583: step 25380, loss = 0.01 (1116.5 examples/sec; 0.115 sec/batch)
2016-05-26 06:47:04.687273: step 25390, loss = 0.03 (1060.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:47:07.159465: step 25400, loss = 0.03 (1104.5 examples/sec; 0.116 sec/batch)
2016-05-26 06:47:09.896835: step 25410, loss = 0.02 (1090.3 examples/sec; 0.117 sec/batch)
2016-05-26 06:47:12.359380: step 25420, loss = 0.01 (1028.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:47:14.827222: step 25430, loss = 0.01 (1028.5 examples/sec; 0.124 sec/batch)
2016-05-26 06:47:17.258237: step 25440, loss = 0.01 (1056.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:47:19.725258: step 25450, loss = 0.01 (1040.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:47:22.223878: step 25460, loss = 0.02 (1037.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:47:24.721887: step 25470, loss = 0.01 (987.6 examples/sec; 0.130 sec/batch)
2016-05-26 06:47:27.205534: step 25480, loss = 0.01 (1036.4 examples/sec; 0.124 sec/batch)
2016-05-26 06:47:29.709941: step 25490, loss = 0.02 (1002.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:47:32.215946: step 25500, loss = 0.00 (1005.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:47:34.963831: step 25510, loss = 0.04 (1064.0 examples/sec; 0.120 sec/batch)
2016-05-26 06:47:37.432659: step 25520, loss = 0.01 (1052.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:47:39.924378: step 25530, loss = 0.05 (1090.0 examples/sec; 0.117 sec/batch)
2016-05-26 06:47:42.387006: step 25540, loss = 0.01 (1051.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:47:44.851370: step 25550, loss = 0.04 (1056.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:47:47.308808: step 25560, loss = 0.01 (1090.9 examples/sec; 0.117 sec/batch)
2016-05-26 06:47:49.798137: step 25570, loss = 0.00 (1073.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:47:52.237700: step 25580, loss = 0.01 (1071.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:47:54.723227: step 25590, loss = 0.00 (1015.3 examples/sec; 0.126 sec/batch)
2016-05-26 06:47:57.186784: step 25600, loss = 0.02 (1057.6 examples/sec; 0.121 sec/batch)
2016-05-26 06:48:00.023350: step 25610, loss = 0.03 (1036.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:48:02.495459: step 25620, loss = 0.02 (1087.2 examples/sec; 0.118 sec/batch)
2016-05-26 06:48:04.952003: step 25630, loss = 0.01 (1069.7 examples/sec; 0.120 sec/batch)
2016-05-26 06:48:07.441808: step 25640, loss = 0.01 (1046.3 examples/sec; 0.122 sec/batch)
2016-05-26 06:48:09.875259: step 25650, loss = 0.05 (1024.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:48:12.378350: step 25660, loss = 0.01 (1037.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:48:14.855068: step 25670, loss = 0.01 (984.3 examples/sec; 0.130 sec/batch)
2016-05-26 06:48:17.378633: step 25680, loss = 0.01 (1009.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:48:19.870531: step 25690, loss = 0.01 (1052.9 examples/sec; 0.122 sec/batch)
2016-05-26 06:48:22.379963: step 25700, loss = 0.02 (1052.3 examples/sec; 0.122 sec/batch)
2016-05-26 06:48:25.114155: step 25710, loss = 0.03 (1083.5 examples/sec; 0.118 sec/batch)
2016-05-26 06:48:27.627194: step 25720, loss = 0.01 (1002.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:48:30.102530: step 25730, loss = 0.02 (1005.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:48:32.563867: step 25740, loss = 0.01 (1015.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:48:35.037964: step 25750, loss = 0.06 (998.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:48:37.531198: step 25760, loss = 0.01 (993.9 examples/sec; 0.129 sec/batch)
2016-05-26 06:48:40.023225: step 25770, loss = 0.00 (988.0 examples/sec; 0.130 sec/batch)
2016-05-26 06:48:42.464774: step 25780, loss = 0.00 (1021.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:48:44.969492: step 25790, loss = 0.03 (993.7 examples/sec; 0.129 sec/batch)
2016-05-26 06:48:47.482839: step 25800, loss = 0.01 (1023.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:48:50.291670: step 25810, loss = 0.00 (1077.1 examples/sec; 0.119 sec/batch)
2016-05-26 06:48:52.734719: step 25820, loss = 0.01 (1036.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:48:55.223671: step 25830, loss = 0.02 (1084.0 examples/sec; 0.118 sec/batch)
2016-05-26 06:48:57.716528: step 25840, loss = 0.03 (997.5 examples/sec; 0.128 sec/batch)
2016-05-26 06:49:00.220138: step 25850, loss = 0.01 (993.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:49:02.680270: step 25860, loss = 0.00 (1080.7 examples/sec; 0.118 sec/batch)
2016-05-26 06:49:05.142120: step 25870, loss = 0.01 (1018.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:49:07.632568: step 25880, loss = 0.00 (1064.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:49:10.143552: step 25890, loss = 0.01 (1003.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:49:12.602529: step 25900, loss = 0.01 (1086.4 examples/sec; 0.118 sec/batch)
2016-05-26 06:49:15.697034: step 25910, loss = 0.02 (1000.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:49:18.181497: step 25920, loss = 0.01 (1076.1 examples/sec; 0.119 sec/batch)
2016-05-26 06:49:20.692219: step 25930, loss = 0.03 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:49:23.190577: step 25940, loss = 0.02 (1069.0 examples/sec; 0.120 sec/batch)
2016-05-26 06:49:25.673145: step 25950, loss = 0.01 (979.1 examples/sec; 0.131 sec/batch)
2016-05-26 06:49:28.179431: step 25960, loss = 0.01 (1033.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:49:30.660871: step 25970, loss = 0.00 (1013.8 examples/sec; 0.126 sec/batch)
2016-05-26 06:49:33.105735: step 25980, loss = 0.01 (1070.5 examples/sec; 0.120 sec/batch)
2016-05-26 06:49:35.604109: step 25990, loss = 0.03 (1017.9 examples/sec; 0.126 sec/batch)
2016-05-26 06:49:38.058669: step 26000, loss = 0.00 (1067.9 examples/sec; 0.120 sec/batch)
eval once
2016-05-26 06:50:19.684384: accuracy @ 1 = 0.996, 49840 / 50048 at 0
2016-05-26 06:50:22.179014: step 26010, loss = 0.02 (1076.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:50:24.676997: step 26020, loss = 0.01 (1079.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:50:27.205650: step 26030, loss = 0.00 (1035.2 examples/sec; 0.124 sec/batch)
2016-05-26 06:50:29.711756: step 26040, loss = 0.01 (985.0 examples/sec; 0.130 sec/batch)
2016-05-26 06:50:32.177717: step 26050, loss = 0.01 (1011.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:50:34.636936: step 26060, loss = 0.00 (1040.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:50:37.112248: step 26070, loss = 0.00 (1066.9 examples/sec; 0.120 sec/batch)
2016-05-26 06:50:39.558002: step 26080, loss = 0.01 (1077.0 examples/sec; 0.119 sec/batch)
2016-05-26 06:50:42.051095: step 26090, loss = 0.02 (1029.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:50:44.512308: step 26100, loss = 0.00 (1056.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:50:47.286358: step 26110, loss = 0.01 (1005.1 examples/sec; 0.127 sec/batch)
2016-05-26 06:50:49.758031: step 26120, loss = 0.01 (1055.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:50:52.219734: step 26130, loss = 0.01 (1059.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:50:54.680595: step 26140, loss = 0.01 (1048.0 examples/sec; 0.122 sec/batch)
2016-05-26 06:50:57.148523: step 26150, loss = 0.03 (1043.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:50:59.652949: step 26160, loss = 0.01 (1035.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:51:02.106557: step 26170, loss = 0.01 (1043.7 examples/sec; 0.123 sec/batch)
2016-05-26 06:51:04.621062: step 26180, loss = 0.01 (999.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:51:07.137777: step 26190, loss = 0.01 (999.6 examples/sec; 0.128 sec/batch)
2016-05-26 06:51:09.646461: step 26200, loss = 0.01 (1042.9 examples/sec; 0.123 sec/batch)
2016-05-26 06:51:12.408987: step 26210, loss = 0.01 (1030.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:51:14.875792: step 26220, loss = 0.02 (1030.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:51:17.334149: step 26230, loss = 0.05 (1047.8 examples/sec; 0.122 sec/batch)
2016-05-26 06:51:19.805059: step 26240, loss = 0.02 (1033.3 examples/sec; 0.124 sec/batch)
2016-05-26 06:51:22.275553: step 26250, loss = 0.01 (1059.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:51:24.732699: step 26260, loss = 0.02 (1025.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:51:27.227432: step 26270, loss = 0.00 (953.0 examples/sec; 0.134 sec/batch)
2016-05-26 06:51:29.709825: step 26280, loss = 0.01 (993.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:51:32.163899: step 26290, loss = 0.02 (1000.0 examples/sec; 0.128 sec/batch)
2016-05-26 06:51:34.660902: step 26300, loss = 0.00 (1080.6 examples/sec; 0.118 sec/batch)
2016-05-26 06:51:37.435855: step 26310, loss = 0.00 (1044.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:51:39.931165: step 26320, loss = 0.01 (1020.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:51:42.456174: step 26330, loss = 0.01 (1001.2 examples/sec; 0.128 sec/batch)
2016-05-26 06:51:44.970026: step 26340, loss = 0.03 (973.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:51:47.417871: step 26350, loss = 0.02 (1016.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:51:49.901441: step 26360, loss = 0.01 (1042.8 examples/sec; 0.123 sec/batch)
2016-05-26 06:51:52.413901: step 26370, loss = 0.02 (1033.2 examples/sec; 0.124 sec/batch)
2016-05-26 06:51:54.933598: step 26380, loss = 0.01 (992.4 examples/sec; 0.129 sec/batch)
2016-05-26 06:51:57.351113: step 26390, loss = 0.04 (1046.9 examples/sec; 0.122 sec/batch)
2016-05-26 06:51:59.744815: step 26400, loss = 0.00 (1109.8 examples/sec; 0.115 sec/batch)
2016-05-26 06:52:02.609377: step 26410, loss = 0.01 (929.6 examples/sec; 0.138 sec/batch)
2016-05-26 06:52:05.079420: step 26420, loss = 0.02 (1048.6 examples/sec; 0.122 sec/batch)
2016-05-26 06:52:07.523858: step 26430, loss = 0.02 (997.7 examples/sec; 0.128 sec/batch)
2016-05-26 06:52:10.012244: step 26440, loss = 0.01 (984.9 examples/sec; 0.130 sec/batch)
2016-05-26 06:52:12.499332: step 26450, loss = 0.04 (1059.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:52:14.997729: step 26460, loss = 0.04 (1041.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:52:17.473387: step 26470, loss = 0.01 (1005.7 examples/sec; 0.127 sec/batch)
2016-05-26 06:52:19.960980: step 26480, loss = 0.01 (1067.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:52:22.404445: step 26490, loss = 0.02 (984.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:52:24.870562: step 26500, loss = 0.04 (1016.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:52:27.703347: step 26510, loss = 0.01 (1000.3 examples/sec; 0.128 sec/batch)
2016-05-26 06:52:30.220812: step 26520, loss = 0.01 (981.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:52:32.733610: step 26530, loss = 0.01 (978.7 examples/sec; 0.131 sec/batch)
2016-05-26 06:52:35.226372: step 26540, loss = 0.01 (1030.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:52:37.718264: step 26550, loss = 0.01 (1076.9 examples/sec; 0.119 sec/batch)
2016-05-26 06:52:40.215133: step 26560, loss = 0.01 (1088.5 examples/sec; 0.118 sec/batch)
2016-05-26 06:52:42.717689: step 26570, loss = 0.01 (1042.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:52:45.144429: step 26580, loss = 0.02 (1060.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:52:47.666723: step 26590, loss = 0.12 (986.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:52:50.164369: step 26600, loss = 0.01 (1072.9 examples/sec; 0.119 sec/batch)
2016-05-26 06:52:53.002807: step 26610, loss = 0.01 (974.6 examples/sec; 0.131 sec/batch)
2016-05-26 06:52:55.493897: step 26620, loss = 0.04 (1064.6 examples/sec; 0.120 sec/batch)
2016-05-26 06:52:58.006674: step 26630, loss = 0.01 (1023.8 examples/sec; 0.125 sec/batch)
2016-05-26 06:53:00.448518: step 26640, loss = 0.00 (995.0 examples/sec; 0.129 sec/batch)
2016-05-26 06:53:02.946501: step 26650, loss = 0.02 (1056.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:53:05.489876: step 26660, loss = 0.01 (1036.1 examples/sec; 0.124 sec/batch)
2016-05-26 06:53:07.977575: step 26670, loss = 0.02 (991.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:53:10.431895: step 26680, loss = 0.01 (1065.4 examples/sec; 0.120 sec/batch)
2016-05-26 06:53:12.934978: step 26690, loss = 0.01 (1046.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:53:15.412310: step 26700, loss = 0.01 (1004.2 examples/sec; 0.127 sec/batch)
2016-05-26 06:53:18.251252: step 26710, loss = 0.02 (1018.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:53:20.769960: step 26720, loss = 0.00 (1053.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:53:23.333079: step 26730, loss = 0.01 (1000.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:53:25.812921: step 26740, loss = 0.02 (1014.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:53:28.326806: step 26750, loss = 0.00 (987.7 examples/sec; 0.130 sec/batch)
2016-05-26 06:53:30.839901: step 26760, loss = 0.02 (990.2 examples/sec; 0.129 sec/batch)
2016-05-26 06:53:33.329196: step 26770, loss = 0.00 (1019.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:53:35.818921: step 26780, loss = 0.01 (985.6 examples/sec; 0.130 sec/batch)
2016-05-26 06:53:38.333802: step 26790, loss = 0.03 (1019.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:53:40.824931: step 26800, loss = 0.01 (1057.0 examples/sec; 0.121 sec/batch)
2016-05-26 06:53:43.595280: step 26810, loss = 0.00 (1098.0 examples/sec; 0.117 sec/batch)
2016-05-26 06:53:46.095569: step 26820, loss = 0.02 (1025.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:53:48.599071: step 26830, loss = 0.04 (1098.8 examples/sec; 0.116 sec/batch)
2016-05-26 06:53:51.060321: step 26840, loss = 0.02 (1036.0 examples/sec; 0.124 sec/batch)
2016-05-26 06:53:53.533815: step 26850, loss = 0.01 (1018.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:53:56.033044: step 26860, loss = 0.02 (951.9 examples/sec; 0.134 sec/batch)
2016-05-26 06:53:58.515291: step 26870, loss = 0.00 (1081.9 examples/sec; 0.118 sec/batch)
2016-05-26 06:54:01.003679: step 26880, loss = 0.01 (1033.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:54:03.491830: step 26890, loss = 0.01 (1019.2 examples/sec; 0.126 sec/batch)
2016-05-26 06:54:05.978109: step 26900, loss = 0.02 (978.3 examples/sec; 0.131 sec/batch)
2016-05-26 06:54:08.805906: step 26910, loss = 0.01 (990.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:54:11.276996: step 26920, loss = 0.04 (1022.9 examples/sec; 0.125 sec/batch)
2016-05-26 06:54:13.794048: step 26930, loss = 0.01 (977.3 examples/sec; 0.131 sec/batch)
2016-05-26 06:54:16.259152: step 26940, loss = 0.02 (1061.3 examples/sec; 0.121 sec/batch)
2016-05-26 06:54:18.730207: step 26950, loss = 0.02 (1054.5 examples/sec; 0.121 sec/batch)
2016-05-26 06:54:21.253660: step 26960, loss = 0.01 (1055.1 examples/sec; 0.121 sec/batch)
2016-05-26 06:54:23.754225: step 26970, loss = 0.01 (1051.4 examples/sec; 0.122 sec/batch)
2016-05-26 06:54:26.181100: step 26980, loss = 0.01 (1052.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:54:28.627207: step 26990, loss = 0.01 (1071.6 examples/sec; 0.119 sec/batch)
2016-05-26 06:54:31.099014: step 27000, loss = 0.01 (1060.3 examples/sec; 0.121 sec/batch)
eval once
2016-05-26 06:55:13.262065: accuracy @ 1 = 0.997, 49905 / 50048 at 0
2016-05-26 06:55:15.773914: step 27010, loss = 0.02 (1089.6 examples/sec; 0.117 sec/batch)
2016-05-26 06:55:18.255569: step 27020, loss = 0.02 (1024.1 examples/sec; 0.125 sec/batch)
2016-05-26 06:55:20.782876: step 27030, loss = 0.02 (1007.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:55:23.283706: step 27040, loss = 0.00 (989.3 examples/sec; 0.129 sec/batch)
2016-05-26 06:55:25.781477: step 27050, loss = 0.02 (1023.1 examples/sec; 0.125 sec/batch)
2016-05-26 06:55:28.253881: step 27060, loss = 0.01 (1064.6 examples/sec; 0.120 sec/batch)
2016-05-26 06:55:30.706457: step 27070, loss = 0.01 (1012.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:55:33.150218: step 27080, loss = 0.05 (1004.3 examples/sec; 0.127 sec/batch)
2016-05-26 06:55:35.658910: step 27090, loss = 0.02 (1038.0 examples/sec; 0.123 sec/batch)
2016-05-26 06:55:38.096206: step 27100, loss = 0.00 (1022.3 examples/sec; 0.125 sec/batch)
2016-05-26 06:55:40.918080: step 27110, loss = 0.01 (1051.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:55:43.429277: step 27120, loss = 0.01 (962.1 examples/sec; 0.133 sec/batch)
2016-05-26 06:55:45.918349: step 27130, loss = 0.03 (1049.1 examples/sec; 0.122 sec/batch)
2016-05-26 06:55:48.436765: step 27140, loss = 0.01 (1037.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:55:50.941021: step 27150, loss = 0.01 (1063.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:55:53.401122: step 27160, loss = 0.01 (1003.9 examples/sec; 0.128 sec/batch)
2016-05-26 06:55:55.864483: step 27170, loss = 0.00 (1023.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:55:58.361459: step 27180, loss = 0.03 (1029.8 examples/sec; 0.124 sec/batch)
2016-05-26 06:56:00.832332: step 27190, loss = 0.01 (1034.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:56:03.304446: step 27200, loss = 0.00 (1070.2 examples/sec; 0.120 sec/batch)
2016-05-26 06:56:06.088392: step 27210, loss = 0.02 (1052.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:56:08.560708: step 27220, loss = 0.01 (1074.4 examples/sec; 0.119 sec/batch)
2016-05-26 06:56:11.071131: step 27230, loss = 0.01 (1090.1 examples/sec; 0.117 sec/batch)
2016-05-26 06:56:13.524252: step 27240, loss = 0.03 (1021.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:56:16.004321: step 27250, loss = 0.00 (1007.4 examples/sec; 0.127 sec/batch)
2016-05-26 06:56:18.501387: step 27260, loss = 0.01 (1034.7 examples/sec; 0.124 sec/batch)
2016-05-26 06:56:20.943799: step 27270, loss = 0.04 (1042.0 examples/sec; 0.123 sec/batch)
2016-05-26 06:56:23.488944: step 27280, loss = 0.02 (1018.0 examples/sec; 0.126 sec/batch)
2016-05-26 06:56:25.966584: step 27290, loss = 0.03 (1041.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:56:28.423875: step 27300, loss = 0.03 (988.2 examples/sec; 0.130 sec/batch)
2016-05-26 06:56:31.232238: step 27310, loss = 0.02 (1062.8 examples/sec; 0.120 sec/batch)
2016-05-26 06:56:33.693099: step 27320, loss = 0.01 (1034.1 examples/sec; 0.124 sec/batch)
2016-05-26 06:56:36.207895: step 27330, loss = 0.06 (1041.5 examples/sec; 0.123 sec/batch)
2016-05-26 06:56:38.661406: step 27340, loss = 0.04 (1010.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:56:41.155821: step 27350, loss = 0.03 (1059.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:56:43.658208: step 27360, loss = 0.02 (1058.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:56:46.092269: step 27370, loss = 0.01 (965.3 examples/sec; 0.133 sec/batch)
2016-05-26 06:56:48.598501: step 27380, loss = 0.03 (980.2 examples/sec; 0.131 sec/batch)
2016-05-26 06:56:51.071940: step 27390, loss = 0.01 (1062.3 examples/sec; 0.120 sec/batch)
2016-05-26 06:56:53.581498: step 27400, loss = 0.02 (1042.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:56:56.330360: step 27410, loss = 0.02 (1022.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:56:58.782116: step 27420, loss = 0.01 (1059.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:57:01.252996: step 27430, loss = 0.01 (1014.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:57:03.737754: step 27440, loss = 0.01 (1046.7 examples/sec; 0.122 sec/batch)
2016-05-26 06:57:06.211827: step 27450, loss = 0.02 (1081.8 examples/sec; 0.118 sec/batch)
2016-05-26 06:57:08.695845: step 27460, loss = 0.02 (1008.9 examples/sec; 0.127 sec/batch)
2016-05-26 06:57:11.170073: step 27470, loss = 0.01 (1026.2 examples/sec; 0.125 sec/batch)
2016-05-26 06:57:13.647477: step 27480, loss = 0.01 (1025.4 examples/sec; 0.125 sec/batch)
2016-05-26 06:57:16.160821: step 27490, loss = 0.00 (1030.6 examples/sec; 0.124 sec/batch)
2016-05-26 06:57:18.628589: step 27500, loss = 0.00 (1012.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:57:21.436138: step 27510, loss = 0.02 (954.0 examples/sec; 0.134 sec/batch)
2016-05-26 06:57:23.911093: step 27520, loss = 0.02 (1044.3 examples/sec; 0.123 sec/batch)
2016-05-26 06:57:26.382630: step 27530, loss = 0.00 (984.8 examples/sec; 0.130 sec/batch)
2016-05-26 06:57:28.849941: step 27540, loss = 0.01 (1034.0 examples/sec; 0.124 sec/batch)
2016-05-26 06:57:31.348963: step 27550, loss = 0.01 (994.4 examples/sec; 0.129 sec/batch)
2016-05-26 06:57:33.831605: step 27560, loss = 0.03 (1009.0 examples/sec; 0.127 sec/batch)
2016-05-26 06:57:36.293732: step 27570, loss = 0.01 (1099.9 examples/sec; 0.116 sec/batch)
2016-05-26 06:57:38.774070: step 27580, loss = 0.01 (979.0 examples/sec; 0.131 sec/batch)
2016-05-26 06:57:41.243973: step 27590, loss = 0.02 (1067.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:57:43.733871: step 27600, loss = 0.00 (1020.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:57:46.538790: step 27610, loss = 0.00 (1030.9 examples/sec; 0.124 sec/batch)
2016-05-26 06:57:49.043441: step 27620, loss = 0.01 (1006.7 examples/sec; 0.127 sec/batch)
2016-05-26 06:57:51.541710: step 27630, loss = 0.02 (1016.1 examples/sec; 0.126 sec/batch)
2016-05-26 06:57:54.019707: step 27640, loss = 0.06 (1043.6 examples/sec; 0.123 sec/batch)
2016-05-26 06:57:56.492109: step 27650, loss = 0.01 (1011.7 examples/sec; 0.127 sec/batch)
2016-05-26 06:57:58.948408: step 27660, loss = 0.04 (1055.7 examples/sec; 0.121 sec/batch)
2016-05-26 06:58:01.467160: step 27670, loss = 0.02 (1031.1 examples/sec; 0.124 sec/batch)
2016-05-26 06:58:03.962929: step 27680, loss = 0.02 (1009.4 examples/sec; 0.127 sec/batch)
2016-05-26 06:58:06.418641: step 27690, loss = 0.05 (1022.7 examples/sec; 0.125 sec/batch)
2016-05-26 06:58:08.949629: step 27700, loss = 0.01 (1014.5 examples/sec; 0.126 sec/batch)
2016-05-26 06:58:11.745988: step 27710, loss = 0.01 (1100.8 examples/sec; 0.116 sec/batch)
2016-05-26 06:58:14.254004: step 27720, loss = 0.01 (947.2 examples/sec; 0.135 sec/batch)
2016-05-26 06:58:16.804728: step 27730, loss = 0.01 (1055.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:58:19.300249: step 27740, loss = 0.01 (1061.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:58:21.830406: step 27750, loss = 0.02 (1042.2 examples/sec; 0.123 sec/batch)
2016-05-26 06:58:24.338326: step 27760, loss = 0.01 (1058.9 examples/sec; 0.121 sec/batch)
2016-05-26 06:58:26.841287: step 27770, loss = 0.02 (1020.3 examples/sec; 0.125 sec/batch)
2016-05-26 06:58:29.375784: step 27780, loss = 0.02 (1084.2 examples/sec; 0.118 sec/batch)
2016-05-26 06:58:31.843581: step 27790, loss = 0.01 (1076.7 examples/sec; 0.119 sec/batch)
2016-05-26 06:58:34.298930: step 27800, loss = 0.01 (1035.4 examples/sec; 0.124 sec/batch)
2016-05-26 06:58:37.178074: step 27810, loss = 0.02 (988.6 examples/sec; 0.129 sec/batch)
2016-05-26 06:58:39.646837: step 27820, loss = 0.01 (1075.4 examples/sec; 0.119 sec/batch)
2016-05-26 06:58:42.133977: step 27830, loss = 0.00 (1045.9 examples/sec; 0.122 sec/batch)
2016-05-26 06:58:44.576138: step 27840, loss = 0.01 (1059.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:58:47.026268: step 27850, loss = 0.01 (1019.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:58:49.526620: step 27860, loss = 0.02 (1017.4 examples/sec; 0.126 sec/batch)
2016-05-26 06:58:52.011600: step 27870, loss = 0.01 (1051.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:58:54.442076: step 27880, loss = 0.00 (1072.8 examples/sec; 0.119 sec/batch)
2016-05-26 06:58:56.921715: step 27890, loss = 0.01 (1060.4 examples/sec; 0.121 sec/batch)
2016-05-26 06:58:59.420521: step 27900, loss = 0.06 (1037.4 examples/sec; 0.123 sec/batch)
2016-05-26 06:59:02.214574: step 27910, loss = 0.01 (1045.5 examples/sec; 0.122 sec/batch)
2016-05-26 06:59:04.687249: step 27920, loss = 0.00 (1053.2 examples/sec; 0.122 sec/batch)
2016-05-26 06:59:07.145537: step 27930, loss = 0.01 (1026.0 examples/sec; 0.125 sec/batch)
2016-05-26 06:59:09.643999: step 27940, loss = 0.01 (1054.2 examples/sec; 0.121 sec/batch)
2016-05-26 06:59:12.126318: step 27950, loss = 0.01 (1070.1 examples/sec; 0.120 sec/batch)
2016-05-26 06:59:14.541946: step 27960, loss = 0.01 (992.2 examples/sec; 0.129 sec/batch)
2016-05-26 06:59:17.048672: step 27970, loss = 0.01 (1067.2 examples/sec; 0.120 sec/batch)
2016-05-26 06:59:19.523290: step 27980, loss = 0.00 (1055.8 examples/sec; 0.121 sec/batch)
2016-05-26 06:59:21.994165: step 27990, loss = 0.01 (1103.1 examples/sec; 0.116 sec/batch)
2016-05-26 06:59:24.495668: step 28000, loss = 0.01 (1042.0 examples/sec; 0.123 sec/batch)
eval once
2016-05-26 07:00:06.038890: accuracy @ 1 = 0.996, 49868 / 50048 at 0
2016-05-26 07:00:08.493576: step 28010, loss = 0.00 (1069.2 examples/sec; 0.120 sec/batch)
2016-05-26 07:00:10.981629: step 28020, loss = 0.03 (1022.2 examples/sec; 0.125 sec/batch)
2016-05-26 07:00:13.456332: step 28030, loss = 0.01 (1007.7 examples/sec; 0.127 sec/batch)
2016-05-26 07:00:15.977838: step 28040, loss = 0.01 (1018.0 examples/sec; 0.126 sec/batch)
2016-05-26 07:00:18.409535: step 28050, loss = 0.02 (1016.8 examples/sec; 0.126 sec/batch)
2016-05-26 07:00:20.865428: step 28060, loss = 0.01 (1086.4 examples/sec; 0.118 sec/batch)
2016-05-26 07:00:23.314078: step 28070, loss = 0.02 (1001.0 examples/sec; 0.128 sec/batch)
2016-05-26 07:00:25.795629: step 28080, loss = 0.02 (1017.5 examples/sec; 0.126 sec/batch)
2016-05-26 07:00:28.317442: step 28090, loss = 0.03 (1106.0 examples/sec; 0.116 sec/batch)
2016-05-26 07:00:30.845910: step 28100, loss = 0.01 (989.8 examples/sec; 0.129 sec/batch)
2016-05-26 07:00:33.643842: step 28110, loss = 0.01 (993.0 examples/sec; 0.129 sec/batch)
2016-05-26 07:00:36.142527: step 28120, loss = 0.01 (991.6 examples/sec; 0.129 sec/batch)
2016-05-26 07:00:38.634490: step 28130, loss = 0.00 (1044.9 examples/sec; 0.122 sec/batch)
2016-05-26 07:00:41.141293: step 28140, loss = 0.00 (1048.1 examples/sec; 0.122 sec/batch)
2016-05-26 07:00:43.600162: step 28150, loss = 0.01 (1067.4 examples/sec; 0.120 sec/batch)
2016-05-26 07:00:46.086769: step 28160, loss = 0.01 (1069.6 examples/sec; 0.120 sec/batch)
2016-05-26 07:00:48.560856: step 28170, loss = 0.01 (1039.8 examples/sec; 0.123 sec/batch)
2016-05-26 07:00:51.057673: step 28180, loss = 0.01 (1079.4 examples/sec; 0.119 sec/batch)
2016-05-26 07:00:53.546323: step 28190, loss = 0.01 (1072.4 examples/sec; 0.119 sec/batch)
2016-05-26 07:00:56.018175: step 28200, loss = 0.01 (995.2 examples/sec; 0.129 sec/batch)
2016-05-26 07:00:58.866924: step 28210, loss = 0.03 (991.0 examples/sec; 0.129 sec/batch)
2016-05-26 07:01:01.383833: step 28220, loss = 0.01 (1079.9 examples/sec; 0.119 sec/batch)
2016-05-26 07:01:03.802857: step 28230, loss = 0.01 (1009.9 examples/sec; 0.127 sec/batch)
2016-05-26 07:01:06.280593: step 28240, loss = 0.02 (1007.3 examples/sec; 0.127 sec/batch)
2016-05-26 07:01:08.804812: step 28250, loss = 0.04 (1038.1 examples/sec; 0.123 sec/batch)
2016-05-26 07:01:11.300789: step 28260, loss = 0.01 (1044.2 examples/sec; 0.123 sec/batch)
2016-05-26 07:01:13.781482: step 28270, loss = 0.01 (1056.4 examples/sec; 0.121 sec/batch)
2016-05-26 07:01:16.254781: step 28280, loss = 0.04 (986.9 examples/sec; 0.130 sec/batch)
2016-05-26 07:01:18.749910: step 28290, loss = 0.00 (1037.3 examples/sec; 0.123 sec/batch)
2016-05-26 07:01:21.298516: step 28300, loss = 0.01 (973.6 examples/sec; 0.131 sec/batch)
2016-05-26 07:01:24.383580: step 28310, loss = 0.01 (1030.4 examples/sec; 0.124 sec/batch)
2016-05-26 07:01:26.795591: step 28320, loss = 0.00 (1077.3 examples/sec; 0.119 sec/batch)
2016-05-26 07:01:29.239843: step 28330, loss = 0.01 (1065.9 examples/sec; 0.120 sec/batch)
2016-05-26 07:01:31.677651: step 28340, loss = 0.00 (1047.1 examples/sec; 0.122 sec/batch)
2016-05-26 07:01:34.154023: step 28350, loss = 0.01 (998.2 examples/sec; 0.128 sec/batch)
2016-05-26 07:01:36.603018: step 28360, loss = 0.00 (1027.5 examples/sec; 0.125 sec/batch)
2016-05-26 07:01:39.068856: step 28370, loss = 0.01 (977.0 examples/sec; 0.131 sec/batch)
2016-05-26 07:01:41.616067: step 28380, loss = 0.00 (1066.0 examples/sec; 0.120 sec/batch)
2016-05-26 07:01:44.095492: step 28390, loss = 0.01 (1035.7 examples/sec; 0.124 sec/batch)
2016-05-26 07:01:46.603221: step 28400, loss = 0.00 (1007.9 examples/sec; 0.127 sec/batch)
2016-05-26 07:01:49.479776: step 28410, loss = 0.01 (1000.0 examples/sec; 0.128 sec/batch)
2016-05-26 07:01:51.992679: step 28420, loss = 0.00 (966.3 examples/sec; 0.132 sec/batch)
2016-05-26 07:01:54.461260: step 28430, loss = 0.01 (1008.3 examples/sec; 0.127 sec/batch)
2016-05-26 07:01:56.952344: step 28440, loss = 0.01 (997.4 examples/sec; 0.128 sec/batch)
2016-05-26 07:01:59.406334: step 28450, loss = 0.03 (1032.8 examples/sec; 0.124 sec/batch)
2016-05-26 07:02:01.816652: step 28460, loss = 0.00 (1088.7 examples/sec; 0.118 sec/batch)
2016-05-26 07:02:04.261491: step 28470, loss = 0.01 (997.3 examples/sec; 0.128 sec/batch)
2016-05-26 07:02:06.704018: step 28480, loss = 0.06 (1048.3 examples/sec; 0.122 sec/batch)
2016-05-26 07:02:09.193322: step 28490, loss = 0.00 (1026.7 examples/sec; 0.125 sec/batch)
2016-05-26 07:02:11.698921: step 28500, loss = 0.01 (1038.1 examples/sec; 0.123 sec/batch)
2016-05-26 07:02:14.486695: step 28510, loss = 0.01 (989.2 examples/sec; 0.129 sec/batch)
2016-05-26 07:02:16.955194: step 28520, loss = 0.01 (1019.6 examples/sec; 0.126 sec/batch)
2016-05-26 07:02:19.512866: step 28530, loss = 0.01 (1056.9 examples/sec; 0.121 sec/batch)
2016-05-26 07:02:22.002580: step 28540, loss = 0.01 (997.5 examples/sec; 0.128 sec/batch)
2016-05-26 07:02:24.466341: step 28550, loss = 0.01 (1065.2 examples/sec; 0.120 sec/batch)
2016-05-26 07:02:26.945763: step 28560, loss = 0.02 (1034.2 examples/sec; 0.124 sec/batch)
2016-05-26 07:02:29.458792: step 28570, loss = 0.01 (1022.0 examples/sec; 0.125 sec/batch)
2016-05-26 07:02:31.959605: step 28580, loss = 0.02 (1057.2 examples/sec; 0.121 sec/batch)
2016-05-26 07:02:34.440551: step 28590, loss = 0.01 (975.1 examples/sec; 0.131 sec/batch)
2016-05-26 07:02:36.969250: step 28600, loss = 0.00 (1021.7 examples/sec; 0.125 sec/batch)
2016-05-26 07:02:39.742410: step 28610, loss = 0.01 (1089.7 examples/sec; 0.117 sec/batch)
2016-05-26 07:02:42.246589: step 28620, loss = 0.02 (1032.1 examples/sec; 0.124 sec/batch)
2016-05-26 07:02:44.724879: step 28630, loss = 0.01 (1070.4 examples/sec; 0.120 sec/batch)
2016-05-26 07:02:47.209810: step 28640, loss = 0.01 (1081.8 examples/sec; 0.118 sec/batch)
2016-05-26 07:02:49.682294: step 28650, loss = 0.00 (1032.2 examples/sec; 0.124 sec/batch)
2016-05-26 07:02:52.154665: step 28660, loss = 0.02 (965.6 examples/sec; 0.133 sec/batch)
2016-05-26 07:02:54.657073: step 28670, loss = 0.02 (1017.2 examples/sec; 0.126 sec/batch)
2016-05-26 07:02:57.109678: step 28680, loss = 0.01 (1010.8 examples/sec; 0.127 sec/batch)
2016-05-26 07:02:59.581409: step 28690, loss = 0.00 (1065.1 examples/sec; 0.120 sec/batch)
2016-05-26 07:03:02.057759: step 28700, loss = 0.01 (1094.1 examples/sec; 0.117 sec/batch)
2016-05-26 07:03:04.799969: step 28710, loss = 0.01 (1051.0 examples/sec; 0.122 sec/batch)
2016-05-26 07:03:07.286808: step 28720, loss = 0.03 (1013.1 examples/sec; 0.126 sec/batch)
2016-05-26 07:03:09.801657: step 28730, loss = 0.02 (1047.1 examples/sec; 0.122 sec/batch)
2016-05-26 07:03:12.308912: step 28740, loss = 0.01 (1075.2 examples/sec; 0.119 sec/batch)
2016-05-26 07:03:14.779656: step 28750, loss = 0.01 (1017.5 examples/sec; 0.126 sec/batch)
2016-05-26 07:03:17.328706: step 28760, loss = 0.03 (1036.1 examples/sec; 0.124 sec/batch)
2016-05-26 07:03:19.858304: step 28770, loss = 0.01 (1005.2 examples/sec; 0.127 sec/batch)
2016-05-26 07:03:22.364355: step 28780, loss = 0.01 (1072.6 examples/sec; 0.119 sec/batch)
2016-05-26 07:03:24.830023: step 28790, loss = 0.00 (1049.9 examples/sec; 0.122 sec/batch)
2016-05-26 07:03:27.300510: step 28800, loss = 0.01 (991.1 examples/sec; 0.129 sec/batch)
2016-05-26 07:03:30.102495: step 28810, loss = 0.01 (969.7 examples/sec; 0.132 sec/batch)
2016-05-26 07:03:32.566493: step 28820, loss = 0.01 (1106.5 examples/sec; 0.116 sec/batch)
2016-05-26 07:03:35.085476: step 28830, loss = 0.01 (1098.2 examples/sec; 0.117 sec/batch)
2016-05-26 07:03:37.579046: step 28840, loss = 0.01 (1050.5 examples/sec; 0.122 sec/batch)
2016-05-26 07:03:40.075864: step 28850, loss = 0.01 (1023.4 examples/sec; 0.125 sec/batch)
2016-05-26 07:03:42.527460: step 28860, loss = 0.01 (1035.7 examples/sec; 0.124 sec/batch)
2016-05-26 07:03:45.017508: step 28870, loss = 0.00 (1036.8 examples/sec; 0.123 sec/batch)
2016-05-26 07:03:47.518793: step 28880, loss = 0.01 (1048.1 examples/sec; 0.122 sec/batch)
2016-05-26 07:03:49.977067: step 28890, loss = 0.02 (1032.1 examples/sec; 0.124 sec/batch)
2016-05-26 07:03:52.499056: step 28900, loss = 0.01 (1054.7 examples/sec; 0.121 sec/batch)
2016-05-26 07:03:55.283661: step 28910, loss = 0.03 (1101.1 examples/sec; 0.116 sec/batch)
2016-05-26 07:03:57.761170: step 28920, loss = 0.01 (1078.4 examples/sec; 0.119 sec/batch)
2016-05-26 07:04:00.254410: step 28930, loss = 0.00 (1045.2 examples/sec; 0.122 sec/batch)
2016-05-26 07:04:02.742257: step 28940, loss = 0.02 (1079.6 examples/sec; 0.119 sec/batch)
2016-05-26 07:04:05.247105: step 28950, loss = 0.03 (1033.9 examples/sec; 0.124 sec/batch)
2016-05-26 07:04:07.739727: step 28960, loss = 0.00 (1044.0 examples/sec; 0.123 sec/batch)
2016-05-26 07:04:10.165925: step 28970, loss = 0.01 (1073.0 examples/sec; 0.119 sec/batch)
2016-05-26 07:04:12.685281: step 28980, loss = 0.01 (1021.5 examples/sec; 0.125 sec/batch)
2016-05-26 07:04:15.129162: step 28990, loss = 0.02 (1056.8 examples/sec; 0.121 sec/batch)
2016-05-26 07:04:17.612175: step 29000, loss = 0.01 (1043.3 examples/sec; 0.123 sec/batch)
eval once
2016-05-26 07:04:59.318918: accuracy @ 1 = 0.996, 49835 / 50048 at 0
2016-05-26 07:05:01.827829: step 29010, loss = 0.01 (1012.4 examples/sec; 0.126 sec/batch)
2016-05-26 07:05:04.324479: step 29020, loss = 0.03 (1001.0 examples/sec; 0.128 sec/batch)
2016-05-26 07:05:06.801072: step 29030, loss = 0.01 (1010.7 examples/sec; 0.127 sec/batch)
2016-05-26 07:05:09.290708: step 29040, loss = 0.02 (1001.2 examples/sec; 0.128 sec/batch)
2016-05-26 07:05:11.760286: step 29050, loss = 0.02 (1010.1 examples/sec; 0.127 sec/batch)
2016-05-26 07:05:14.237840: step 29060, loss = 0.01 (1055.7 examples/sec; 0.121 sec/batch)
2016-05-26 07:05:16.743179: step 29070, loss = 0.03 (957.3 examples/sec; 0.134 sec/batch)
2016-05-26 07:05:19.253682: step 29080, loss = 0.01 (1075.1 examples/sec; 0.119 sec/batch)
2016-05-26 07:05:21.744434: step 29090, loss = 0.02 (1081.1 examples/sec; 0.118 sec/batch)
2016-05-26 07:05:24.233212: step 29100, loss = 0.00 (1011.3 examples/sec; 0.127 sec/batch)
2016-05-26 07:05:27.074820: step 29110, loss = 0.00 (1000.2 examples/sec; 0.128 sec/batch)
2016-05-26 07:05:29.577268: step 29120, loss = 0.01 (1018.5 examples/sec; 0.126 sec/batch)
2016-05-26 07:05:32.067949: step 29130, loss = 0.01 (1064.9 examples/sec; 0.120 sec/batch)
2016-05-26 07:05:34.532273: step 29140, loss = 0.02 (1020.4 examples/sec; 0.125 sec/batch)
2016-05-26 07:05:37.035141: step 29150, loss = 0.00 (1006.0 examples/sec; 0.127 sec/batch)
2016-05-26 07:05:39.504728: step 29160, loss = 0.01 (1056.8 examples/sec; 0.121 sec/batch)
2016-05-26 07:05:41.943240: step 29170, loss = 0.00 (1051.8 examples/sec; 0.122 sec/batch)
2016-05-26 07:05:44.463699: step 29180, loss = 0.02 (997.9 examples/sec; 0.128 sec/batch)
2016-05-26 07:05:46.936150: step 29190, loss = 0.00 (1077.8 examples/sec; 0.119 sec/batch)
2016-05-26 07:05:49.397501: step 29200, loss = 0.01 (1078.8 examples/sec; 0.119 sec/batch)
2016-05-26 07:05:52.192671: step 29210, loss = 0.02 (1026.9 examples/sec; 0.125 sec/batch)
2016-05-26 07:05:54.630049: step 29220, loss = 0.00 (1014.8 examples/sec; 0.126 sec/batch)
2016-05-26 07:05:57.135977: step 29230, loss = 0.01 (1010.3 examples/sec; 0.127 sec/batch)
2016-05-26 07:05:59.675805: step 29240, loss = 0.03 (1032.9 examples/sec; 0.124 sec/batch)
2016-05-26 07:06:02.198481: step 29250, loss = 0.00 (1071.1 examples/sec; 0.120 sec/batch)
2016-05-26 07:06:04.650981: step 29260, loss = 0.00 (1021.1 examples/sec; 0.125 sec/batch)
2016-05-26 07:06:07.117871: step 29270, loss = 0.01 (1076.6 examples/sec; 0.119 sec/batch)
2016-05-26 07:06:09.610771: step 29280, loss = 0.01 (1080.9 examples/sec; 0.118 sec/batch)
2016-05-26 07:06:12.110502: step 29290, loss = 0.04 (1008.1 examples/sec; 0.127 sec/batch)
2016-05-26 07:06:14.572581: step 29300, loss = 0.01 (991.0 examples/sec; 0.129 sec/batch)
2016-05-26 07:06:17.357117: step 29310, loss = 0.01 (1036.6 examples/sec; 0.123 sec/batch)
2016-05-26 07:06:19.815172: step 29320, loss = 0.02 (985.8 examples/sec; 0.130 sec/batch)
2016-05-26 07:06:22.316979: step 29330, loss = 0.00 (998.2 examples/sec; 0.128 sec/batch)
2016-05-26 07:06:24.813482: step 29340, loss = 0.00 (980.7 examples/sec; 0.131 sec/batch)
2016-05-26 07:06:27.351462: step 29350, loss = 0.01 (1059.4 examples/sec; 0.121 sec/batch)
2016-05-26 07:06:29.821740: step 29360, loss = 0.01 (1039.7 examples/sec; 0.123 sec/batch)
2016-05-26 07:06:32.292331: step 29370, loss = 0.02 (983.8 examples/sec; 0.130 sec/batch)
2016-05-26 07:06:34.738440: step 29380, loss = 0.00 (1051.0 examples/sec; 0.122 sec/batch)
2016-05-26 07:06:37.243612: step 29390, loss = 0.02 (1015.3 examples/sec; 0.126 sec/batch)
2016-05-26 07:06:39.783479: step 29400, loss = 0.01 (966.8 examples/sec; 0.132 sec/batch)
2016-05-26 07:06:42.558274: step 29410, loss = 0.00 (1073.7 examples/sec; 0.119 sec/batch)
2016-05-26 07:06:45.058112: step 29420, loss = 0.02 (1055.0 examples/sec; 0.121 sec/batch)
2016-05-26 07:06:47.563016: step 29430, loss = 0.00 (941.8 examples/sec; 0.136 sec/batch)
2016-05-26 07:06:50.094070: step 29440, loss = 0.01 (969.7 examples/sec; 0.132 sec/batch)
2016-05-26 07:06:52.593436: step 29450, loss = 0.00 (959.5 examples/sec; 0.133 sec/batch)
2016-05-26 07:06:55.056739: step 29460, loss = 0.01 (1023.1 examples/sec; 0.125 sec/batch)
2016-05-26 07:06:57.553323: step 29470, loss = 0.00 (1052.5 examples/sec; 0.122 sec/batch)
2016-05-26 07:07:00.037902: step 29480, loss = 0.01 (1052.3 examples/sec; 0.122 sec/batch)
2016-05-26 07:07:02.521853: step 29490, loss = 0.01 (971.7 examples/sec; 0.132 sec/batch)
2016-05-26 07:07:05.032104: step 29500, loss = 0.02 (1011.1 examples/sec; 0.127 sec/batch)
2016-05-26 07:07:07.852859: step 29510, loss = 0.00 (1078.3 examples/sec; 0.119 sec/batch)
2016-05-26 07:07:10.266339: step 29520, loss = 0.00 (1087.3 examples/sec; 0.118 sec/batch)
2016-05-26 07:07:12.726347: step 29530, loss = 0.00 (1084.6 examples/sec; 0.118 sec/batch)
2016-05-26 07:07:15.213918: step 29540, loss = 0.00 (1040.5 examples/sec; 0.123 sec/batch)
2016-05-26 07:07:17.786749: step 29550, loss = 0.00 (1035.1 examples/sec; 0.124 sec/batch)
2016-05-26 07:07:20.267797: step 29560, loss = 0.03 (1072.6 examples/sec; 0.119 sec/batch)
2016-05-26 07:07:22.726936: step 29570, loss = 0.00 (1056.4 examples/sec; 0.121 sec/batch)
2016-05-26 07:07:25.210096: step 29580, loss = 0.02 (1075.6 examples/sec; 0.119 sec/batch)
2016-05-26 07:07:27.721671: step 29590, loss = 0.04 (1056.7 examples/sec; 0.121 sec/batch)
2016-05-26 07:07:30.207280: step 29600, loss = 0.01 (977.7 examples/sec; 0.131 sec/batch)
2016-05-26 07:07:33.291383: step 29610, loss = 0.00 (993.4 examples/sec; 0.129 sec/batch)
2016-05-26 07:07:35.811742: step 29620, loss = 0.00 (1070.4 examples/sec; 0.120 sec/batch)
2016-05-26 07:07:38.235350: step 29630, loss = 0.03 (1074.1 examples/sec; 0.119 sec/batch)
2016-05-26 07:07:40.728013: step 29640, loss = 0.00 (1034.2 examples/sec; 0.124 sec/batch)
2016-05-26 07:07:43.211408: step 29650, loss = 0.01 (1038.7 examples/sec; 0.123 sec/batch)
2016-05-26 07:07:45.741407: step 29660, loss = 0.02 (997.5 examples/sec; 0.128 sec/batch)
2016-05-26 07:07:48.275249: step 29670, loss = 0.01 (1053.3 examples/sec; 0.122 sec/batch)
2016-05-26 07:07:50.764626: step 29680, loss = 0.00 (1055.3 examples/sec; 0.121 sec/batch)
2016-05-26 07:07:53.200405: step 29690, loss = 0.04 (1078.3 examples/sec; 0.119 sec/batch)
2016-05-26 07:07:55.699160: step 29700, loss = 0.00 (1011.1 examples/sec; 0.127 sec/batch)
2016-05-26 07:07:58.431094: step 29710, loss = 0.01 (1058.6 examples/sec; 0.121 sec/batch)
2016-05-26 07:08:00.907108: step 29720, loss = 0.00 (1032.7 examples/sec; 0.124 sec/batch)
2016-05-26 07:08:03.383662: step 29730, loss = 0.00 (989.4 examples/sec; 0.129 sec/batch)
2016-05-26 07:08:05.878094: step 29740, loss = 0.00 (1039.2 examples/sec; 0.123 sec/batch)
2016-05-26 07:08:08.385424: step 29750, loss = 0.02 (991.7 examples/sec; 0.129 sec/batch)
2016-05-26 07:08:10.849074: step 29760, loss = 0.03 (1038.0 examples/sec; 0.123 sec/batch)
2016-05-26 07:08:13.343092: step 29770, loss = 0.02 (999.7 examples/sec; 0.128 sec/batch)
2016-05-26 07:08:15.873207: step 29780, loss = 0.00 (1014.2 examples/sec; 0.126 sec/batch)
2016-05-26 07:08:18.339458: step 29790, loss = 0.01 (1038.6 examples/sec; 0.123 sec/batch)
2016-05-26 07:08:20.795860: step 29800, loss = 0.01 (1089.7 examples/sec; 0.117 sec/batch)
2016-05-26 07:08:23.636688: step 29810, loss = 0.01 (1052.2 examples/sec; 0.122 sec/batch)
2016-05-26 07:08:26.127328: step 29820, loss = 0.00 (1003.5 examples/sec; 0.128 sec/batch)
2016-05-26 07:08:28.599405: step 29830, loss = 0.00 (991.5 examples/sec; 0.129 sec/batch)
2016-05-26 07:08:31.043800: step 29840, loss = 0.02 (1016.2 examples/sec; 0.126 sec/batch)
2016-05-26 07:08:33.560744: step 29850, loss = 0.00 (982.8 examples/sec; 0.130 sec/batch)
2016-05-26 07:08:36.055429: step 29860, loss = 0.01 (1020.7 examples/sec; 0.125 sec/batch)
2016-05-26 07:08:38.532405: step 29870, loss = 0.00 (1092.6 examples/sec; 0.117 sec/batch)
2016-05-26 07:08:41.035409: step 29880, loss = 0.01 (1022.0 examples/sec; 0.125 sec/batch)
2016-05-26 07:08:43.544585: step 29890, loss = 0.01 (1000.4 examples/sec; 0.128 sec/batch)
2016-05-26 07:08:46.021517: step 29900, loss = 0.00 (1004.1 examples/sec; 0.127 sec/batch)
2016-05-26 07:08:48.862696: step 29910, loss = 0.01 (1022.6 examples/sec; 0.125 sec/batch)
2016-05-26 07:08:51.384008: step 29920, loss = 0.01 (1033.7 examples/sec; 0.124 sec/batch)
2016-05-26 07:08:53.919854: step 29930, loss = 0.00 (1036.9 examples/sec; 0.123 sec/batch)
2016-05-26 07:08:56.407124: step 29940, loss = 0.00 (1019.0 examples/sec; 0.126 sec/batch)
2016-05-26 07:08:58.890905: step 29950, loss = 0.01 (1050.9 examples/sec; 0.122 sec/batch)
2016-05-26 07:09:01.409059: step 29960, loss = 0.01 (1013.3 examples/sec; 0.126 sec/batch)
2016-05-26 07:09:03.893012: step 29970, loss = 0.00 (1076.5 examples/sec; 0.119 sec/batch)
2016-05-26 07:09:06.402632: step 29980, loss = 0.00 (959.4 examples/sec; 0.133 sec/batch)
2016-05-26 07:09:08.887372: step 29990, loss = 0.04 (995.8 examples/sec; 0.129 sec/batch)
eval once
2016-05-26 07:09:52.870316: accuracy @ 1 = 0.998, 49947 / 50048 at 0
